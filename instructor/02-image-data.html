<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>Image Classification with Convolutional Neural Networks: Introduction to Image Data</title><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" type="text/css" href="../assets/styles.css"><script src="../assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png"><link rel="manifest" href="../site.webmanifest"><link rel="mask-icon" href="../safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" content="#ffffff"></head><body>
    <header id="top" class="navbar navbar-expand-md navbar-light bg-white top-nav incubator"><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-6">
      <div class="large-logo">
        <img alt="Carpentries Incubator" src="../assets/images/incubator-logo.svg"><abbr class="icon" title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught." style="text-decoration: unset">
           
          <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#early-development-pre-alpha-through-alpha" class="external-link alert-link" style="color: #383838">Pre-Alpha
            <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="color: #FF4955; border-radius: 5px"></i>
          </a>
          <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
        </abbr>
        
      </div>
    </div>
    <div class="selector-container">
      
      
      <div class="dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1"><li><button class="dropdown-item" type="button" onclick="window.location.href='../02-image-data.html';">Learner View</button></li>
        </ul></div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl navbar-light bg-white bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="../assets/images/incubator-logo-sm.svg"></div>
    <div class="lesson-title-md">
      Image Classification with Convolutional Neural Networks
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
      <i role="img" aria-label="search button" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item">
          <span class="lesson-title">
            Image Classification with Convolutional Neural Networks
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/instructor-notes.html">Instructor Notes</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/images.html">Extract All Images</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"></ul></li>
      </ul></div>
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled><input class="form-control me-2 searchbox" type="search" placeholder="Search" aria-label="Search"><button class="btn btn-outline-success tablet-search-button" type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="search button"></i>
        </button>
      </fieldset></form>
  </div><!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Image Classification with Convolutional Neural Networks
</div>

<aside class="col-md-12 lesson-progress"><div style="width: 25%" class="percentage">
    25%
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: 25%" aria-valuenow="25" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="../02-image-data.html">Learner View</a>
                      </div>
                    </div>
                  </div><!--/div.accordion-item-->
                </div><!--/div.accordion-flush-->
              </div><!--div.sidenav-view-selector -->
            </div><!--/div.col -->
      
            <hr></div><!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Schedule</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="setup-gpu.html">1. Setup - GPU</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="01-introduction.html">2. Introduction to Deep Learning</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlushcurrent">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-headingcurrent">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapsecurrent" aria-expanded="true" aria-controls="flush-collapsecurrent">
        <span class="visually-hidden">Current Chapter</span>
        <span class="current-chapter">
        3. Introduction to Image Data
        </span>
      </button>
    </div><!--/div.accordion-header-->
        
    <div id="flush-collapsecurrent" class="accordion-collapse collapse show" aria-labelledby="flush-headingcurrent" data-bs-parent="#accordionFlushcurrent">
      <div class="accordion-body">
        <ul><li><a href="#deep-learning-workflow">Deep Learning Workflow</a></li>
<li><a href="#cinic-10-test-dataset-preparation">CINIC-10 Test Dataset Preparation</a></li>
        </ul></div><!--/div.accordion-body-->
    </div><!--/div.accordion-collapse-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="03-build-cnn.html">4. Build a Convolutional Neural Network</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="04-fit-cnn.html">5. Compile and Train a Convolutional Neural Network</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="05-evaluate-predict-cnn.html">6. Evaluate a Convolutional Neural Network and Make Predictions (Classifications)</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush8">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading8">
        <a href="06-conclusion.html">7. Conclusion</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width"><div class="accordion accordion-flush resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul><li>
                        <a href="../instructor/key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="../instructor/instructor-notes.html">Instructor Notes</a>
                      </li>
                      <li>
                        <a href="../instructor/images.html">Extract All Images</a>
                      </li>
                      
                    </ul></div>
                </div>
              </div>
            </div>
            <hr class="half-width resources"><a href="../instructor/aio.html">See all in one page</a>
            

            <hr class="d-none d-sm-block d-md-none"><div class="d-grid gap-1">
            
            </div>
          </div><!-- /div.accordion -->
        </div><!-- /div.sidebar-inner -->
      </nav></div><!-- /div.sidebar -->
  </div><!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-instructor.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <nav class="lesson-content mx-md-4" aria-label="Previous and Next Chapter"><!-- content for small screens --><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/01-introduction.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/03-build-cnn.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/01-introduction.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Introduction to Deep
        </a>
        <a class="chapter-link float-end" href="../instructor/03-build-cnn.html" rel="next">
          Next: Build a... 
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
      <hr></nav><main id="main-content" class="main-content"><div class="container lesson-content">
        <h1>Introduction to Image Data</h1>
        <p> Last updated on 2023-10-06 | 
        
        <a href="https://https://github.com/erinmgraham/icwithcnn/edit/main/episodes/02-image-data.html" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
        
        
        
        <p>Estimated time <i aria-hidden="true" data-feather="clock"></i> 12 minutes </p>
        
        <div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right"> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>

        

<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul><li>How much data do you need for Deep Learning?</li>
<li>Where can I find image data to train my model?</li>
<li>How do I plot image data in python?</li>
<li>How do I prepare image data for use in a convolutional neural
network (CNN)?</li>
</ul></div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul><li>Identify sources of image data</li>
<li>Write code to plot image data</li>
<li>Understand the properties of image data</li>
<li>Prepare an image data set to train a convolutional neural network
(CNN)</li>
</ul></div>
</div>
</div>
</div>
</div>
<section id="deep-learning-workflow"><h2 class="section-heading">Deep Learning Workflow<a class="anchor" aria-label="anchor" href="#deep-learning-workflow"></a>
</h2>
<hr class="half-width"><p>Let’s start over with the first steps in our workflow.</p>
<div class="section level3">
<h3 id="step-1--formulate-outline-the-problem">Step 1. Formulate/ Outline the problem<a class="anchor" aria-label="anchor" href="#step-1--formulate-outline-the-problem"></a></h3>
<p>Firstly we must decide what it is we want our Deep Learning system to
do. This lesson is all about image classification and our aim is to put
an image into one of ten categories: airplane, automobile, bird, cat,
deer, dog, frog, horse, ship, or truck</p>
</div>
<div class="section level3">
<h3 id="step-2--identify-inputs-and-outputs">Step 2. Identify inputs and outputs<a class="anchor" aria-label="anchor" href="#step-2--identify-inputs-and-outputs"></a></h3>
<p>Next we need to identify what the inputs and outputs of the neural
network will be. In our case, the data is images and the inputs could be
the individual pixels of the images.</p>
<p>We are performing a classification problem and we want to output one
category for each image.</p>
</div>
<div class="section level3">
<h3 id="step-3--prepare-data">Step 3. Prepare data<a class="anchor" aria-label="anchor" href="#step-3--prepare-data"></a></h3>
<p>Deep Learning requires extensive training using example data which
shows the network what output it should produce for a given input. In
this workshop our network will be trained by being “shown” a series of
images and told what they contain. Once the network is trained it should
be able to take another image and correctly classify its contents.</p>
<p>You can use pre-existing data or prepare your own.</p>
<div class="section level4">
<h4 id="pre-existing-image-data">Pre-existing image data<a class="anchor" aria-label="anchor" href="#pre-existing-image-data"></a></h4>
<p>In some cases you will be able to download an image dataset that is
already labelled and can be used to classify a number of different
object like we see with the CIFAR-10 dataset. Other examples
include:</p>
<ul><li>
<a href="https://en.wikipedia.org/wiki/MNIST_database" class="external-link">MNIST
database</a> - 60,000 training images of handwritten digits (0-9)</li>
<li>
<a href="https://www.image-net.org/" class="external-link">ImageNet</a> - 14 million
hand-annotated images indicating objects from more than 20,000
categories. ImageNet sponsors an <a href="https://www.image-net.org/challenges/LSVRC/#:~:text=The%20ImageNet%20Large%20Scale%20Visual,image%20classification%20at%20large%20scale." class="external-link">annual
software contest</a> where programs compete to achieve the highest
accuracy. When choosing a pretrained network, the winners of these sorts
of competitions are generally a good place to start.</li>
<li>
<a href="https://cocodataset.org/#home" class="external-link">MS COCO</a> - &gt;200,000
labelled images used for object detection, instance segmentation,
keypoint analysis, and captioning</li>
</ul><p>Where labelled data exists, in most cases the data provider or other
users will have created functions that you can use to load the data. We
already saw an example of this in the introduction:</p>
<pre><code><span><span class="co"># load the CIFAR-10 dataset included with the keras packages</span></span>
<span><span class="co">#from tensorflow import keras</span></span>
<span></span>
<span><span class="co">#(train_images, train_labels), (val_images, val_labels) = #keras.datasets.cifar10.load_data()</span></span></code></pre>
<p>In this instance the data is likely already prepared for use in a
CNN. However, it is always a good idea to first read any associated
documentation to find out what steps the data providers took to prepare
the images and second to take a closer at the images once loaded and
query their attributes.</p>
<div id="callout1" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout1"></a>
</h3>
<div class="callout-content">
<p>How much data do you need for Deep Learning?</p>
<p>The rise of Deep Learning is partially due to the increased
availability of very large datasets. But how much data do you actually
need to train a Deep Learning model? Unfortunately, this question is not
easy to answer. It depends, among other things, on the complexity of the
task (which you often do not know beforehand), the quality of the
available dataset and the complexity of the network. For complex tasks
with large neural networks, we often see that adding more data continues
to improve performance. However, this is also not a generic truth: if
the data you add is too similar to the data you already have, it will
not give much new information to the neural network.</p>
<p>In case you have too little data available to train a complex network
from scratch, it is sometimes possible to use a pretrained network that
was trained on a similar problem. Another trick is data augmentation,
where you expand the dataset with artificial data points that could be
real. An example of this is mirroring images when trying to classify
cats and dogs. An horizontally mirrored animal retains the label, but
exposes a different view.</p>
</div>
</div>
</div>
</div>
<div class="section level4">
<h4 id="custom-image-data">Custom image data<a class="anchor" aria-label="anchor" href="#custom-image-data"></a></h4>
<p>In other cases, you will need to create your own set of labelled
images.</p>
<p><strong>Custom data i. Data collection and Labeling:</strong></p>
<p>For image classification the label applies to the entire image;
object detection requires bounding boxes around objects of interest, and
instance or semantic segmentation requires each pixel to be
labelled.</p>
<p>There are a number of open source software that can be used to label
your dataset, including:</p>
<ul><li>(Visual Geometry Group) <a href="https://www.robots.ox.ac.uk/~vgg/software/via/" class="external-link">VGG Image
Annotator</a> (VIA)</li>
<li>
<a href="https://imagej.net/" class="external-link">ImageJ</a> can be extended with
plugins for annotation</li>
<li>
<a href="https://github.com/jsbroks/coco-annotator" class="external-link">COCO
Annotator</a> is designed specifically for creating annotations
compatible with Common Objects in Context (COCO) format</li>
</ul><p><strong>Custom data ii. Data preprocessing:</strong></p>
<p>This step involves various tasks to enhance the quality and
consistency of the data:</p>
<ul><li><p><strong>Resizing</strong>: Resize images to a consistent
resolution to ensure uniformity and reduce computational load.</p></li>
<li><p><strong>Normalization</strong>: Scale pixel values to a common
range, often between 0 and 1 or -1 and 1. Normalization helps the model
converge faster during training.</p></li>
<li><p><strong>Data Augmentation</strong>: Apply random transformations
(e.g., rotations, flips, shifts) to create new variations of the same
image. This helps improve the model’s robustness and generalization by
exposing it to more diverse data.</p></li>
<li><p><strong>Color Channels</strong>: Depending on the model and
library you use, you might need to handle different color channel orders
(RGB, BGR, etc.).</p></li>
</ul><p>Before we look at some of these tasks in more detail we need to
understand that the images we see on hard copy, view with our electronic
devices, or process with our programs are represented and stored in the
computer as numeric abstractions, or approximations of what we see with
our eyes in the real world. And before we begin to learn how to process
images with Python programs, we need to spend some time understanding
how these abstractions work.</p>
</div>
</div>
<div class="section level3">
<h3 id="pixels">Pixels<a class="anchor" aria-label="anchor" href="#pixels"></a></h3>
<p>It is important to realise that images are stored as rectangular
arrays of hundreds, thousands, or millions of discrete “picture
elements,” otherwise known as pixels. Each pixel can be thought of as a
single square point of coloured light.</p>
<p>For example, consider this image of a Jabiru, with a square area
designated by a red box:</p>
<figure><img src="../fig/02_Jabiru_TGS_marked.jpg" alt="Original size image of a Jabiru with a red square surrounding an area to zoom in on" class="figure mx-auto d-block"></figure><p>Now, if we zoomed in close enough to see the pixels in the red box,
we would see something like this:</p>
<figure><img src="../fig/02_Jabiru_TGS_marked_zoom_enlarged.jpg" alt="Enlarged image area of Jabiru" class="figure mx-auto d-block"></figure><p>Note that each square in the enlarged image area - each pixel - is
all one colour, but that each pixel can have a different colour from its
neighbors. Viewed from a distance, these pixels seem to blend together
to form the image we see.</p>
</div>
<div class="section level3">
<h3 id="working-with-pixels">Working with Pixels<a class="anchor" aria-label="anchor" href="#working-with-pixels"></a></h3>
<p>As noted, in practice, real world images will typically be made up of
a vast number of pixels, and each of these pixels will be one of
potentially millions of colours. In python, an image can be represented
as a multidimensional array, also known as a <code>tensor</code>, where
each element in the array corresponds to a pixel value in the image. In
the context of images, these arrays often have dimensions for height,
width, and color channels (if applicable).</p>
<div id="callout2" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout2"></a>
</h3>
<div class="callout-content">
<p>Matrices, arrays, images and pixels</p>
<p>The matrix is mathematical concept - numbers evenly arranged in a
rectangle. This can be a two dimensional rectangle, like the shape of
the screen you’re looking at now. Or it could be a three dimensional
equivalent, a cuboid, or have even more dimensions, but always keeping
the evenly spaced arrangement of numbers. In computing, array refers to
a structure in the computer’s memory where data is stored in
evenly-spaced elements. This is strongly analogous to a matrix. A NumPy
array is a type of variable (a simpler example of a type is an integer).
For our purposes, the distinction between matrices and arrays is not
important, we don’t really care how the computer arranges our data in
its memory. The important thing is that the computer stores values
describing the pixels in images, as arrays. And the terms matrix and
array can be used interchangeably.</p>
</div>
</div>
</div>
<div id="callout3" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout3"></a>
</h3>
<div class="callout-content">
<p>Python image libraries</p>
<p>Two of the most commonly used libraries for image representation and
manipulation are NumPy and Pillow (PIL). Additionally, when working with
deep learning frameworks like TensorFlow and PyTorch, images are often
represented as tensors within these frameworks.</p>
<ul><li>NumPy is a powerful library for numerical computing in Python. It
provides support for creating and manipulating arrays, which can be used
to represent images as multidimensional arrays.
<ul><li><code>import numpy as np</code></li>
</ul></li>
<li>The Pillow library (PIL fork) provides functions to open,
manipulate, and save various image file formats. It represents images
using its own Image class.
<ul><li><code>from PIL import Image</code></li>
<li>see <a href="https://pillow.readthedocs.io/en/latest/reference/Image.html" class="external-link">PIL
Image Module</a>
</li>
</ul></li>
<li>TensorFlow images are often represented as tensors that have
dimensions for batch size, height, width, and color channels. This
framework provide tools to load, preprocess, and work with image data
seamlessly.
<ul><li><code>from tensorflow import keras</code></li>
<li>see <a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image" class="external-link">image
preprocessing</a> documentation</li>
<li>Note Keras image functions also use PIL</li>
</ul></li>
</ul></div>
</div>
</div>
<p>Let us start by looking at the image we used in the introduction.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load the libraries required</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.utils <span class="im">import</span> img_to_array</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.utils <span class="im">import</span> load_img</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># specify the image path</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>new_img_path <span class="op">=</span> <span class="st">"../data/Jabiru_TGS.JPG"</span> <span class="co"># path to image</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co"># read in the image with default arguments</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>new_img_pil <span class="op">=</span> load_img(new_img_path)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co"># confirm the data class and size</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The new image is of type :'</span>, new_img_pil.__class__, <span class="st">'and has the size'</span>, new_img_pil.size)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The new image is of type : &lt;class 'PIL.JpegImagePlugin.JpegImageFile'&gt; and has the size (552, 573)</code></pre>
</div>
</div>
<div class="section level3">
<h3 id="image-dimensions---resizing">Image Dimensions - Resizing<a class="anchor" aria-label="anchor" href="#image-dimensions---resizing"></a></h3>
<p>Here we see our new image has shape <code>(573, 552, 3)</code>,
meaning it is much larger in size, 573x552 pixels; a rectangle instead
of a square; and consists of 3 colour channels (RGB).</p>
<p>Recall from the introduction that our training data set consists of
50000 images of 32x32 pixels and 3 channels.</p>
<p>To reduce the computational load and ensure all of our images have a
uniform size, we need to choose an image resolution (or size in pixels)
and ensure that all of the images we use are resized to that shape to be
consistent.</p>
<p>There are a couple of ways to do this in python but one way is to
specify the size you want using an argument to the
<code>load_img()</code> function from <code>keras.utils</code>.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># read in the new image and specify the target size to be the same as our training images</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>new_img_pil_small <span class="op">=</span> load_img(new_img_path, target_size<span class="op">=</span>(<span class="dv">32</span>,<span class="dv">32</span>))</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># confirm the data class and shape</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The new image is still of type:'</span>, new_img_pil_small.__class__, <span class="st">'but now has the same size'</span>, new_img_pil_small.size, <span class="st">'as our training data'</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The new image is still of type: &lt;class 'PIL.Image.Image'&gt; but now has the same size (32, 32) as our training data.</code></pre>
</div>
</div>
<div class="section level3">
<h3 id="normalization">Normalization<a class="anchor" aria-label="anchor" href="#normalization"></a></h3>
<p>Image RGB values are between 0 and 255. As input for neural networks,
it is better to have small input values. The process of converting the
RGB values to be between 0 and 1 is called
<strong>normalization</strong>.</p>
<div id="callout4" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout4"></a>
</h3>
<div class="callout-content">
<p>ChatGPT</p>
<p>Normalizing the RGB values to be between 0 and 1 is a common
pre-processing step in machine learning tasks, especially when dealing
with image data. This normalization has several benefits:</p>
<ol style="list-style-type: decimal"><li><p><strong>Numerical Stability</strong>: By scaling the RGB values
to a range between 0 and 1, you avoid potential numerical instability
issues that can arise when working with large values. Neural networks
and many other machine learning algorithms are sensitive to the scale of
input features, and normalizing helps to keep the values within a
manageable range.</p></li>
<li><p><strong>Faster Convergence</strong>: Normalizing the RGB values
often helps in faster convergence during the training process. Neural
networks and other optimization algorithms rely on gradient descent
techniques, and having inputs in a consistent range aids in smoother and
faster convergence.</p></li>
<li><p><strong>Equal Weightage for All Channels</strong>: In RGB images,
each channel (Red, Green, Blue) represents different color intensities.
By normalizing to the range [0, 1], you ensure that each channel is
treated with equal weightage during training. This is important because
some machine learning algorithms could assign more importance to larger
values.</p></li>
<li><p><strong>Generalization</strong>: Normalization helps the model to
generalize better to unseen data. When the input features are in the
same range, the learned weights and biases can be more effectively
applied to new examples, making the model more robust.</p></li>
<li><p><strong>Compatibility</strong>: Many image-related libraries,
algorithms, and models expect pixel values to be in the range of [0, 1].
By normalizing the RGB values, you ensure compatibility and seamless
integration with these tools.</p></li>
</ol><p>The normalization process is typically done by dividing each RGB
value (ranging from 0 to 255) by 255, which scales the values to the
range [0, 1].</p>
<p>For example, if you have an RGB image with pixel values (100, 150,
200), after normalization, the pixel values would become (100/255,
150/255, 200/255) ≈ (0.39, 0.59, 0.78).</p>
<p>Remember that normalization is not always mandatory, and there could
be cases where other scaling techniques might be more suitable based on
the specific problem and data distribution. However, for most
image-related tasks in machine learning, normalizing RGB values to [0,
1] is a good starting point.</p>
</div>
</div>
</div>
<p>Before we can normalize our image values we must convert the image to
an numpy array.</p>
<p>We saw how to do this in the introduction but what you may not have
noticed is that the <code>keras.datasets.cifar10.load_data</code>
function did the conversion for us whereas now we will do it
ourselves.</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># convert the Image into an array for normalization</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>new_img_arr <span class="op">=</span> img_to_array(new_img_pil_small)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># confirm the data class and shape</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The new image is now of type :'</span>, new_img_arr.__class__, <span class="st">'and has the shape'</span>, new_img_arr.shape)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The new image is now of type : &lt;class 'numpy.ndarray'&gt; and has the shape (32, 32, 3)</code></pre>
</div>
<p>Now we can normalize the values. Let us also investigate the image
values before and after we normalize them.</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># extract the min, max, and mean pixel values</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The min, max, and mean pixel values are'</span>, new_img_arr.<span class="bu">min</span>(), <span class="st">','</span>, new_img_arr.<span class="bu">max</span>(), <span class="st">', and'</span>, new_img_arr.mean().<span class="bu">round</span>(), <span class="st">'respectively.'</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># normalize the RGB values to be between 0 and 1</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>new_img_arr_norm <span class="op">=</span> new_img_arr <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># extract the min, max, and mean pixel values AFTER</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'After normalization, the min, max, and mean pixel values are'</span>, new_img_arr_norm.<span class="bu">min</span>(), <span class="st">','</span>, new_img_arr_norm.<span class="bu">max</span>(), <span class="st">', and'</span>, new_img_arr_norm.mean().<span class="bu">round</span>(), <span class="st">'respectively.'</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The min, max, and mean pixel values are 0.0 , 255.0 , and 87.0 respectively.
After normalization, the min, max, and mean pixel values are 0.0 , 1.0 , and 0.0 respectively.</code></pre>
</div>
<p>Of course, if there are a large number of images to prepare you do
not want to copy and paste these steps for each image we have in our
dataset.</p>
<p>Here we will use <code>for</code> loops to find all the images in a
directory and prepare them to create a test dataset that aligns with our
training dataset.</p>
</div>
</section><section id="cinic-10-test-dataset-preparation"><h2 class="section-heading">CINIC-10 Test Dataset Preparation<a class="anchor" aria-label="anchor" href="#cinic-10-test-dataset-preparation"></a>
</h2>
<hr class="half-width"><p>Our test dataset is a sample of images from an existing image dataset
known as <a href="https://github.com/BayesWatch/cinic-10/" class="external-link">CINIC-10</a>
(CINIC-10 Is Not ImageNet or CIFAR-10) that was designed to be used as a
drop-in alternative to the CIFAR-10 dataset we used in the
introduction.</p>
<p>The test image directory was set up to have the following
structure:</p>
<pre><code><span><span class="va">main_directory</span><span class="op">/</span></span>
<span><span class="va">...class_a</span><span class="op">/</span></span>
<span><span class="va">......image_1.jpg</span></span>
<span><span class="va">......image_2.jpg</span></span>
<span><span class="va">...class_b</span><span class="op">/</span></span>
<span><span class="va">......image_1.jpg</span></span>
<span><span class="va">......image_2.jpg</span></span></code></pre>
<p>We will use this structure to create two lists: one for images in
those folders and one for the image label. We can then use the lists to
resize, convert to arrays, and normalize the images.</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># set the mian directory  </span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>test_image_dir <span class="op">=</span> <span class="st">'D:/20230724_CINIC10/test_images'</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co"># make two lists of the subfolders (ie class or label) and filenames</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>test_filenames <span class="op">=</span> []</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>test_labels <span class="op">=</span> []</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> dn <span class="kw">in</span> os.listdir(test_image_dir):</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> fn <span class="kw">in</span> os.listdir(os.path.join(test_image_dir, dn)):</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>        test_filenames.append(fn)</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>        test_labels.append(dn)</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="co"># prepare the images</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="co"># create an empty numpy array to hold the processed images</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>test_images <span class="op">=</span> np.empty((<span class="bu">len</span>(test_filenames), <span class="dv">32</span>, <span class="dv">32</span>, <span class="dv">3</span>), dtype<span class="op">=</span>np.float32)</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="co"># use the dirnames and filenanes to process each </span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(test_filenames)):</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># set the path to the image</span></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    img_path <span class="op">=</span> os.path.join(test_image_dir, test_labels[i], test_filenames[i])</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># load the image and resize at the same time</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> load_img(img_path, target_size<span class="op">=</span>(<span class="dv">32</span>,<span class="dv">32</span>))</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># convert to an array</span></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>    img_arr <span class="op">=</span> img_to_array(img)</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># normalize</span></span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>    test_images[i] <span class="op">=</span> img_arr<span class="op">/</span><span class="fl">255.0</span></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(test_images.shape)</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(test_images.__class__)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(10000, 32, 32, 3)
&lt;class 'numpy.ndarray'&gt;</code></pre>
</div>
<div id="challenge1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge1"></a>
</h3>
<div class="callout-content">
<p>Training and Test sets</p>
<p>Take a look at the training and test set we created.</p>
<p>Q1. How many samples does the training set have and are the classes
well balanced?</p>
<p>Q2. How many samples does the test set have and are the classes well
balanced?</p>
<p>Hint1: Check the object class to understand what methods are
available.</p>
<p>Hint2: Use the `train_labels’ object to find out if the classes are
well balanced.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">
  Show me the solution
  </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>Q1. Training Set</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The training set is of type'</span>, train_images.__class__)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The training set has'</span>, train_images.shape[<span class="dv">0</span>] <span class="st">'samples.</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The number of labels in our training set and the number images in each class are:</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.unique(train_labels, return_counts<span class="op">=</span><span class="va">True</span>))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The training set is of type &lt;class 'numpy.ndarray'&gt;
The training set has 50000 samples.

The number of labels in our training set and the number images in each class are:

(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),
 array([5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000], dtype=int64))</code></pre>
</div>
<p>Q2. Test Set (we can use the same code as the training set)</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The test set is of type'</span>, test_images.__class__)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The test set has'</span>, test.shape[<span class="dv">0</span>] <span class="st">'samples.</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The number of labels in our test set and the number images in each class are:</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.unique(test_labels, return_counts<span class="op">=</span><span class="va">True</span>))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The test set is of type &lt;class 'numpy.ndarray'&gt;
The test set has 10000 samples.

The number of labels in our test set and the number images in each class are:

(array(['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog',
       'horse', 'ship', 'truck'], dtype='&lt;U10'), array([1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000],
      dtype=int64))</code></pre>
</div>
</div>
</div>
</div>
</div>
<p>TODO lables are strings, need to be numeric for predict; here or ep
05?</p>
<p>There are other preprocessing steps you might need to take for your
particular problem. We will discuss a few common ones briefly before
getting back to our model.</p>
<div class="section level3">
<h3 id="data-splitting">Data Splitting<a class="anchor" aria-label="anchor" href="#data-splitting"></a></h3>
<p>In the previous episode we saw that the keras installation includes
the Cifar-10 dataset and that by using the ‘cifar10.load_data()’ method
the returned data is split into two (train and validations sets). There
was not a test dataset which is why we just created one.</p>
<p>When using a different dataset, or loading your own set of images,
you may need to do the splitting yourself.</p>
<div id="callout5" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout5"></a>
</h3>
<div class="callout-content">
<p>ChatGPT</p>
<p>Data Splitting Techniques</p>
<p>Data is typically split into the training, validation, and test data
sets using a process called data splitting or data partitioning. There
are various methods to perform this split, and the choice of technique
depends on the specific problem, dataset size, and the nature of the
data. Here are some common approaches:</p>
<p><strong>Hold-Out Method:</strong></p>
<ul><li><p>In the hold-out method, the dataset is divided into two parts
initially: a training set and a test set.</p></li>
<li><p>The training set is used to train the model, and the test set is
kept completely separate to evaluate the model’s final
performance.</p></li>
<li><p>This method is straightforward and widely used when the dataset
is sufficiently large.</p></li>
</ul><p><strong>Train-Validation-Test Split:</strong></p>
<ul><li><p>The dataset is split into three parts: the training set, the
validation set, and the test set.</p></li>
<li><p>The training set is used to train the model, the validation set
is used to tune hyperparameters and prevent overfitting during training,
and the test set is used to assess the final model performance.</p></li>
<li><p>This method is commonly used when fine-tuning model
hyperparameters is necessary.</p></li>
</ul><p><strong>K-Fold Cross-Validation:</strong></p>
<ul><li><p>In k-fold cross-validation, the dataset is divided into k subsets
(folds) of roughly equal size.</p></li>
<li><p>The model is trained and evaluated k times, each time using a
different fold as the test set while the remaining k-1 folds are used as
the training set.</p></li>
<li><p>The final performance metric is calculated as the average of the
k evaluation results, providing a more robust estimate of model
performance.</p></li>
<li><p>This method is particularly useful when the dataset size is
limited, and it helps in better utilizing available data.</p></li>
</ul><p><strong>Stratified Sampling:</strong></p>
<ul><li><p>Stratified sampling is used when the dataset is imbalanced,
meaning some classes or categories are underrepresented.</p></li>
<li><p>The data is split in such a way that each subset (training,
validation, or test) maintains the same class distribution as the
original dataset.</p></li>
<li><p>This ensures that all classes are well-represented in each
subset, which is important to avoid biased model evaluation.</p></li>
</ul><p>It’s important to note that the exact split ratios (e.g., 80-10-10 or
70-15-15) may vary depending on the problem, dataset size, and specific
requirements. Additionally, data splitting should be performed randomly
to avoid introducing any biases into the model training and evaluation
process.</p>
</div>
</div>
</div>
<div id="challenge2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge2"></a>
</h3>
<div class="callout-content">
<p>Data Splitting Example</p>
<p>To split a dataset into training and test sets there is a very
convenient function from sklearn called <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" class="external-link">train_test_split</a>.</p>
<p><code>train_test_split</code> takes a number of parameters:</p>
<p><code>sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)</code></p>
<p>Take a look at the help and write a function to split an imaginary
dataset into a train/test split of 80/20 using stratified sampling.</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">
  Show me the solution
  </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p>Noting there are a couple ways to do this, here is one example:</p>
<pre><code>from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(image_dataset, target, test_size=0.2, random_state=42, shuffle=True, stratify=target)</code></pre>
<ul><li>The first two parameters are the dataset (X) and the corresponding
targets (y) (i.e. class labels)</li>
<li>Next is the named parameter <code>test_size</code> this is the
fraction of the dataset that is used for testing, in this case
<code>0.2</code> means 20% of the data will be used for testing.</li>
<li>
<code>random_state</code> controls the shuffling of the dataset,
setting this value will reproduce the same results (assuming you give
the same integer) every time it is called.</li>
<li>
<code>shuffle</code> which can be either <code>True</code> or
<code>False</code>, it controls whether the order of the rows of the
dataset is shuffled before splitting. It defaults to
<code>True</code>.</li>
<li>
<code>stratify</code> is a more advanced parameter that controls how
the split is done. By setting it to <code>target</code> the train and
test sets the function will return will have roughly the same
proportions (with regards to the number of images of a certain class) as
the dataset.</li>
</ul></div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="image-colours">Image Colours<a class="anchor" aria-label="anchor" href="#image-colours"></a></h3>
<p><strong>RGB</strong> Images:</p>
<ul><li><p>For image classification tasks, RGB images are used because they
capture the full spectrum of colors that human vision can perceive,
allowing the model to learn intricate features and patterns present in
the images.</p></li>
<li><p>RGB (Red, Green, Blue) images have three color channels: red,
green, and blue, with each channel having an intensity value that ranges
from 0 to 255. Each channel represents the intensity of the
corresponding color for each pixel. This results in a 3D array, where
the dimensions are height, width, and color channel.</p></li>
</ul><p>While RGB is the most common representation, there are scenarios
where other color palettes might be considered, such as:</p>
<p><strong>Grayscale</strong> Images:</p>
<ul><li><p>Grayscale images have only one channel, representing the
intensity of the pixels. Each pixel’s intensity is usually represented
by a single numerical value that ranges from 0 (black) to 255 (white).
The image is essentially a 2D array where each element holds the
intensity value of the corresponding pixel.</p></li>
<li><p>In cases where color information isn’t critical, you might
convert RGB images to grayscale to reduce the computational
load.</p></li>
</ul></div>
<div class="section level3">
<h3 id="one-hot-encoding">One-hot encoding<a class="anchor" aria-label="anchor" href="#one-hot-encoding"></a></h3>
<p>A neural network can only take numerical inputs and outputs, and
learns by calculating how “far away” the class predicted by the neural
network is from the true class. When the target (label) is a categorical
data, or strings, it is very difficult to determine this “distance” or
error. Therefore we will transform this column into a more suitable
format. There are many ways to do this, however we will be using
<strong>one-hot encoding</strong>.</p>
<p>One-hot encoding is a technique to represent categorical data as
binary vectors, making it compatible with machine learning algorithms.
Each category becomes a separate feature, and the presence or absence of
a category is indicated by 1s and 0s in the respective columns.</p>
<p>Let’s say you have a dataset with a “Color” column containing three
categories: Red, Blue, Green.</p>
<p>Table 1. Original Data.</p>
<table class="table"><thead><tr class="header"><th>color</th>
<th align="right"></th>
</tr></thead><tbody><tr class="odd"><td>red</td>
<td align="right"><span class="emoji" data-emoji="red_square">🟥</span></td>
</tr><tr class="even"><td>green</td>
<td align="right"><span class="emoji" data-emoji="green_square">🟩</span></td>
</tr><tr class="odd"><td>blue</td>
<td align="right"><span class="emoji" data-emoji="blue_square">🟦</span></td>
</tr><tr class="even"><td>red</td>
<td align="right"><span class="emoji" data-emoji="red_square">🟥</span></td>
</tr></tbody></table><p>Table 2. After One-Hot Encoding.</p>
<table class="table"><thead><tr class="header"><th>Color_red</th>
<th align="center">Color_blue</th>
<th align="right">Color_green</th>
</tr></thead><tbody><tr class="odd"><td>1</td>
<td align="center">0</td>
<td align="right">0</td>
</tr><tr class="even"><td>0</td>
<td align="center">1</td>
<td align="right">0</td>
</tr><tr class="odd"><td>0</td>
<td align="center">0</td>
<td align="right">1</td>
</tr><tr class="even"><td>1</td>
<td align="center">0</td>
<td align="right">0</td>
</tr></tbody></table><p>Each category has its own binary column, and the value is set to 1 in
the corresponding column for each row that matches that category.</p>
<p>Our image data is not one-hot encoded because image data is
continuous and typically represented as arrays of pixel values. While in
some cases you may want to one-hot encode the labels, here we are using
integer labels (0,1,2) instead. Many machine learning libraries and
frameworks handle this label encoding internally.</p>
</div>
<div class="section level3">
<h3 id="image-augmentation">Image augmentation<a class="anchor" aria-label="anchor" href="#image-augmentation"></a></h3>
<p>There are several ways to augment your data to increase the diversity
of the training data and improve model robustness.</p>
<ul><li>Geometric Transformations
<ul><li>rotation, translation, scaling, zooming, cropping</li>
</ul></li>
<li>Flipping or Mirroring
<ul><li>some classes, like horse, have a different shape when facing left or
right and you want your model to recognize both</li>
</ul></li>
<li>Color properties
<ul><li>brightness, contrast, or hue</li>
<li>these changes simulate variations in lighting conditions</li>
</ul></li>
</ul><div class="section level4">
<h4 id="finally">Finally!<a class="anchor" aria-label="anchor" href="#finally"></a></h4>
<p>Our test dataset is ready to go and we can move on to how to build an
architecture.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Keypoints<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul><li>Image datasets can be found online or created uniquely for your
research question</li>
<li>Images consist of pixels arranged in a particular order</li>
<li>Image data is usually preprocessed before use in a CNN for
efficiency, consistency, and robustness</li>
</ul></div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</div>
</div>
</section></div> <!-- / div.lesson-content -->
    </main><!-- / main#main-content.main-content --><nav class="bottom-pagination mx-md-4" aria-label="Previous and Next Chapter"><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/01-introduction.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/03-build-cnn.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/01-introduction.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Introduction to Deep
        </a>
        <a class="chapter-link float-end" href="../instructor/03-build-cnn.html" rel="next">
          Next: Build a... 
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
    </nav></div> <!-- / div.primary-content.col-xs-12 -->
<!-- END:   inst/pkgdown/templates/content-instructor.html-->

      </div><!--/div.row-->
      		<footer class="row footer mx-md-3"><hr><div class="col-md-6">
				<p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>
        
        <a href="https://https://github.com/erinmgraham/icwithcnn/edit/main/episodes/02-image-data.html" class="external-link">Edit on GitHub</a>
        
	
        | <a href="https://https://github.com/erinmgraham/icwithcnn/blob/main/CONTRIBUTING.html" class="external-link">Contributing</a> 
        | <a href="https://https://github.com/erinmgraham/icwithcnn/" class="external-link">Source</a></p>
				<p><a href="https://https://github.com/erinmgraham/icwithcnn/blob/main/CITATION" class="external-link">Cite</a> | <a href="mailto:erin.graham@jcu.edu.au">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">
        
        <p>Materials licensed under <a href="../LICENSE.html">CC-BY 4.0</a> by the authors</p>
        
        <p><a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">Template licensed under CC-BY 4.0</a> by <a href="https://carpentries.org" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.14.0" class="external-link">sandpaper (0.14.0)</a>,
        <a href="https://github.com/carpentries/pegboard/tree/0.7.1" class="external-link">pegboard (0.7.1)</a>,
      and <a href="https://github.com/carpentries/varnish/tree/0.3.0" class="external-link">varnish (0.3.0)</a>.</p>
			</div>
		</footer></div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
			<i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back to top"></i><br><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://.github.io/github.com/instructor/02-image-data.html",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries",
  "name": "Introduction to Image Data",
  "creativeWorkStatus": "active",
  "url": "https://.github.io/github.com/instructor/02-image-data.html",
  "identifier": "https://.github.io/github.com/instructor/02-image-data.html",
  "dateCreated": "2023-05-03",
  "dateModified": "2023-10-06",
  "datePublished": "2023-10-06"
}

  </script><script>
		feather.replace();
	</script><!-- Matomo
    2022-11-07: we have gotten a notification that we have an overage for our
    tracking and I'm pretty sure this has to do with Workbench usage.
    Considering that I am not _currently_ using this tracking because I do not
    yet know how to access the data, I am turning this off for now.
  <script>
    var _paq = window._paq = window._paq || [];
    /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
    _paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
    _paq.push(["setDomains", ["*.preview.carpentries.org","*.datacarpentry.github.io","*.datacarpentry.org","*.librarycarpentry.github.io","*.librarycarpentry.org","*.swcarpentry.github.io", "*.carpentries.github.io"]]);
    _paq.push(["setDoNotTrack", true]);
    _paq.push(["disableCookies"]);
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
          var u="https://carpentries.matomo.cloud/";
          _paq.push(['setTrackerUrl', u+'matomo.php']);
          _paq.push(['setSiteId', '1']);
          var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
          g.async=true; g.src='https://cdn.matomo.cloud/carpentries.matomo.cloud/matomo.js'; s.parentNode.insertBefore(g,s);
        })();
  </script>
  End Matomo Code --></body></html><!-- END:   inst/pkgdown/templates/layout.html-->

