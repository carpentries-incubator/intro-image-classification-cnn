<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>Image Classification with Convolutional Neural Networks: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="assets/styles.css">
<script src="assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="site.webmanifest">
<link rel="mask-icon" href="safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md navbar-light bg-white top-nav incubator"><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-6">
      <div class="large-logo">
        <img alt="Carpentries Incubator" src="assets/images/incubator-logo.svg"><abbr class="badge badge-light" title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught." style="background-color: #FF4955; border-radius: 5px">
          <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#early-development-pre-alpha-through-alpha" class="external-link alert-link" style="color: #000">
            <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
            Pre-Alpha
          </a>
          <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
        </abbr>
        
      </div>
    </div>
    <div class="selector-container">
      
      
      <div class="dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Learner View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='instructor/aio.html';">Instructor View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl navbar-light bg-white bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="assets/images/incubator-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      Image Classification with Convolutional Neural Networks
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="search button" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            Image Classification with Convolutional Neural Networks
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="reference.html#glossary">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="profiles.html">Learner Profiles</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
<li><a class="dropdown-item" href="reference.html">Reference</a></li>
          </ul>
</li>
      </ul>
</div>
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
<input class="form-control me-2 searchbox" type="search" placeholder="Search" aria-label="Search"><button class="btn btn-outline-success tablet-search-button" type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="search button"></i>
        </button>
      </fieldset>
</form>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Image Classification with Convolutional Neural Networks
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Learner View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="instructor/aio.html">Instructor View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->
      
            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Setup</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="setup-gpu.html">1. Setup - GPU</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="01-introduction.html">2. Introduction to Deep Learning</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="02-image-data.html">3. Introduction to Image Data</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="03-build-cnn.html">4. Build a Convolutional Neural Network</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="04-fit-cnn.html">5. Compile and Train (Fit) a Convolutional Neural Network</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="05-evaluate-predict-cnn.html">6. Evaluate a Convolutional Neural Network and Make Predictions (Classifications)</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush8">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading8">
        <a href="06-conclusion.html">7. Conclusion</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="reference.html#glossary">Glossary</a>
                      </li>
                      <li>
                        <a href="profiles.html">Learner Profiles</a>
                      </li>
                      <li><a href="reference.html">Reference</a></li>
                    </ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width resources">
<a href="aio.html">See all in one page</a>
            

            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">
            
            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-setup-gpu"><p>Content from <a href="setup-gpu.html">Setup - GPU</a></p>
<hr>
<p>Last updated on 2024-02-23 |
        
        <a href="https://https://github.com/erinmgraham/icwithcnn/edit/main/episodes/setup-gpu.html" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<p>IMPORTANT NOTICE: IF YOU HAVE ALREADY CREATED A CPU ENVIRONMENT YOU
DO NOT NEED TO DO ANYTHING ON THIS PAGE. CONTINUE TO THE INTRODUCTION
EPISODE.</p>
<p>This lesson is designed for Software Carpentry users who have
completed <a href="https://swcarpentry.github.io/python-novice-gapminder/" class="external-link">Plotting
and Programming in Python</a> and want to jump straight into image
classification. We recognize this jump is quite large and have done our
best to provide the content and code to perform these types of
analyses.</p>
<p>The default <a href="index.html#setup">Setup</a> is for CPU only
environments.</p>
<p>These instructions are for setting up tensorflow in a
<strong>GPU</strong> capable environment. Because this is a more
advanced topic and installation varies depending on your computer’s
architecture, please make sure you <strong>set up and test</strong> your
installation before the workshop begins. We will not be able to spend
class time assisting on GPU setups.</p>
<section id="software-setup"><h2 class="section-heading">Software Setup<a class="anchor" aria-label="anchor" href="#software-setup"></a>
</h2>
<hr class="half-width">
<div id="install-python-using-anaconda" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="install-python-using-anaconda" class="callout-inner">
<h3 class="callout-title">Install Python using Anaconda<a class="anchor" aria-label="anchor" href="#install-python-using-anaconda"></a>
</h3>
<div class="callout-content">
<p><a href="https://python.org" class="external-link">Python</a> is a popular language for
scientific computing, and a frequent choice for machine learning as
well. Installing all of its scientific packages individually can be a
bit difficult, however, so we recommend the installer <a href="https://www.anaconda.com/products/individual" class="external-link">Anaconda</a> which
includes most (but not all) of the software you need. Make sure you
install the latest Python version 3.xx.</p>
<p>Also, please set up your python environment <strong>at least</strong>
a day in advance of the workshop. If you encounter problems with the
installation procedure <em>for Anaconda</em>, ask your workshop
organizers via e-mail for assistance so you are ready to go as soon as
the workshop begins.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Windows</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>Check out the <a href="https://www.youtube.com/watch?v=xxQ0mzZ8UvA" class="external-link">Windows - Video
tutorial</a> or:</p>
<ol style="list-style-type: decimal">
<li><p>Open [<a href="https://www.anaconda.com/products/distribution" class="external-link uri">https://www.anaconda.com/products/distribution</a>] with
your web browser.</p></li>
<li><p>Download the Python 3 installer for Windows.</p></li>
<li><p>Double-click the executable and install Python 3 using
<em>MOST</em> of the default settings. The only exception is to check
the <strong>Make Anaconda the default Python</strong> option. (Note this
may already be checked.)</p></li>
</ol>
</div>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">MacOS</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p>Check out the <a href="https://www.youtube.com/watch?v=TcSAln46u9U" class="external-link">Mac OS X - Video
tutorial</a> or:</p>
<ol style="list-style-type: decimal">
<li><p>Open [<a href="https://www.anaconda.com/products/distribution" class="external-link uri">https://www.anaconda.com/products/distribution</a>] with
your web browser.</p></li>
<li><p>Download the Python 3 installer for Mac.</p></li>
<li><p>Install Python 3 using all of the defaults for
installation.</p></li>
</ol>
</div>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Linux</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<p>Note the following installation steps require you to work from the
shell. If you run into any difficulties, please request help before the
workshop begins.</p>
<ol style="list-style-type: decimal">
<li><p>Open [<a href="https://www.anaconda.com/products/distribution" class="external-link uri">https://www.anaconda.com/products/distribution</a>] with
your web browser.</p></li>
<li><p>Download the Python 3 installer for Linux.</p></li>
<li>
<p>Install Python 3 using all of the defaults for installation.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Open a terminal window.</p></li>
<li><p>Navigate to the folder where you downloaded the
installer</p></li>
<li><p>Type</p></li>
</ol>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">bash</span> Anaconda3-</span></code></pre>
</div>
<p>and press tab. The name of the file you just downloaded should
appear.</p>
<ol start="4" style="list-style-type: lower-alpha">
<li><p>Press enter.</p></li>
<li><p>Follow the text-only prompts. When the license agreement appears
(a colon will be present at the bottom of the screen) hold the down
arrow until the bottom of the text. Type <code>yes</code> and press
enter to approve the license. Press enter again to approve the default
location for the files. Type <code>yes</code> and press enter to prepend
Anaconda to your <code>PATH</code> (this makes the Anaconda distribution
the default Python).</p></li>
</ol>
</li>
</ol>
</div>
</div>
</div>
</div>
</section><section id="install-the-required-packages"><h2 class="section-heading">Install the required packages<a class="anchor" aria-label="anchor" href="#install-the-required-packages"></a>
</h2>
<hr class="half-width">
<p><a href="https://docs.conda.io/projects/conda/en/latest/" class="external-link">Conda</a>
is the package management system associated with <a href="https://www.anaconda.com/products/individual" class="external-link">Anaconda</a> and
runs on Windows, macOS and Linux.</p>
<p>Conda should already be available in your system once you installed
Anaconda successfully. Conda thus works regardless of the operating
system. Make sure you have an up-to-date version of Conda running. See
<a href="https://docs.anaconda.com/anaconda/install/update-version/" class="external-link">these
instructions</a> for updating Conda if required.</p>
<p>The easiest way to create a conda environment for this lesson is to
use the Anaconda Prompt. You can search for “anaconda prompt” using the
Windows search function (Windows Logo Key) or Spotlight on macOS
(Command + spacebar).</p>
<figure><img src="fig/00_anaconda_prompt_search.png" alt="Screenshot of the Anaconda Prompt application" class="figure mx-auto d-block"></figure><p>A terminal window will open with the title ‘Anaconda Prompt’:</p>
<figure><img src="fig/00_anaconda_prompt_window.png" alt="Screenshot of the terminal window that opens when you launch the Anaconda Prompt application" class="figure mx-auto d-block"></figure><p>Note the notation of the prompt inside the terminal window. The name
inside the parentheses refers to which conda environment you are working
inside of, and ‘base’ is the name given to the default environment
included with every Anaconda distribution.</p>
<p>To create a new environment for this lesson, the command starts with
the conda keywords <code>conda create</code>. This command can be
followed by a name for the new environment and the package(s) to install
but to make things easier, inside the script download folder, we have
given you an environment.yml file to use instead. (See download link
below if you haven’t already.)</p>
<pre class="code"><code>(base) C:\Users\Lab&gt; conda env create --file cnn_workshop_gpu_environment.yml</code></pre>
<p>If the yml is not in your current directory, you can specify the full
path to the file, eg:</p>
<pre class="code"><code>(base) C:\Users\Lab&gt; conda env create --file C:\Users\Lab\intro-image-classification-cnn\files\cnn_workshop_gpu_environment.yml</code></pre>
<p>Be patient because it might take a while (15-20 min) for conda to
work out all of the dependencies.</p>
<p>After the environment is created we tell Anaconda to use the new
environment with the conda keywords <code>conda activate</code> followed
by the environment name:</p>
<pre class="code"><code>(base) C:\Users\Lab&gt; conda activate cnn_workshop_gpu
(cnn_workshop_gpu) C:\Users\Lab&gt;</code></pre>
<p>You will know you are in the right environment because the prompt
changes from (base) to (cnn_workshop_gpu).</p>
<div id="callout1" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout1"></a>
</h3>
<div class="callout-content">
<p>To set up a GPU environment, make sure you have the appropriate
hardware, system, and software necessary for GPU support. Here we are
following the <a href="https://www.tensorflow.org/install/pip#windows-native_1" class="external-link">Windows
TensorFlow installation instructions</a> starting at <strong>Step 5. GPU
setup</strong>. Specific instructions can also be found there for <a href="https://www.tensorflow.org/install/pip#macos_1" class="external-link">MacOS</a> and <a href="https://www.tensorflow.org/install/pip#linux_1" class="external-link">Linux</a>
environments.</p>
</div>
</div>
</div>
<div class="section level3">
<h3 id="nvidia-gpu">NVIDIA GPU<a class="anchor" aria-label="anchor" href="#nvidia-gpu"></a>
</h3>
<p>First install NVIDIA GPU driver [<a href="https://www.nvidia.com/download/index.aspx" class="external-link uri">https://www.nvidia.com/download/index.aspx</a>] if you have
not.</p>
<p>Then install the CUDA, cuDNN with conda.</p>
<pre class="code"><code>(cnn_workshop_gpu) C:\Users\Lab&gt; conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0</code></pre>
<div id="accordionSpoiler1" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler1" aria-expanded="false" aria-controls="collapseSpoiler1">
  <h3 class="accordion-header" id="headingSpoiler1">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>AMD GPU</h3>
</button>
<div id="collapseSpoiler1" class="accordion-collapse collapse" aria-labelledby="headingSpoiler1" data-bs-parent="#accordionSpoiler1">
<div class="accordion-body">
<p>First install AMD GPU driver [<a href="https://www.amd.com/en/support" class="external-link uri">https://www.amd.com/en/support</a>] if you have not.</p>
<p>TODO Finish these instructions</p>
</div>
</div>
</div>
</div>
</div>
</section><section id="start-spyder"><h2 class="section-heading">Start Spyder<a class="anchor" aria-label="anchor" href="#start-spyder"></a>
</h2>
<hr class="half-width">
<p>We teach this lesson using Python in <a href="https://www.spyder-ide.org/" class="external-link">Spyder</a> (Scientific Python
Development Environment), a free integrated development environment
(IDE) included with Anaconda. Editing, interactive testing, debugging,
and introspection tools are all included in Spyder.</p>
<p>To start Spyder, type the command <code>spyder</code>, making sure
you are still in the workshop environment:</p>
<pre class="conda"><code>(cnn_workshop) C:\Users\Lab&gt; spyder</code></pre>
<figure><img src="fig/00_spyder_ide_layout.png" alt="Screenshot of the Spyder IDE annotated with boxes and labels for the Editor; Help, Variable Explorer, Plots, Files; and IPython Console areas" class="figure mx-auto d-block"></figure></section><section id="check-your-setup"><h2 class="section-heading">Check your setup<a class="anchor" aria-label="anchor" href="#check-your-setup"></a>
</h2>
<hr class="half-width">
<p>To check whether all packages installed correctly, go to the
interactive <code>IPython Console</code> in Spyder (lower right hand
side panel) and type in the following commands:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'matplotlib version: '</span>, matplotlib.__version__)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'numpy version: '</span>, numpy.__version__)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'pandas version: '</span>, pandas.__version__)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'seaborn version: '</span>, seaborn.__version__)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'sklearn version: '</span>, sklearn.__version__)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scikeras</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'scikeras version: '</span>, scikeras.__version__)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Tensorflow version: '</span>, tensorflow.__version__)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Num GPUs Available: "</span>, <span class="bu">len</span>(tensorflow.config.experimental.list_physical_devices(<span class="st">'GPU'</span>)))</span></code></pre>
</div>
<p>This should output the versions of all required packages without
giving errors and well as the number of GPU’s tensorflow has
detected.</p>
</section><section id="download-the-scripts-files-and-model-outputs"><h2 class="section-heading">Download the scripts, files and model outputs<a class="anchor" aria-label="anchor" href="#download-the-scripts-files-and-model-outputs"></a>
</h2>
<hr class="half-width">
<p>Download the <a href="https://drive.google.com/file/d/1SpcusVYomhukFKWuUcK7LwF7RtrKB8Z_/view?usp=drive_link" class="external-link">scripts,
files, and model outputs</a>.</p>
</section><section id="get-the-data"><h2 class="section-heading">Get the data<a class="anchor" aria-label="anchor" href="#get-the-data"></a>
</h2>
<hr class="half-width">
<p>This lesson uses the CIFAR-10 image data that comes prepackaged with
Keras.</p>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-01-introduction"><p>Content from <a href="01-introduction.html">Introduction to Deep Learning</a></p>
<hr>
<p>Last updated on 2024-02-25 |
        
        <a href="https://https://github.com/erinmgraham/icwithcnn/edit/main/episodes/01-introduction.html" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is machine learning and what is it used for?</li>
<li>What is deep learning?</li>
<li>How do I use a neural network for image classification?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Explain the difference between artificial intelligence, machine
learning and deep learning.</li>
<li>Understand the different types of computer vision tasks.</li>
<li>Perform an image classification using a convolutional neural network
(CNN).</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section id="what-is-machine-learning"><h2 class="section-heading">What is machine learning?<a class="anchor" aria-label="anchor" href="#what-is-machine-learning"></a>
</h2>
<hr class="half-width">
<p>Machine learning is a set of tools and techniques which let us find
patterns in data. This lesson will introduce you to only one of these
techniques, <strong>Deep Learning</strong> with <strong>Convolutional
Neural Network</strong>, abbreviated as <strong>CNN</strong>, but there
are many more.</p>
<p>The techniques break down into two broad categories, predictors and
classifiers. Predictors are used to predict a value (or set of values)
given a set of inputs whereas classifiers try to classify data into
different categories, or assign a labelcond env.</p>
<p>Many, but not all, machine learning systems “learn” by taking a
series of input data and output data and using it to form a model. The
maths behind the machine learning doesn’t care what the data is as long
as it can represented numerically or categorised. Some examples might
include:</p>
<ul>
<li>Predicting a person’s weight based on their height.</li>
<li>Predicting house prices given stock market prices.</li>
<li>Classifying an email as spam or not.</li>
<li>Classifying an image as, e.g., a person, place, or particular
object.</li>
</ul>
<p>Typically we train our models with hundreds, thousands or even
millions of examples before they work well enough to do any useful
predictions or classifications with them.</p>
</section><section id="deep-learning-machine-learning-and-artificial-intelligence"><h2 class="section-heading">Deep Learning, Machine Learning and Artificial Intelligence<a class="anchor" aria-label="anchor" href="#deep-learning-machine-learning-and-artificial-intelligence"></a>
</h2>
<hr class="half-width">
<p>Deep Learning (DL) is just one of many machine learning techniques,
in which people often talk about machine learning being a form of
artificial intelligence (AI). Definitions of artificial intelligence
vary, but usually involve having computers mimic the behaviour of
intelligent biological systems. Since the 1950s many works of science
fiction have dealt with the idea of an artificial intelligence which
matches, or exceeds, human intelligence in all areas. Although there
have been great advances in AI and ML research recently, we can only
come close to human like intelligence in a few specialist areas and are
still a long way from a general purpose AI. The image below illustrates
some differences between artificial intelligence, machine learning and
deep learning.</p>
<figure><img src="fig/01_AI_ML_DL_differences.png" alt="Three nested circles defining deep learning as a subset of machine learning which is a subset of artifical intelligence" class="figure mx-auto d-block"><figcaption>The image above is by Tukijaaliwa, CC BY-SA 4.0, via
Wikimedia Commons, <a href="https://en.wikipedia.org/wiki/File:AI-ML-DL.svg" class="external-link">original
source</a></figcaption></figure><div id="callout1" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout1"></a>
</h3>
<div class="callout-content">
<p>Concept: Differentiation between traditional Machine Learning models
and Deep Learning models:</p>
<p><strong>Traditional ML algorithms</strong>, known as shallow models,
are limited to just one or maybe two layers of data transformation to
generate an output. When dealing with complex data featuring high
dimensions and growing feature space (i.e. many attributes and an
expanding set of potential values for each feature), these shallow
models become limited in their ability to compute accurate outputs.</p>
<p><strong>Deep neural networks</strong> are the extension of shallow
models with three layers: input, hidden, and outputs layers. The hidden
layer(s) is where learning takes place. As a result, deep learning is
best applied to large datasets for training and prediction. As
observations and feature inputs decrease, shallow ML approaches begin to
perform noticeably better.</p>
</div>
</div>
</div>
</section><section id="what-is-image-classification"><h2 class="section-heading">What is image classification?<a class="anchor" aria-label="anchor" href="#what-is-image-classification"></a>
</h2>
<hr class="half-width">
<p>Image classification is a fundamental task in computer vision, which
is a field of artificial intelligence focused on teaching computers to
interpret and understand visual information from the world. Image
classification specifically involves the process of assigning a label or
category to an input image. The goal is to enable computers to recognise
and categorise objects, scenes, or patterns within images, just as a
human would. Image classification can refer to one of several tasks:</p>
<figure><img src="fig/01_Fei-Fei_Li_Justin_Johnson_Serena_Young__CS231N_2017.png" alt="Four types of image classification tasks include semantic segmentation to label every pixel; classification and localisation to detect a single object like a cat; object detection to detect multiple objects like cats and dogs; and instance segmentation to detect each pixel of multiple objects" class="figure mx-auto d-block"></figure><p>Image classification has numerous practical applications,
including:</p>
<ul>
<li>
<strong>Object Recognition</strong>: Identifying objects within
images, such as cars, animals, or household items.</li>
<li>
<strong>Medical Imaging</strong>: Diagnosing diseases from medical
images like X-rays or MRIs.</li>
<li>
<strong>Quality Control</strong>: Inspecting products for defects on
manufacturing lines.</li>
<li>
<strong>Autonomous Vehicles</strong>: Identifying pedestrians,
traffic signs, and other vehicles in self-driving cars.</li>
<li>
<strong>Security and Surveillance</strong>: Detecting anomalies or
unauthorised objects in security footage.</li>
</ul>
<p>A Convolutional Neural Networks (CNN) is a Deep Learning algorithm
that has become a cornerstone in image classification due to its ability
to automatically learn features from images in a hierarchical fashion
(i.e. each layer builds upon what was learned by the previous layer). It
can achieve remarkable performance on a wide range of tasks.</p>
</section><section id="deep-learning-workflow"><h2 class="section-heading">Deep Learning Workflow<a class="anchor" aria-label="anchor" href="#deep-learning-workflow"></a>
</h2>
<hr class="half-width">
<p>To apply Deep Learning to a problem there are several steps to go
through:</p>
<div class="section level3">
<h3 id="step-1--formulate-outline-the-problem">Step 1. Formulate / Outline the problem<a class="anchor" aria-label="anchor" href="#step-1--formulate-outline-the-problem"></a>
</h3>
<p>Firstly we must decide what it is we want our Deep Learning system to
do. This lesson is all about image classification so our aim is to put
an image into one of a few categories. Specifically in our case, we have
10 categories: airplane, automobile, bird, cat, deer, dog, frog, horse,
ship, truck</p>
</div>
<div class="section level3">
<h3 id="step-2--identify-inputs-and-outputs">Step 2. Identify inputs and outputs<a class="anchor" aria-label="anchor" href="#step-2--identify-inputs-and-outputs"></a>
</h3>
<p>Next identify what the inputs and outputs of the neural network will
be. In our case, the data is images and the inputs could be the
individual pixels of the images. We are performing a classification
problem and we will have one output for each potential class.</p>
</div>
<div class="section level3">
<h3 id="step-3--prepare-data">Step 3. Prepare data<a class="anchor" aria-label="anchor" href="#step-3--prepare-data"></a>
</h3>
<p>Many datasets are not ready for immediate use in a neural network and
will require some preparation. Neural networks can only really deal with
numerical data, so any non-numerical data (e.g., images) will have to be
somehow converted to numerical data. Information on how this is done and
the data structure will be explored in <a href="02-image-data">Episode
02 Introduction to Image Data</a>.</p>
<p>For this lesson, we will use an existing image dataset known as
CIFAR-10 (Canadian Institute for Advanced Research). We will introduce
this dataset and the different data preparation tasks in more detail in
the next episode but for this introduction, we want to divide the data
into <strong>training</strong>, <strong>validation</strong>, and
<strong>test</strong> subsets; normalise the image pixel values to be
between 0 and 1; and one-hot encode our image labels.</p>
<div class="section level4">
<h4 id="preparing-the-code">Preparing the code<a class="anchor" aria-label="anchor" href="#preparing-the-code"></a>
</h4>
<p>It is the goal of this training workshop to produce a Deep Learning
program, using a Convolutional Neural Network. At the end of this
workshop, we hope this code can be used as a “starting point”. We will
create an “initial program” for this introduction chapter that will be
copied and used as a foundation for the rest of the episodes.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load the required packages</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> keras <span class="co"># for neural networks </span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split <span class="co"># for splitting data into sets</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt <span class="co"># for plotting</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># load the CIFAR-10 dataset included with keras</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>(train_images, train_labels), (test_images, test_labels) <span class="op">=</span> keras.datasets.cifar10.load_data()</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># normalise the RGB values to be between 0 and 1</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>train_images <span class="op">=</span> train_images <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>test_images <span class="op">=</span> test_images <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># create a list of class names</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>class_names <span class="op">=</span> [<span class="st">'airplane'</span>, <span class="st">'automobile'</span>, <span class="st">'bird'</span>, <span class="st">'cat'</span>, <span class="st">'deer'</span>, <span class="st">'dog'</span>, <span class="st">'frog'</span>, <span class="st">'horse'</span>, <span class="st">'ship'</span>, <span class="st">'truck'</span>]</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># one-hot encode labels</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>train_labels <span class="op">=</span> keras.utils.to_categorical(train_labels, <span class="bu">len</span>(class_names))</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>val_labels <span class="op">=</span> keras.utils.to_categorical(val_labels, <span class="bu">len</span>(class_names))</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># split the training data into training and validation sets</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co"> the function is train_test_split() but we are using it to split train into train and validation</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>train_images, val_images, train_labels, val_labels <span class="op">=</span> train_test_split(train_images, train_labels, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span></code></pre>
</div>
<div id="challenge-examine-the-cifar-10-dataset" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-examine-the-cifar-10-dataset" class="callout-inner">
<h3 class="callout-title">CHALLENGE Examine the CIFAR-10 dataset<a class="anchor" aria-label="anchor" href="#challenge-examine-the-cifar-10-dataset"></a>
</h3>
<div class="callout-content">
<p>Explain the output of these commands?</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Train: Images=</span><span class="sc">%s</span><span class="st">, Labels=</span><span class="sc">%s</span><span class="st">'</span> <span class="op">%</span> (train_images.shape, train_labels.shape))</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Validate: Images=</span><span class="sc">%s</span><span class="st">, Labels=</span><span class="sc">%s</span><span class="st">'</span> <span class="op">%</span> (val_images.shape, val_labels.shape))</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Test: Images=</span><span class="sc">%s</span><span class="st">, Labels=</span><span class="sc">%s</span><span class="st">'</span> <span class="op">%</span> (test_images.shape, test_labels.shape))</span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Output</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Train: Images=(40000, 32, 32, 3), Labels=(40000, 10)
Validate: Images=(10000, 32, 32, 3), Labels=(10000, 10)
Test: Images=(10000, 32, 32, 3), Labels=(10000, 10)</code></pre>
</div>
<p>The training set consists of 40000 images of 32x32 pixels and three
channels (RGB values) and labels.</p>
<p>The validation and test datasets consist of 10000 images of 32x32
pixels and three channels (RGB values) and labels.</p>
</div>
</div>
</div>
</div>
</div>
<div class="section level4">
<h4 id="visualise-a-subset-of-the-cifar-10-dataset">Visualise a subset of the CIFAR-10 dataset<a class="anchor" aria-label="anchor" href="#visualise-a-subset-of-the-cifar-10-dataset"></a>
</h4>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a figure object and specify width, height in inches</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">10</span>))</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># plot a subset of the images </span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">25</span>):</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">5</span>,<span class="dv">5</span>,i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    plt.imshow(train_images[i])</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    plt.title(class_names[train_labels[i,].argmax()])</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/01_cifar10_plot_subset.png" alt="Subset of 25 CIFAR-10 images representing different object classes" class="figure mx-auto d-block"></figure>
</div>
</div>
<div class="section level3">
<h3 id="step-4--choose-a-pre-trained-model-or-build-a-new-architecture-from-scratch">Step 4. Choose a pre-trained model or build a new architecture from
scratch<a class="anchor" aria-label="anchor" href="#step-4--choose-a-pre-trained-model-or-build-a-new-architecture-from-scratch"></a>
</h3>
<p>Often we can use an existing neural network instead of designing one
from scratch. Training a network can take a lot of time and
computational resources. There are a number of well publicised networks
which have been demonstrated to perform well at certain tasks. If you
know of one which already does a similar task well, then it makes sense
to use one of these.</p>
<p>If instead we decide to design our own network, then we need to think
about how many input neurons it will have, how many hidden layers and
how many outputs, and what types of layers to use. This will require
some experimentation and tweaking of the network design a few times
before achieving acceptable results.</p>
<p>Here we present an initial model to be explained in detail later
on:</p>
<div class="section level4">
<h4 id="define-the-model">Define the Model<a class="anchor" aria-label="anchor" href="#define-the-model"></a>
</h4>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># CNN Part 1</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Input layer of 32x32 images with three channels (RGB)</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>inputs_intro <span class="op">=</span> keras.Input(shape<span class="op">=</span>train_images.shape[<span class="dv">1</span>:])</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># CNN Part 2</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Convolutional layer with 16 filters, 3x3 kernel size, and ReLU activation</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>x_intro <span class="op">=</span> keras.layers.Conv2D(<span class="dv">16</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(inputs_intro)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Pooling layer with input window sized 2,2</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>x_intro <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x_intro)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Second Convolutional layer with 32 filters, 3x3 kernel size, and ReLU activation</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>x_intro <span class="op">=</span> keras.layers.Conv2D(<span class="dv">32</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x_intro)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Second Pooling layer with input window sized 2,2</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>x_intro <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x_intro)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Flatten layer to convert 2D feature maps into a 1D vector</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>x_intro <span class="op">=</span> keras.layers.Flatten()(x_intro)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Dense layer with 64 neurons and ReLU activation</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>x_intro <span class="op">=</span> keras.layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x_intro)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co"># CNN Part 3</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Output layer with 10 units (one for each class) and softmax activation</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>outputs_intro <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">'softmax'</span>)(x_intro)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="co"># create the model</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>model_intro <span class="op">=</span> keras.Model(inputs <span class="op">=</span> inputs_intro, </span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>                          outputs <span class="op">=</span> outputs_intro, </span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>                          name <span class="op">=</span> <span class="st">"cifar_model_intro"</span>)</span></code></pre>
</div>
</div>
</div>
<div class="section level3">
<h3 id="step-5--choose-a-loss-function-and-optimizer-and-compile-model">Step 5. Choose a loss function and optimizer and compile model<a class="anchor" aria-label="anchor" href="#step-5--choose-a-loss-function-and-optimizer-and-compile-model"></a>
</h3>
<p>To set up a model for training we need to compile it. This is when
you set up the rules and strategies for how your network is going to
learn.</p>
<p>The loss function tells the training algorithm how far away the
predicted value was from the true value. We will learn how to choose a
loss function in more detail in <a href="04-fit-cnn.html">Episode 4
Compile and Train (Fit) a Convolutional Neural Network</a>.</p>
<p>The optimizer is responsible for taking the output of the loss
function and then applying some changes to the weights within the
network. It is through this process that “learning” (adjustment of the
weights) is achieved.</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># compile the model</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>model_intro.<span class="bu">compile</span>(optimizer <span class="op">=</span> <span class="st">'adam'</span>,</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>                    loss <span class="op">=</span> keras.losses.CategoricalCrossentropy(),</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>                    metrics <span class="op">=</span> [<span class="st">'accuracy'</span>])</span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="step-6--train-the-model">Step 6. Train the model<a class="anchor" aria-label="anchor" href="#step-6--train-the-model"></a>
</h3>
<p>We can now go ahead and start training our neural network. We will
probably keep doing this for a given number of iterations through our
training dataset (referred to as epochs) or until the loss function
gives a value under a certain threshold.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the model</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>history_intro <span class="op">=</span> model_intro.fit(train_images, train_labels, epochs <span class="op">=</span> <span class="dv">10</span>, </span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>                                validation_data <span class="op">=</span> (val_images, val_labels),</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>                                batch_size <span class="op">=</span> <span class="dv">32</span>)</span></code></pre>
</div>
<p>Your output will begin to print similar to the output below:</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Epoch 1/10

1250/1250 [==============================] - 15s 12ms/step - loss: 1.4651 - accuracy: 0.4738 - val_loss: 1.2736 - val_accuracy: 0.5507</code></pre>
</div>
<div class="section level4">
<h4 id="what-does-this-output-mean">What does this output mean?<a class="anchor" aria-label="anchor" href="#what-does-this-output-mean"></a>
</h4>
<p>This output printed during the fit phase, i.e. training the model
against known image labels, can be broken down as follows:</p>
<ul>
<li><p><code>Epoch</code> describes the number of full passes over all
<em>training data</em>.</p></li>
<li><p>In the output above, there are <strong>1250</strong> batches
(steps) to complete each epoch. This number is calculated as the total
number of images used as input divided by the batch size (40000/32).
After 1250 batches, all training images will have been seen once and the
model moves on to the next epoch.</p></li>
<li><p><code>loss</code> is a value the model will attempt to minimise
and is a measure of the dissimilarity or error between the true label of
an image and the model prediction. Minimising this distance is where
<em>learning</em> occurs to adjust weights and bias which reduce
<code>loss</code>.</p></li>
<li><p><code>val_loss</code> is a value calculated against the
validation data and is a measure of the model’s performance against
unseen data.</p></li>
<li><p>Both values are a summation of errors made during each
epoch.</p></li>
<li><p><code>accuracy</code> and <code>val_accuracy</code> values are a
percentage and are only revelant to <strong>classification
problems</strong>.</p></li>
<li><p>The <code>val_accuracy</code> score can be used to communicate a
model’s effectiveness on unseen data.</p></li>
</ul>
</div>
</div>
<div class="section level3">
<h3 id="step-7--perform-a-predictionclassification">Step 7. Perform a Prediction/Classification<a class="anchor" aria-label="anchor" href="#step-7--perform-a-predictionclassification"></a>
</h3>
<p>After training the network we can use it to perform predictions. This
is how you would use the network after you have fully trained it to a
satisfactory performance. The predictions performed here on a special
hold-out set is used in the next step to measure the performance of the
network.</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># predict the class name of the first test image</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>result_intro <span class="op">=</span> model_intro.predict(test_images[<span class="dv">0</span>].reshape(<span class="dv">1</span>,<span class="dv">32</span>,<span class="dv">32</span>,<span class="dv">3</span>))</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The predicted probability of each class is: '</span>, result_intro.<span class="bu">round</span>(<span class="dv">4</span>))</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The class with the highest predicted probability is: '</span>, class_names[result_intro.argmax()])</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the image with its true label</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>plt.imshow(test_images[<span class="dv">0</span>])</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'True class:'</span> <span class="op">+</span> class_names[test_labels[<span class="dv">0</span>,].argmax()])</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The predicted probability of each class is:  [[0.0074 0.0006 0.0456 0.525  0.0036 0.1062 0.0162 0.0006 0.2908 0.004 ]]
The class with the highest predicted probability is:  cat</code></pre>
</div>
<figure><img src="fig/01_test_image.png" alt="poor resolution image of a cat" class="figure mx-auto d-block"></figure><p>Congratulations, you just created your first image classification
model and used it to classify an image!</p>
<p>Was the classification correct? Why might it be incorrect and what
can we do about?</p>
<p>There are many ways to try to improve the accuracy of our model, such
as adding or removing layers to the model definition and fine-tuning the
hyperparameters, which takes us to the next steps in our workflow.</p>
<div id="callout2" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout2"></a>
</h3>
<div class="callout-content">
<p>My result is different!</p>
<p>While the neural network itself is deterministic (ie without
randomness), various factors in the training process, system setup, and
data variability can lead to small variations in the output. These
variations are usually minor and should not significantly impact the
overall performance or behavior of the model.</p>
<p>If you are finding significant differences in the model predictions,
this could be a sign the model is not fully converged. “Convergence”
refers to the point where the model has reached an optimal or
near-optimal state in terms of learning from the training data.</p>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="step-8--measure-performance">Step 8. Measure Performance<a class="anchor" aria-label="anchor" href="#step-8--measure-performance"></a>
</h3>
<p>Once we trained the network we want to measure its performance. To do
this, we use additional data that was <strong>not</strong> part of the
training, called a test dataset. There are many different methods
available for measuring performance and which one is best depends on the
type of task we are attempting. These metrics are often published as an
indication of how well our network performs.</p>
</div>
<div class="section level3">
<h3 id="step-9--tune-hyperparameters">Step 9. Tune Hyperparameters<a class="anchor" aria-label="anchor" href="#step-9--tune-hyperparameters"></a>
</h3>
<p>When building image recognition models in Python, especially using
libraries like TensorFlow or Keras, the process involves not only
designing a neural network but also choosing the best values for various
hyperparameters that govern the training process.</p>
<p><strong>Hyperparameters</strong> are all the parameters set by the
person configuring the model as opposed to those learned by the
algorithm itself. These hyperparameters can include the learning rate,
the number of layers in the network, the number of neurons per layer,
and many more. Hyperparameter tuning refers to the process of
systematically searching for the best combination of hyperparameters
that will optimise the model’s performance. This concept will be
continued, with practical examples, in <a href="05-evaluate-predict-cnn.html">Episode 05 Evaluate a Convolutional
Neural Network and Make Predictions (Classifications)</a></p>
</div>
<div class="section level3">
<h3 id="step-10--share-model">Step 10. Share Model<a class="anchor" aria-label="anchor" href="#step-10--share-model"></a>
</h3>
<p>Now that we have a trained network that performs at a level we are
happy with we can go and use it on real live data to perform a
prediction. At this point we might want to consider publishing a file
with both the architecture of our network and the weights which it has
learned (assuming we did not use a pre-trained network). This will allow
others to use it as as pre-trained network for their own purposes and
for them to (mostly) reproduce our result.</p>
<p>To share the model we must save it first:</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># save the model</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>model_intro.save(<span class="st">'fit_outputs/model_intro.keras'</span>)</span></code></pre>
</div>
<p>We will return to each of these workflow steps throughout this lesson
and discuss each component in more detail.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Machine learning is the process where computers learn to recognise
patterns of data.</li>
<li>Deep learning is a subset of machine learning, which is a subset of
artificial intelligence.</li>
<li>Convolutional neural networks are well suited for image
classification.</li>
<li>To use Deep Learning effectively we follow a workflow of: defining
the problem, identifying inputs and outputs, preparing data, choosing
the type of network, training the model, tuning hyperparameters, and
measuring performance before we can classify data.</li>
</ul>
</div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</div>
</section></section><section id="aio-02-image-data"><p>Content from <a href="02-image-data.html">Introduction to Image Data</a></p>
<hr>
<p>Last updated on 2024-02-25 |
        
        <a href="https://https://github.com/erinmgraham/icwithcnn/edit/main/episodes/02-image-data.html" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How much data do you need for Deep Learning?</li>
<li>Where can I find image data to train my model?</li>
<li>How do I plot image data in python?</li>
<li>How do I prepare image data for use in a convolutional neural
network (CNN)?</li>
<li>Know the difference between training, testing, and validation
datasets.</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Identify sources of image data.</li>
<li>Understand the properties of image data.</li>
<li>Write code to plot image data.</li>
<li>Prepare an image dataset to train a convolutional neural network
(CNN).</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section id="deep-learning-workflow"><h2 class="section-heading">Deep Learning Workflow<a class="anchor" aria-label="anchor" href="#deep-learning-workflow"></a>
</h2>
<hr class="half-width">
<p>Let’s start over with the first steps in our workflow.</p>
<div class="section level3">
<h3 id="step-1--formulate-outline-the-problem">Step 1. Formulate/ Outline the problem<a class="anchor" aria-label="anchor" href="#step-1--formulate-outline-the-problem"></a>
</h3>
<p>Firstly we must decide what it is we want our Deep Learning system to
do. This lesson is all about image classification and our aim is to put
an image into one of ten categories: airplane, automobile, bird, cat,
deer, dog, frog, horse, ship, or truck</p>
</div>
<div class="section level3">
<h3 id="step-2--identify-inputs-and-outputs">Step 2. Identify inputs and outputs<a class="anchor" aria-label="anchor" href="#step-2--identify-inputs-and-outputs"></a>
</h3>
<p>Next we identify the inputs and outputs of the neural network. In our
case, the data is images and the inputs could be the individual pixels
of the images.</p>
<p>We are performing a classification problem and we want to output one
category for each image.</p>
</div>
<div class="section level3">
<h3 id="step-3--prepare-data">Step 3. Prepare data<a class="anchor" aria-label="anchor" href="#step-3--prepare-data"></a>
</h3>
<p>Deep Learning requires extensive training using example data which
tells the network what output it should produce for a given input. In
this workshop, our network will be trained on a series of images and
told what they contain. Once the network is trained, it should be able
to take another image and correctly classify its contents.</p>
<p>You can use pre-existing data or prepare your own.</p>
<div id="challenge-how-much-data-do-you-need-for-deep-learning" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-how-much-data-do-you-need-for-deep-learning" class="callout-inner">
<h3 class="callout-title">CHALLENGE How much data do you need for Deep
Learning?<a class="anchor" aria-label="anchor" href="#challenge-how-much-data-do-you-need-for-deep-learning"></a>
</h3>
<div class="callout-content">
<p>The rise of Deep Learning is partially due to the increased
availability of very large datasets. But how much data do you actually
need to train a Deep Learning model?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>Unfortunately, this question is not easy to answer. It depends, among
other things, on the complexity of the task (which you often do not know
beforehand), the quality of the available dataset and the complexity of
the network. For complex tasks with large neural networks, adding more
data often improves performance. However, this is also not a generic
truth: if the data you add is too similar to the data you already have,
it will not give much new information to the neural network.</p>
<p>In case you have too little data available to train a complex network
from scratch, it is sometimes possible to use a pretrained network that
was trained on a similar problem. Another trick is data augmentation,
where you expand the dataset with artificial data points that could be
real. An example of this is mirroring images when trying to classify
cats and dogs. An horizontally mirrored animal retains the label, but
exposes a different view.</p>
</div>
</div>
</div>
</div>
<div class="section level4">
<h4 id="pre-existing-image-data">Pre-existing image data<a class="anchor" aria-label="anchor" href="#pre-existing-image-data"></a>
</h4>
<p>In some cases you will be able to download an image dataset that is
already labelled and can be used to classify a number of different
object like the CIFAR-10 dataset. Other examples include:</p>
<ul>
<li>
<a href="https://en.wikipedia.org/wiki/MNIST_database" class="external-link">MNIST
database</a> - 60,000 training images of handwritten digits (0-9)</li>
<li>
<a href="https://www.image-net.org/" class="external-link">ImageNet</a> - 14 million
hand-annotated images indicating objects from more than 20,000
categories. ImageNet sponsors an <a href="https://www.image-net.org/challenges/LSVRC/#:~:text=The%20ImageNet%20Large%20Scale%20Visual,image%20classification%20at%20large%20scale." class="external-link">annual
software contest</a> where programs compete to achieve the highest
accuracy. When choosing a pretrained network, the winners of these sorts
of competitions are generally a good place to start.</li>
<li>
<a href="https://cocodataset.org/#home" class="external-link">MS COCO</a> - &gt;200,000
labelled images used for object detection, instance segmentation,
keypoint analysis, and captioning</li>
</ul>
<p>Where labelled data exists, in most cases the data provider or other
users will have created data-specific functions you can use to load the
data. We already did this in the introduction:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> keras</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># load the CIFAR-10 dataset included with the keras library</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>(train_images, train_labels), (test_images, test_labels) <span class="op">=</span> keras.datasets.cifar10.load_data()</span></code></pre>
</div>
<p>In this instance the data is likely already prepared for use in a
CNN. However, it is always a good idea to first read any associated
documentation to find out what steps the data providers took to prepare
the images and second to take a closer at the images once loaded and
query their attributes.</p>
</div>
<div class="section level4">
<h4 id="custom-image-data">Custom image data<a class="anchor" aria-label="anchor" href="#custom-image-data"></a>
</h4>
<p>In other cases, you will create your own set of labelled images.</p>
<p><strong>Custom data i. Data collection and Labeling:</strong></p>
<p>For image classification the label applies to the entire image;
object detection requires bounding boxes around objects of interest, and
instance or semantic segmentation requires each pixel to be
labelled.</p>
<p>There are a number of open source software used to label your
dataset, including:</p>
<ul>
<li>(Visual Geometry Group) <a href="https://www.robots.ox.ac.uk/~vgg/software/via/" class="external-link">VGG Image
Annotator</a> (VIA)</li>
<li>
<a href="https://imagej.net/" class="external-link">ImageJ</a> can be extended with
plugins for annotation</li>
<li>
<a href="https://github.com/jsbroks/coco-annotator" class="external-link">COCO
Annotator</a> is designed specifically for creating annotations
compatible with Common Objects in Context (COCO) format</li>
</ul>
<p><strong>Custom data ii. Data preprocessing:</strong></p>
<p>This step involves various tasks to enhance the quality and
consistency of the data:</p>
<ul>
<li><p><strong>Resizing</strong>: Resize images to a consistent
resolution to ensure uniformity and reduce computational load.</p></li>
<li><p><strong>Normalisation</strong>: Scale pixel values to a common
range, often between 0 and 1 or -1 and 1. Normalisation helps the model
converge faster during training.</p></li>
<li><p><strong>Label encoding</strong> is a technique used to represent
categorical data with numerical labels.</p></li>
<li><p><strong>Data Augmentation</strong>: Apply random transformations
(e.g., rotations, flips, shifts) to create new variations of the same
image. This helps improve the model’s robustness and generalisation by
exposing it to more diverse data.</p></li>
</ul>
<p>Before jumping into these specific preprocessing tasks related to
images, it’s important to understand that images on a computer are
stored as numerical representations or simplified versions of the real
world. Therefore it’s essential to take some time to understand these
numerical abstractions.</p>
</div>
</div>
<div class="section level3">
<h3 id="pixels">Pixels<a class="anchor" aria-label="anchor" href="#pixels"></a>
</h3>
<p>It is important to realise that images are stored as rectangular
arrays of hundreds, thousands, or millions of discrete “picture
elements,” otherwise known as pixels. Each pixel can be thought of as a
single square point of coloured light.</p>
<p>For example, consider this image of a Jabiru, with a square area
designated by a red box:</p>
<figure><img src="fig/02_Jabiru_TGS_marked.jpg" alt="Jabiru image that is 552 pixels wide and 573 pixels high. A red square around the neck region indicates the area to zoom in on." class="figure mx-auto d-block"></figure><p>Now, if we zoomed in close enough to the red box, the individual
pixels would stand out:</p>
<figure><img src="fig/02_Jabiru_TGS_marked_zoom_enlarged.jpg" alt="zoomed in area of Jabiru where the individual pixels stand out" class="figure mx-auto d-block"></figure><p>Note each square in the enlarged image area (i.e. each pixel) is all
one colour, but each pixel can be a different colour from its
neighbours. Viewed from a distance, these pixels seem to blend together
to form the image.</p>
</div>
<div class="section level3">
<h3 id="working-with-pixels">Working with Pixels<a class="anchor" aria-label="anchor" href="#working-with-pixels"></a>
</h3>
<p>As noted, in practice, real world images will typically be made up of
a vast number of pixels, and each of these pixels will be one of
potentially millions of colours.</p>
<p>In python, an image can represented as a 2- or 3-dimensional array,
where each element corresponds to a pixel value in the image. In the
context of images, these arrays often have dimensions for height, width,
and colour channels (if applicable).</p>
<p>Let us start with the Jabiru image.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load the required packages</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.utils <span class="im">import</span> img_to_array</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.utils <span class="im">import</span> load_img</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># specify the image path</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>new_img_path <span class="op">=</span> <span class="st">"../data/Jabiru_TGS.JPG"</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co"># read in the image with default arguments</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>new_img_pil <span class="op">=</span> load_img(new_img_path)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co"># confirm the data class and size</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The new image is of class :'</span>, new_img_pil.__class__, <span class="st">'and has the size'</span>, new_img_pil.size)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The new image is of class : &lt;class 'PIL.JpegImagePlugin.JpegImageFile'&gt; and has the size (552, 573)</code></pre>
</div>
</div>
<div class="section level3">
<h3 id="image-dimensions---resizing">Image Dimensions - Resizing<a class="anchor" aria-label="anchor" href="#image-dimensions---resizing"></a>
</h3>
<p>The new image has shape <code>(573, 552, 3)</code>, meaning it is
much larger in size, 573x552 pixels; a rectangle instead of a square;
and consists of three colour channels (RGB).</p>
<p>Recall from the introduction that our training data set consists of
50000 images of 32x32 pixels and three channels.</p>
<p>To reduce the computational load and ensure all of our images have a
uniform size, we need to choose an image resolution (or size in pixels)
and ensure all of the images we use are resized to that shape to be
consistent.</p>
<p>There are a couple of ways to do this in python but one way is to
specify the size you want using an argument to the
<code>load_img()</code> function from <code>keras.utils</code>.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># read in the new image and specify the target size to be the same as our training images</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>new_img_pil_small <span class="op">=</span> load_img(new_img_path, target_size<span class="op">=</span>(<span class="dv">32</span>,<span class="dv">32</span>))</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># confirm the data class and shape</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The new image is still of class:'</span>, new_img_pil_small.__class__, <span class="st">'but now has the same size'</span>, new_img_pil_small.size, <span class="st">'as our training data'</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The new image is still of class: &lt;class 'PIL.Image.Image'&gt; but now has the same size (32, 32) as our training data.</code></pre>
</div>
<div id="accordionSpoiler1" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler1" aria-expanded="false" aria-controls="collapseSpoiler1">
  <h3 class="accordion-header" id="headingSpoiler1">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>WANT TO KNOW MORE: Python image libraries</h3>
</button>
<div id="collapseSpoiler1" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler1" aria-labelledby="headingSpoiler1">
<div class="accordion-body">
<p>Two of the most commonly used libraries for image representation and
manipulation are NumPy and Pillow (PIL). Additionally, when working with
deep learning frameworks like TensorFlow and PyTorch, images are often
represented as tensors within these frameworks.</p>
<ul>
<li>NumPy is a powerful library for numerical computing in Python. It
provides support for creating and manipulating arrays, which can be used
to represent images as multidimensional arrays.
<ul>
<li><code>import numpy as np</code></li>
</ul>
</li>
<li>The Pillow library provides functions to open, manipulate, and save
various image file formats. It represents images using its own Image
class.
<ul>
<li><code>from PIL import Image</code></li>
<li>
<a href="https://pillow.readthedocs.io/en/latest/reference/Image.html" class="external-link">PIL
Image Module</a> documentation</li>
</ul>
</li>
<li>TensorFlow images are often represented as tensors that have
dimensions for batch size, height, width, and colour channels. This
framework provide tools to load, preprocess, and work with image data
seamlessly.
<ul>
<li><code>from tensorflow import keras</code></li>
<li>
<a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image" class="external-link">image
preprocessing</a> documentation</li>
<li>Note Keras image functions also use PIL</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="normalisation">Normalisation<a class="anchor" aria-label="anchor" href="#normalisation"></a>
</h3>
<p>Image RGB values are between 0 and 255. As input for neural networks,
it is better to have small input values. The process of converting the
RGB values to be between 0 and 1 is called
<strong>normalization</strong>.</p>
<p>Before we can normalize our image values we must convert the image to
an numpy array.</p>
<p>We introduced how to do this in <a href="01-introduction.html">Episode 01 Introduction to Deep Learning</a>
but what you may not have noticed is that the
<code>keras.datasets.cifar10.load_data</code> function did the
conversion for us whereas now we will do it ourselves.</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># convert the Image into an array for normalization</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>new_img_arr <span class="op">=</span> img_to_array(new_img_pil_small)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># confirm the data class and shape</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The new image is now of class :'</span>, new_img_arr.__class__, <span class="st">'and has the shape'</span>, new_img_arr.shape)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The new image is now of class : &lt;class 'numpy.ndarray'&gt; and has the shape (32, 32, 3)</code></pre>
</div>
<p>Now we can normalize the values. Let us also investigate the image
values before and after we normalize them.</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># extract the min, max, and mean pixel values</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The min, max, and mean pixel values are'</span>, new_img_arr.<span class="bu">min</span>(), <span class="st">','</span>, new_img_arr.<span class="bu">max</span>(), <span class="st">', and'</span>, new_img_arr.mean().<span class="bu">round</span>(), <span class="st">'respectively.'</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># normalize the RGB values to be between 0 and 1</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>new_img_arr_norm <span class="op">=</span> new_img_arr <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># extract the min, max, and mean pixel values AFTER</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'After normalization, the min, max, and mean pixel values are'</span>, new_img_arr_norm.<span class="bu">min</span>(), <span class="st">','</span>, new_img_arr_norm.<span class="bu">max</span>(), <span class="st">', and'</span>, new_img_arr_norm.mean().<span class="bu">round</span>(), <span class="st">'respectively.'</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The min, max, and mean pixel values are 0.0 , 255.0 , and 87.0 respectively.
After normalization, the min, max, and mean pixel values are 0.0 , 1.0 , and 0.0 respectively.</code></pre>
</div>
<p>Of course, if there are a large number of images to preprocess you do
not want to copy and paste these steps for each image! Fortunately,
Keras has a solution: <a href="https://keras.io/api/data_loading/image/" class="external-link">tf.keras.utils.image_dataset_from_directory</a></p>
<div id="accordionSpoiler2" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler2" aria-expanded="false" aria-controls="collapseSpoiler2">
  <h3 class="accordion-header" id="headingSpoiler2">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>WANT TO KNOW MORE: Why Normalize?</h3>
</button>
<div id="collapseSpoiler2" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler2" aria-labelledby="headingSpoiler2">
<div class="accordion-body">
<p>ChatGPT</p>
<p>Normalizing the RGB values to be between 0 and 1 is a common
pre-processing step in machine learning tasks, especially when dealing
with image data. This normalization has several benefits:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Numerical Stability</strong>: By scaling the RGB values
to a range between 0 and 1, you avoid potential numerical instability
issues that can arise when working with large values. Neural networks
and many other machine learning algorithms are sensitive to the scale of
input features, and normalizing helps to keep the values within a
manageable range.</p></li>
<li><p><strong>Faster Convergence</strong>: Normalizing the RGB values
often helps in faster convergence during the training process. Neural
networks and other optimization algorithms rely on gradient descent
techniques, and having inputs in a consistent range aids in smoother and
faster convergence.</p></li>
<li><p><strong>Equal Weightage for All Channels</strong>: In RGB images,
each channel (Red, Green, Blue) represents different colour intensities.
By normalizing to the range [0, 1], you ensure that each channel is
treated with equal weightage during training. This is important because
some machine learning algorithms could assign more importance to larger
values.</p></li>
<li><p><strong>Generalization</strong>: Normalization helps the model to
generalize better to unseen data. When the input features are in the
same range, the learned weights and biases can be more effectively
applied to new examples, making the model more robust.</p></li>
<li><p><strong>Compatibility</strong>: Many image-related libraries,
algorithms, and models expect pixel values to be in the range of [0, 1].
By normalizing the RGB values, you ensure compatibility and seamless
integration with these tools.</p></li>
</ol>
<p>The normalization process is typically done by dividing each RGB
value (ranging from 0 to 255) by 255, which scales the values to the
range [0, 1].</p>
<p>For example, if you have an RGB image with pixel values (100, 150,
200), after normalization, the pixel values would become (100/255,
150/255, 200/255) ≈ (0.39, 0.59, 0.78).</p>
<p>Remember that normalization is not always mandatory, and there could
be cases where other scaling techniques might be more suitable based on
the specific problem and data distribution. However, for most
image-related tasks in machine learning, normalizing RGB values to [0,
1] is a good starting point.</p>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="one-hot-encoding">One-hot encoding<a class="anchor" aria-label="anchor" href="#one-hot-encoding"></a>
</h3>
<p>A neural network can only take numerical inputs and outputs, and
learns by calculating how “far away” the class predicted by the neural
network is from the true class. When the target (label) is categorical
data, or strings, it is very difficult to determine this “distance” or
error. Therefore we will transform this column into a more suitable
format. There are many ways to do this, however we will be using
<strong>one-hot encoding</strong>.</p>
<p>One-hot encoding is a technique to represent categorical data as
binary vectors, making it compatible with machine learning algorithms.
Each category becomes a separate column, and the presence or absence of
a category is indicated by 1s and 0s in the respective columns.</p>
<p>Let’s say you have a dataset with a “colour” column containing three
categories: yellow, orange, purple.</p>
<p>Table 1. Original Data.</p>
<table class="table">
<thead><tr class="header">
<th>colour</th>
<th align="right"></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>yellow</td>
<td align="right"><span class="emoji" data-emoji="yellow_square">🟨</span></td>
</tr>
<tr class="even">
<td>orange</td>
<td align="right"><span class="emoji" data-emoji="orange_square">🟧</span></td>
</tr>
<tr class="odd">
<td>purple</td>
<td align="right"><span class="emoji" data-emoji="purple_square">🟪</span></td>
</tr>
<tr class="even">
<td>yellow</td>
<td align="right"><span class="emoji" data-emoji="yellow_square">🟨</span></td>
</tr>
</tbody>
</table>
<p>Table 2. After One-Hot Encoding.</p>
<table class="table">
<thead><tr class="header">
<th>colour_yellow</th>
<th align="center">colour_orange</th>
<th align="right">colour_purple</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>1</td>
<td align="center">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td>0</td>
<td align="center">1</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td>0</td>
<td align="center">0</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td>1</td>
<td align="center">0</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p>The Keras function for one_hot encoding is called <a href="https://keras.io/api/utils/python_utils/#to_categorical-function" class="external-link">to_categorical</a>:</p>
<p><code>tf.keras.utils.to_categorical(y, num_classes=None, dtype="float32")</code></p>
<ul>
<li>
<code>y</code> is an array of class values to be converted into a
matrix (integers from 0 to num_classes - 1).</li>
<li>
<code>num_classes</code> is the total number of classes. If None,
this would be inferred as max(y) + 1.</li>
<li>
<code>dtype</code> is the data type expected by the input. Default:
‘float32’</li>
</ul>
<p>We performed this operation in <strong>Step 3. Prepare data</strong>
of the Introduction but let us inspect the labels before and after
one-hot encoding.</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'train_labels before one hot encoding'</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(train_labels)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co"># one-hot encode labels</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>train_labels <span class="op">=</span> keras.utils.to_categorical(train_labels, <span class="bu">len</span>(class_names))</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'train_labels after one hot encoding'</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(train_labels)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>train_labels before one hot encoding
[[6]
 [9]
 [9]
 ...
 [9]
 [1]
 [1]]

train_labels after one hot encoding
[[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 1.]
 [0. 0. 0. ... 0. 0. 1.]
 ...
 [0. 0. 0. ... 0. 0. 1.]
 [0. 1. 0. ... 0. 0. 0.]
 [0. 1. 0. ... 0. 0. 0.]]</code></pre>
</div>
<div id="callout1" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout1"></a>
</h3>
<div class="callout-content">
<p>WAIT I thought there were TEN classes!? Where is the rest of the
data?</p>
<p>The Spyder IDE uses the ‘…’ notation when it “hides” some of the data
for display purposes.</p>
<p>To view the entire array, go the Variable Explorer in the upper right
hand corner of your Spyder IDE and double click on the ‘train_labels’
object. This will open a new window that shows all of the columns.</p>
<figure><img src="fig/02_spyder_onehot_train_labels_inFULL.png" alt="Screenshot of Spyder window displaying the entire train_labels array." class="figure mx-auto d-block"></figure>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="image-augmentation">Image augmentation<a class="anchor" aria-label="anchor" href="#image-augmentation"></a>
</h3>
<p>There are several ways to augment your data to increase the diversity
of the training data and improve model robustness.</p>
<ul>
<li>Geometric Transformations
<ul>
<li>rotation, scaling, zooming, cropping</li>
</ul>
</li>
<li>Flipping or Mirroring
<ul>
<li>some classes, like horse, have a different shape when facing left or
right and you want your model to recognize both</li>
</ul>
</li>
<li>Colour properties
<ul>
<li>brightness, contrast, or hue</li>
<li>these changes simulate variations in lighting conditions</li>
</ul>
</li>
</ul>
<p>We will not discuss image augmentation in this lesson, but it is
important that you are aware of this type of data preparation because it
can make a big difference in your model’s ability to predict outside of
your training data.</p>
<p>Information about these operations are included in the Keras document
for <a href="https://keras.io/api/layers/preprocessing_layers/image_augmentation/" class="external-link">Image
augmentation layers</a>.</p>
</div>
<div class="section level3">
<h3 id="data-splitting">Data Splitting<a class="anchor" aria-label="anchor" href="#data-splitting"></a>
</h3>
<p>The typical practice in machine learning is to split your data into
two subsets: a <strong>training</strong> set and a <strong>test</strong>
set. This initial split separates the data you will use to train your
model from the data you will use to evaluate its performance.</p>
<p>After this initial split, you can choose to further split the
training set into a training set and a <strong>validation set</strong>.
This is often done when you are fine-tuning hyperparameters, selecting
the best model from a set of candidate models, or preventing
overfitting.</p>
<p>In the previous episode, we used the ‘cifar10.load_data()’ method
included with the Keras installation to return a dataset split into
train and test sets. Now we want to split the training data into
training and validation sets.</p>
<p>To split a dataset into training and test sets there is a very
convenient function from sklearn called <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" class="external-link">train_test_split</a>:</p>
<p><code>sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)</code></p>
<ul>
<li>The first two parameters are the dataset (X) and the corresponding
targets (y) (i.e. class labels).</li>
<li>Next is the named parameter <code>test_size</code>. This is the
fraction of the dataset used for testing and in this case
<code>0.2</code> means 20 per cent of the data will be used for
testing.</li>
<li>
<code>random_state</code> controls the shuffling of the dataset,
setting this value will reproduce the same results (assuming you give
the same integer) every time it is called.</li>
<li>
<code>shuffle</code> which can be either <code>True</code> or
<code>False</code>, it controls whether the order of the rows of the
dataset is shuffled before splitting. It defaults to
<code>True</code>.</li>
<li>
<code>stratify</code> is a more advanced parameter that controls how
the split is done. By setting it to <code>target</code> the train and
test sets the function will return will have roughly the same
proportions (with regards to the number of images of a certain class) as
the dataset.</li>
</ul>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># split the training data into training and validation sets</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>train_images, val_images, train_labels, val_labels <span class="op">=</span> train_test_split(train_images, train_labels, test_size <span class="op">=</span> <span class="fl">0.2</span>, </span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>                random_state <span class="op">=</span> <span class="dv">42</span>)</span></code></pre>
</div>
<div id="challenge-training-and-validation" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="challenge-training-and-validation" class="callout-inner">
<h3 class="callout-title">CHALLENGE Training and Validation<a class="anchor" aria-label="anchor" href="#challenge-training-and-validation"></a>
</h3>
<div class="callout-content">
<p>Inspect the training and validation sets we created.</p>
<p>How many samples does each set have and are the classes well
balanced?</p>
<p>Hint: Use <code>np.sum()</code> on the ’*_labels’ to find out if the
classes are well balanced. :::::::::::::::::::::::: solution</p>
<p>A. Training Set</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The training set has'</span>, train_images.shape[<span class="dv">0</span>], <span class="st">'samples.</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The number of images in each class:</span><span class="ch">\n</span><span class="st">'</span>, train_labels.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The training set has 40000 samples.

The number of images in each class:
 [4027. 4021. 3970. 3977. 4067. 3985. 4004. 4006. 3983. 3960.]</code></pre>
</div>
<p>B. Validation Set (we can use the same code as the training set)</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The validation set has'</span>, val_images.shape[<span class="dv">0</span>], <span class="st">'samples.</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The number of images in each class:</span><span class="ch">\n</span><span class="st">'</span>, val_labels.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The validation set has 10000 samples.

The number of images in each class:
 [ 973.  979. 1030. 1023.  933. 1015.  996.  994. 1017. 1040.]</code></pre>
</div>
</div>
</div>
</div>
<p>::::::::::::::::::::::::::::::::::::::::::::::::</p>
<div id="accordionSpoiler3" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler3" aria-expanded="false" aria-controls="collapseSpoiler3">
  <h3 class="accordion-header" id="headingSpoiler3">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>WANT TO KNOW MORE: Data Splitting Techniques</h3>
</button>
<div id="collapseSpoiler3" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler3" aria-labelledby="headingSpoiler3">
<div class="accordion-body">
<p>ChatGPT</p>
<p>Data is typically split into the training, validation, and test data
sets using a process called data splitting or data partitioning. There
are various methods to perform this split, and the choice of technique
depends on the specific problem, dataset size, and the nature of the
data. Here are some common approaches:</p>
<p><strong>Hold-Out Method:</strong></p>
<ul>
<li><p>In the hold-out method, the dataset is divided into two parts
initially: a training set and a test set.</p></li>
<li><p>The training set is used to train the model, and the test set is
kept completely separate to evaluate the model’s final
performance.</p></li>
<li><p>This method is straightforward and widely used when the dataset
is sufficiently large.</p></li>
</ul>
<p><strong>Train-Validation-Test Split:</strong></p>
<ul>
<li><p>The dataset is split into three parts: the training set, the
validation set, and the test set.</p></li>
<li><p>The training set is used to train the model, the validation set
is used to tune hyperparameters and prevent overfitting during training,
and the test set is used to assess the final model performance.</p></li>
<li><p>This method is commonly used when fine-tuning model
hyperparameters is necessary.</p></li>
</ul>
<p><strong>K-Fold Cross-Validation:</strong></p>
<ul>
<li><p>In k-fold cross-validation, the dataset is divided into k subsets
(folds) of roughly equal size.</p></li>
<li><p>The model is trained and evaluated k times, each time using a
different fold as the test set while the remaining k-1 folds are used as
the training set.</p></li>
<li><p>The final performance metric is calculated as the average of the
k evaluation results, providing a more robust estimate of model
performance.</p></li>
<li><p>This method is particularly useful when the dataset size is
limited, and it helps in better utilizing available data.</p></li>
</ul>
<p><strong>Stratified Sampling:</strong></p>
<ul>
<li><p>Stratified sampling is used when the dataset is imbalanced,
meaning some classes or categories are underrepresented.</p></li>
<li><p>The data is split in such a way that each subset (training,
validation, or test) maintains the same class distribution as the
original dataset.</p></li>
<li><p>This ensures all classes are well-represented in each subset,
which is important to avoid biased model evaluation.</p></li>
</ul>
<p>It’s important to note that the exact split ratios (e.g., 80-10-10 or
70-15-15) may vary depending on the problem, dataset size, and specific
requirements. Additionally, data splitting should be performed randomly
to avoid introducing any biases into the model training and evaluation
process.</p>
</div>
</div>
</div>
</div>
</div>
</section><section id="data-preprocessing-completed"><h2 class="section-heading">Data preprocessing completed!<a class="anchor" aria-label="anchor" href="#data-preprocessing-completed"></a>
</h2>
<hr class="half-width">
<p>Our dataset is preprocessed and split into three sets which means we
are ready to learn how to build a CNN like we used in the
introduction.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Image datasets can be found online or created uniquely for your
research question.</li>
<li>Images consist of pixels arranged in a particular order.</li>
<li>Image data is usually preprocessed before use in a CNN for
efficiency, consistency, and robustness.</li>
<li>Input data generally consists of three sets: a training set used to
fit model parameters; a validation set used to evaluate the model fit on
training data; a test set used to evaluate the final model
performance.</li>
</ul>
</div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-03-build-cnn"><p>Content from <a href="03-build-cnn.html">Build a Convolutional Neural Network</a></p>
<hr>
<p>Last updated on 2024-02-25 |
        
        <a href="https://https://github.com/erinmgraham/icwithcnn/edit/main/episodes/03-build-cnn.html" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is a (artificial) neural network (ANN)?</li>
<li>How is a convolutional neural network (CNN) different from an
ANN?</li>
<li>What are the types of layers used to build a CNN?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understand how a convolutional neural network (CNN) differs from an
artificial neural network (ANN).</li>
<li>Explain the terms: kernel, filter.</li>
<li>Know the different layers: convolutional, pooling, flatten,
dense.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section id="neural-networks"><h2 class="section-heading">Neural Networks<a class="anchor" aria-label="anchor" href="#neural-networks"></a>
</h2>
<hr class="half-width">
<p>A <strong>neural network</strong> is an artificial intelligence
technique loosely based on the way neurons in the brain work.</p>
<div class="section level3">
<h3 id="a-single-nueron">A single nueron<a class="anchor" aria-label="anchor" href="#a-single-nueron"></a>
</h3>
<p>A neural network consists of connected computational units called
<strong>neurons</strong>. Each neuron will:</p>
<ul>
<li>Take one or more inputs (<span class="math inline">\(x_1, x_2,
...\)</span>), e.g., input data expressed as floating point
numbers.</li>
<li>Conduct three main operations most of the time:
<ul>
<li>Calculate the weighted sum of the inputs where ($w_1, w_2, … $)
indicate weights</li>
<li>Add an extra constant weight (i.e. a bias term) to this weighted
sum</li>
<li>Apply a non-linear function to the output so far (using a predefined
activation function such as the ReLU function)</li>
</ul>
</li>
<li>Return one output value, again a floating point number.</li>
</ul>
<p>One example equation to calculate the output for a neuron is: <span class="math inline">\(output=ReLU(∑i(xi∗wi)+bias)\)</span></p>
<figure><img src="fig/03_neuron.png" alt="diagram of a single neuron taking multiple inputs and their associated weights in and then applying an activation function to predict a single output" class="figure mx-auto d-block"></figure>
</div>
<div class="section level3">
<h3 id="combining-multiple-neurons-into-a-network">Combining multiple neurons into a network<a class="anchor" aria-label="anchor" href="#combining-multiple-neurons-into-a-network"></a>
</h3>
<p>Multiple neurons can be joined together by connecting the output of
one to the input of another. These connections are associated with
weights that determine the ‘strength’ of the connection, and the weights
are adjusted during training. In this way, the combination of neurons
and connections describe a computational graph, an example can be seen
in the image below.</p>
<p>In most neural networks neurons are aggregated into layers. Signals
travel from the input layer to the output layer, possibly through one or
more intermediate layers called hidden layers. The image below
illustrates an example of a neural network with three layers, each
circle is a neuron, each line is an edge and the arrows indicate the
direction data moves in.</p>
<figure><img src="fig/03_neural_net.png" alt="diagram of a neural with four neurons taking multiple inputs and their weights and predicting multiple outputs" class="figure mx-auto d-block"><figcaption>The image above is by Glosser.ca, <a href="https://creativecommons.org/licenses/by-sa/3.0" class="external-link">CC BY-SA 3.0</a>,
via Wikimedia Commons, <a href="https://commons.wikimedia.org/wiki/File:Colored_neural_network.svg" class="external-link">original
source</a></figcaption></figure><p>Neural networks aren’t a new technique, they have been around since
the late 1940s. But until around 2010 neural networks tended to be quite
small, consisting of only 10s or perhaps 100s of neurons. This limited
them to only solving quite basic problems. Around 2010 improvements in
computing power and the algorithms for training the networks made much
larger and more powerful networks practical. These are known as deep
neural networks or Deep Learning.</p>
</div>
</section><section id="convolutional-neural-networks"><h2 class="section-heading">Convolutional Neural Networks<a class="anchor" aria-label="anchor" href="#convolutional-neural-networks"></a>
</h2>
<hr class="half-width">
<p>A convolutional neural network (CNN) is a type of artificial neural
network (ANN) most commonly applied to analyze visual imagery. They are
designed to recognize the spatial structure of images when extracting
features.</p>
<div class="section level3">
<h3 id="step-4--build-an-architecture-from-scratch-or-choose-a-pretrained-model">Step 4. Build an architecture from scratch or choose a pretrained
model<a class="anchor" aria-label="anchor" href="#step-4--build-an-architecture-from-scratch-or-choose-a-pretrained-model"></a>
</h3>
<p>Let us explore how to build a neural network from scratch. Although
this sounds like a daunting task, with Keras it is surprisingly
straightforward. With Keras you compose a neural network by creating
layers and linking them together.</p>
<p>This is the same network from the introduction:</p>
<pre><code><span><span class="co"># # CNN Part 1</span></span>
<span><span class="co"># # Input layer of 32x32 images with three channels (RGB)</span></span>
<span><span class="co"># inputs_intro = keras.Input(shape=train_images.shape[1:])</span></span>
<span></span>
<span><span class="co"># # CNN Part 2</span></span>
<span><span class="co"># # Convolutional layer with 16 filters, 3x3 kernel size, and ReLU activation</span></span>
<span><span class="co"># x_intro = keras.layers.Conv2D(16, (3, 3), activation='relu')(inputs_intro)</span></span>
<span><span class="co"># # Pooling layer with input window sized 2,2</span></span>
<span><span class="co"># x_intro = keras.layers.MaxPooling2D((2, 2))(x_intro)</span></span>
<span><span class="co"># # Second Convolutional layer with 32 filters, 3x3 kernel size, and ReLU activation</span></span>
<span><span class="co"># x_intro = keras.layers.Conv2D(32, (3, 3), activation='relu')(x_intro)</span></span>
<span><span class="co"># # Second Pooling layer with input window sized 2,2</span></span>
<span><span class="co"># x_intro = keras.layers.MaxPooling2D((2, 2))(x_intro)</span></span>
<span><span class="co"># # Flatten layer to convert 2D feature maps into a 1D vector</span></span>
<span><span class="co"># x_intro = keras.layers.Flatten()(x_intro)</span></span>
<span><span class="co"># # Dense layer with 64 neurons and ReLU activation</span></span>
<span><span class="co"># x_intro = keras.layers.Dense(64, activation='relu')(x_intro)</span></span>
<span></span>
<span><span class="co"># # CNN Part 3</span></span>
<span><span class="co"># # Output layer with 10 units (one for each class) and softmax activation</span></span>
<span><span class="co"># outputs_intro = keras.layers.Dense(10, activation='softmax')(x_intro)</span></span></code></pre>
</div>
<div class="section level3">
<h3 id="parts-of-a-neural-network">Parts of a neural network<a class="anchor" aria-label="anchor" href="#parts-of-a-neural-network"></a>
</h3>
<p>There are three main components of a neural network:</p>
<ul>
<li>CNN Part 1. Input Layer</li>
<li>CNN Part 2. Hidden Layers</li>
<li>CNN Part 3. Output Layer</li>
</ul>
<p>The output from each layer becomes the input to the next layer.</p>
<div class="section level4">
<h4 id="cnn-part-1--input-layer">CNN Part 1. Input Layer<a class="anchor" aria-label="anchor" href="#cnn-part-1--input-layer"></a>
</h4>
<p>The Input in Keras gets special treatment when images are used. Keras
automatically calculates the number of inputs and outputs a specific
layer needs and therefore how many edges need to be created. This means
we must let Keras know how big our input is going to be. We do this by
instantiating a <code>keras.Input</code> class and pass it a tuple to
indicate the dimensionality of the input data.</p>
<p>In our case, the shape of an image is defined by its pixel dimensions
and number of channels:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># recall the shape of the images in our dataset</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(train_images.shape)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(40000, 32, 32, 3) # number of images, image width in pixels, image height in pixels, number of channels (RGB)</code></pre>
</div>
<p>The input layer is created with the <code>tf.keras.Input</code>
function and its first parameter is the expected shape of the input.</p>
<p>Because the shape of our input dataset includes the total number of
images, we want to take a slice of the shape related to an individual
image, hence:</p>
<pre><code><span><span class="co"># Input layer of 32x32 images with three channels (RGB)</span></span>
<span><span class="co">#inputs_intro = keras.Input(shape=train_images.shape[1:])</span></span></code></pre>
</div>
<div class="section level4">
<h4 id="cnn-part-2--hidden-layers">CNN Part 2. Hidden Layers<a class="anchor" aria-label="anchor" href="#cnn-part-2--hidden-layers"></a>
</h4>
<p>The next component consists of the so-called hidden layers of the
network. The reason they are referred to as hidden is because the true
values of their nodes are unknown.</p>
<p>In a CNN, the hidden layers typically consist of convolutional,
pooling, reshaping (e.g., Flatten), and dense layers.</p>
<p>Check out the <a href="https://keras.io/api/layers/" class="external-link">Layers API</a>
section of the Keras documentation for each layer type and its
parameters.</p>
<div class="section level5">
<h5 id="convolutional-layers">
<strong>Convolutional Layers</strong><a class="anchor" aria-label="anchor" href="#convolutional-layers"></a>
</h5>
<p>A <strong>convolutional</strong> layer is a fundamental building
block in a CNN designed for processing structured grid data, such as
images. It applies convolution operations to input data using learnable
filters or kernels, extracting local patterns and features (e.g. edges,
corners). These filters enable the network to capture hierarchical
representations of visual information, allowing for effective feature
learning.</p>
<p>To find the particular features of an image, CNNs make use of a
concept from image processing that precedes Deep Learning.</p>
<p>A <strong>convolution matrix</strong>, or <strong>kernel</strong>, is
a matrix transformation that we ‘slide’ over the image to calculate
features at each position of the image. For each pixel, we calculate the
matrix product between the kernel and the pixel with its surroundings.
Here is one example of a 3x3 kernel used to detect edges:</p>
<pre><code>[[-1, -1, -1],
 [0,   0,  0]
 [1,   1,  1]]</code></pre>
<p>This kernel will give a high value to a pixel if it is on a
horizontal border between dark and light areas.</p>
<p>In the following image, the effect of such a kernel on the values of
a single-channel image stands out. The red cell in the output matrix is
the result of multiplying and summing the values of the red square in
the input, and the kernel. Applying this kernel to a real image
demonstrates it does indeed detect horizontal edges.</p>
<figure><img src="fig/03_conv_matrix.png" alt="6x5 input matrix representing a single colour channel image being multipled by a 3x3 kernel to produce a 4x4 output matrix to detect horizonal edges in an image " class="figure mx-auto d-block"></figure><figure><img src="fig/03_conv_image.png" alt="single colour channel image of a cat multiplied by a 3x3 kernel to produce an image of a cat where the edges  stand out" class="figure mx-auto d-block"></figure><p>There are several types of convolutional layers available in Keras
depending on your application. We use the two-dimensional layer
typically used for images, <code>tf.keras.layers.Conv2D</code>.</p>
<p>We define arguments for the number of filters, the kernel size, and
the activation function.</p>
<pre><code><span><span class="co"># # Convolutional layer with 32 filters, 3x3 kernel size, and ReLU activation</span></span>
<span><span class="co"># x_intro = keras.layers.Conv2D(32, (3, 3), activation='relu')(inputs_intro)</span></span></code></pre>
<p>The instantiation here has three parameters and a seemingly strange
combination of parentheses, so let us break it down.</p>
<ul>
<li>The first parameter is the number of filters in this layer. This is
one of the hyperparameters of our system and should be chosen carefully.
<ul>
<li>Good practice is to start with a relatively small number of filters
in the first layer to prevent overfitting.</li>
<li>Choosing a number of filters as a power of two (e.g., 32, 64, 128)
is common.</li>
</ul>
</li>
<li>The second parameter is the kernel size which we already discussed.
Smaller kernels are often used to capture fine-grained features and
odd-sized filters are preferred because they have a centre pixel which
helps maintain spatial symmetry during convolutions.</li>
<li>The third parameter is the activation function to use.
<ul>
<li>Here we choose <strong>relu</strong> which is one of the most
commonly used in deep neural networks that is proven to work well.</li>
<li>We will discuss activation functions later in <strong>Step 9. Tune
hyperparameters</strong> but to satisfy your curiosity,
<code>ReLU</code> stands for Rectified Linear Unit (ReLU).</li>
</ul>
</li>
<li>Next is an extra set of parenthenses with inputs in them that means
after an instance of the Conv2D layer is created, it can be called as if
it was a function. This tells the Conv2D layer to connect the layer
passed as a parameter, in this case the inputs.</li>
<li>Finally, we store a reference so we can pass it to the next
layer.</li>
</ul>
<div id="playing-with-convolutions" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="playing-with-convolutions" class="callout-inner">
<h3 class="callout-title">Playing with convolutions<a class="anchor" aria-label="anchor" href="#playing-with-convolutions"></a>
</h3>
<div class="callout-content">
<p>Convolutions applied to images can be hard to grasp at first.
Fortunately, there are resources out there that enable users to
interactively play around with images and convolutions:</p>
<ul>
<li><p><a href="https://setosa.io/ev/image-kernels/" class="external-link">Image kernels
explained</a> illustrates how different convolutions can achieve certain
effects on an image, like sharpening and blurring.</p></li>
<li><p>The <a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks" class="external-link">convolutional
neural network cheat sheet</a> provides animated examples of the
different components of convolutional neural nets.</p></li>
</ul>
</div>
</div>
</div>
<div id="challenge-border-pixels" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-border-pixels" class="callout-inner">
<h3 class="callout-title">CHALLENGE Border pixels<a class="anchor" aria-label="anchor" href="#challenge-border-pixels"></a>
</h3>
<div class="callout-content">
<p>What do you think happens to the border pixels when applying a
convolution?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>There are different ways of dealing with border pixels.</p>
<ul>
<li>You can ignore them, which means your output image is slightly
smaller then your input.</li>
<li>It is also possible to ‘pad’ the borders, e.g., with the same value
or with zeros, so that the convolution can also be applied to the border
pixels. In that case, the output image will have the same size as the
input image.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level5">
<h5 id="pooling-layers">
<strong>Pooling Layers</strong><a class="anchor" aria-label="anchor" href="#pooling-layers"></a>
</h5>
<p>The convolutional layers are often intertwined with
<strong>Pooling</strong> layers. As opposed to the convolutional layer
used in feature extraction, the pooling layer alters the dimensions of
the image and reduces it by a scaling factor effectively decreasing the
resolution of your picture.</p>
<p>The rationale behind this is that higher layers of the network should
focus on higher-level features of the image. By introducing a pooling
layer, the subsequent convolutional layer has a broader ‘view’ on the
original image.</p>
<p>Similar to convolutional layers, Keras offers several pooling layers
and one used for images (2D spatial data) is the
<code>tf.keras.layers.MaxPooling2D</code> class.</p>
<pre><code><span><span class="co"># # Pooling layer with input window sized 2,2</span></span>
<span><span class="co"># x_intro = keras.layers.MaxPooling2D((2, 2))(x_intro)</span></span></code></pre>
<p>The instantiation here has a single parameter, pool_size.</p>
<p>The function downsamples the input along its spatial dimensions
(height and width) by taking the <strong>maximum</strong> value over an
input window (of size defined by pool_size) for each channel of the
input. By taking the maximum instead of the average, the most prominent
features in the window are emphasized.</p>
<p>A 2x2 pooling size reduces the width and height of the input by a
factor of 2. Empirically, a 2x2 pooling size has been found to work well
in various for image classification tasks and also strikes a balance
between down-sampling for computational efficiency and retaining
important spatial information.</p>
</div>
<div class="section level5">
<h5 id="dense-layers">
<strong>Dense layers</strong><a class="anchor" aria-label="anchor" href="#dense-layers"></a>
</h5>
<p>A <strong>dense</strong> layer has a number of neurons, which is a
parameter you choose when you create the layer. When connecting the
layer to its input and output layers every neuron in the dense layer
gets an edge (i.e. connection) to <strong>all</strong> of the input
neurons and <strong>all</strong> of the output neurons.</p>
<figure><img src="fig/03-neural_network_sketch_dense.png" alt="diagram of a neural network with multiple inputs feeding into to two seperate dense layers with connections between all the inputs and outputs" class="figure mx-auto d-block"></figure><p>This layer is called fully connected, because all input neurons are
taken into account by each output neuron. It aggregates global
information about the features learned in previous layers to make a
decision about the class of the input.</p>
<p>In Keras, a densely-connected layer is defined by the
<code>tf.keras.layers.Dense</code> class.</p>
<pre><code><span><span class="co"># # Dense layer with 64 neurons and ReLU activation</span></span>
<span><span class="co"># x_intro = keras.layers.Dense(64, activation='relu')(x_intro)</span></span></code></pre>
<p>This instantiation has two parameters: the number of neurons and the
activation function.</p>
<p>The choice of how many neurons to specify is often determined through
experimentation and can impact the performance of our CNN. Too few
neurons may not capture complex patterns in the data but too many
neurons may lead to overfitting.</p>
<div id="challenge-number-of-parameters" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-number-of-parameters" class="callout-inner">
<h3 class="callout-title">CHALLENGE Number of parameters<a class="anchor" aria-label="anchor" href="#challenge-number-of-parameters"></a>
</h3>
<div class="callout-content">
<p>Suppose we create a single Dense (fully connected) layer with 100
hidden units that connects to the input pixels. How many parameters does
this layer have?</p>
<ul>
<li>A. 307200</li>
<li>B. 307300</li>
<li>C. 100</li>
<li>D. 3072</li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p>The correct answer is B.</p>
<p>Each entry of the input dimensions is connected with 100 neurons of
our hidden layer, and each of these neurons has a bias term associated
to it. So we have 307300 parameters to learn.</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>width, height <span class="op">=</span> (<span class="dv">32</span>, <span class="dv">32</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>n_hidden_neurons <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>n_bias <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>n_input_items <span class="op">=</span> width <span class="op">*</span> height <span class="op">*</span> <span class="dv">3</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>n_parameters <span class="op">=</span> (n_input_items <span class="op">*</span> n_hidden_neurons) <span class="op">+</span> n_bias</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(n_parameters)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="fl">307300</span></span></code></pre>
</div>
<p>We can also check this by building the layer in Keras:</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>inputs_ex <span class="op">=</span> keras.Input(shape<span class="op">=</span>n_input_items)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>outputs_ex <span class="op">=</span> keras.layers.Dense(<span class="dv">100</span>)(inputs_ex)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>model_ex <span class="op">=</span> keras.models.Model(inputs<span class="op">=</span>inputs_ex, outputs<span class="op">=</span>outputs_ex)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>model_ex.summary()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         [(None, 3072)]            0
_________________________________________________________________
dense (Dense)                (None, 100)               307300
=================================================================
Total params: 307,300
Trainable params: 307,300
Non-trainable params: 0</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level5">
<h5 id="reshaping-layers-flatten">
<strong>Reshaping Layers: Flatten</strong><a class="anchor" aria-label="anchor" href="#reshaping-layers-flatten"></a>
</h5>
<p>The next type of hidden layer used in our introductory model is a
type of reshaping layer defined in Keras by the
<code>tf.keras.layers.Flatten</code> class. It is necessary when
transitioning from convolutional and pooling layers to fully connected
layers.</p>
<pre><code><span><span class="co"># # Flatten layer to convert 2D feature maps into a 1D vector</span></span>
<span><span class="co"># x_intro = keras.layers.Flatten()(x_intro)</span></span></code></pre>
<p>The <strong>Flatten</strong> layer converts the output of the
previous layer into a single one-dimensional vector that can be used as
input for a dense layer.</p>
</div>
</div>
<div class="section level4">
<h4 id="cnn-part-3--output-layer">CNN Part 3. Output Layer<a class="anchor" aria-label="anchor" href="#cnn-part-3--output-layer"></a>
</h4>
<p>Recall for the outputs we asked ourselves what we want to identify
from the data. If we are performing a classification problem, then
typically we have one output for each potential class. We finish with a
Dense layer to connect the output cells of the convolutional layer to
the outputs for our 10 classes.</p>
<p>Note the use of <code>softmax</code> activation for this Dense layer
as opposed to the <code>ReLU</code> activation used above. We use
softmax for multiclass data because it helps the computer give each
option (class) a likelihood score, and the scores add up to 100 per
cent. This way, it’s easier to pick the one the computer thinks is most
probable.</p>
<pre><code><span><span class="co"># # Output layer with 10 units (one for each class) and softmax activation</span></span>
<span><span class="co"># outputs_intro = keras.layers.Dense(10, activation='softmax')(x_intro))</span></span></code></pre>
</div>
</div>
</section><section id="putting-it-all-together"><h2 class="section-heading">Putting it all together<a class="anchor" aria-label="anchor" href="#putting-it-all-together"></a>
</h2>
<hr class="half-width">
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co">#### Define the Model</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co"># CNN Part 1</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Input layer of 32x32 images with three channels (RGB)</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>inputs_intro <span class="op">=</span> keras.Input(shape<span class="op">=</span>train_images.shape[<span class="dv">1</span>:])</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co"># CNN Part 2</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Convolutional layer with 32 filters, 3x3 kernel size, and ReLU activation</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>x_intro <span class="op">=</span> keras.layers.Conv2D(<span class="dv">16</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(inputs_intro)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Pooling layer with input window sized 2,2</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>x_intro <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x_intro)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Second Convolutional layer with 32 filters, 3x3 kernel size, and ReLU activation</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>x_intro <span class="op">=</span> keras.layers.Conv2D(<span class="dv">32</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x_intro)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Second Pooling layer with input window sized 2,2</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>x_intro <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x_intro)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Flatten layer to convert 2D feature maps into a 1D vector</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>x_intro <span class="op">=</span> keras.layers.Flatten()(x_intro)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Dense layer with 64 neurons and ReLU activation</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>x_intro <span class="op">=</span> keras.layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x_intro)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a><span class="co"># CNN Part 3</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Output layer with 10 units (one for each class) and softmax activation</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>outputs_intro <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">'softmax'</span>)(x_intro)</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a><span class="co"># create the model</span></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>model_intro <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs_intro, outputs<span class="op">=</span>outputs_intro, name<span class="op">=</span><span class="st">"cifar_model_intro"</span>)</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a><span class="co"># view the model summary</span></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>model_intro.summary()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Model: "cifar_model_intro"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 32, 32, 3)]       0         
                                                                 
 conv2d (Conv2D)             (None, 30, 30, 16)        448       
                                                                 
 max_pooling2d (MaxPooling2  (None, 15, 15, 16)        0         
 D)                                                              
                                                                 
 conv2d_1 (Conv2D)           (None, 13, 13, 32)        4640      
                                                                 
 max_pooling2d_1 (MaxPoolin  (None, 6, 6, 32)          0         
 g2D)                                                            
                                                                 
 flatten (Flatten)           (None, 1152)              0         
                                                                 
 dense (Dense)               (None, 64)                73792     
                                                                 
 dense_1 (Dense)             (None, 10)                650       
                                                                 
=================================================================
Total params: 79530 (310.66 KB)
Trainable params: 79530 (310.66 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________</code></pre>
</div>
<div id="how-to-choose-an-architecture" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="how-to-choose-an-architecture" class="callout-inner">
<h3 class="callout-title">How to choose an architecture?<a class="anchor" aria-label="anchor" href="#how-to-choose-an-architecture"></a>
</h3>
<div class="callout-content">
<p>Even for this neural network, we had to make a choice on the number
of hidden neurons. Other choices to be made are the number of layers and
type of layers. You might wonder how you should make these architectural
choices. Unfortunately, there are no clear rules to follow here, and it
often boils down to a lot of trial and error. However, it is recommended
to explore what others have done with similar datasets and problems.
Another best practice is to start with a relatively simple architecture.
Once running start to add layers and tweak the network to test if
performance increases.</p>
</div>
</div>
</div>
</section><section id="we-have-a-model-now-what"><h2 class="section-heading">We have a model now what?<a class="anchor" aria-label="anchor" href="#we-have-a-model-now-what"></a>
</h2>
<hr class="half-width">
<p>This CNN should be able to run with the CIFAR-10 dataset and provide
reasonable results for basic classification tasks. However, do keep in
mind this model is relatively simple, and its performance may not be as
high as more complex architectures. The reason it’s called deep learning
is because in most cases, the more layers we have, i.e. the deeper and
more sophisticated CNN architecture we use, the better the
performance.</p>
<p>How can we tell? We can inspect a couple metrics produced during the
training process to detect whether our model is underfitting or
overfitting. To do that, we continue with the next steps in our Deep
Learning workflow, <strong>Step 5. Choose a loss function and
optimizer</strong> and <strong>Step 6. Train model</strong>.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Artificial neural networks (ANN) are a machine learning technique
based on a model inspired by groups of neurons in the brain.</li>
<li>Convolution neural networks (CNN) are a type of ANN designed for
image classification and object detection.</li>
<li>The number of filters corresponds to the number of distinct features
the layer is learning to recognise whereas the kernel size determines
the level of features being captured.</li>
<li>A CNN can consist of many types of layers including convolutional,
pooling, flatten, and dense (fully connected) layers</li>
<li>Convolutional layers are responsible for learning features from the
input data.</li>
<li>Pooling layers are often used to reduce the spatial dimensions of
the data.</li>
<li>The flatten layer is used to convert the multi-dimensional output of
the convolutional and pooling layers into a flat vector.</li>
<li>Dense layers are responsible for combining features learned by the
previous layers to perform the final classification.</li>
</ul>
</div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-04-fit-cnn"><p>Content from <a href="04-fit-cnn.html">Compile and Train (Fit) a Convolutional Neural Network</a></p>
<hr>
<p>Last updated on 2024-02-25 |
        
        <a href="https://https://github.com/erinmgraham/icwithcnn/edit/main/episodes/04-fit-cnn.html" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How do you compile a convolutional neural network (CNN)?</li>
<li>What is a loss function?</li>
<li>What is an optimizer?</li>
<li>How do you train (fit) a CNN?</li>
<li>How do you evaluate a model during training?</li>
<li>What is overfitting?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Explain the difference between compiling and training (fitting) a
CNN.</li>
<li>Know how to select a loss function for your model.</li>
<li>Understand what an optimizer is.</li>
<li>Define the terms: learning rate, batch size, epoch.</li>
<li>Understand what loss and accuracy are and how to monitor them during
training.</li>
<li>Explain what overfitting is and what to do about it.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="step-5--choose-a-loss-function-and-optimizer-and-compile-model">Step 5. Choose a loss function and optimizer and compile model<a class="anchor" aria-label="anchor" href="#step-5--choose-a-loss-function-and-optimizer-and-compile-model"></a>
</h3>
<p>We have designed a convolutional neural network (CNN) that in theory
we should be able to train to classify images.</p>
<p>We now need to <strong>compile</strong> the model, or set up the
rules and strategies for how the network will learn. To do this, we
select an appropriate loss function and optimizer to use during training
(fitting).</p>
<p>Recall how we compiled our model in the introduction:</p>
<pre><code><span><span class="co">## compile the model</span></span>
<span><span class="co">#model_intro.compile(optimizer = 'adam', </span></span>
<span><span class="co">#                    loss = keras.losses.CategoricalCrossentropy(), </span></span>
<span><span class="co">#                    metrics = ['accuracy'])</span></span></code></pre>
<div class="section level4">
<h4 id="loss-function">Loss function<a class="anchor" aria-label="anchor" href="#loss-function"></a>
</h4>
<p>The <strong>loss function</strong> tells the training algorithm how
wrong, or how ‘far away’ from the true value the predicted value is. The
purpose of loss functions is to compute the quantity that a model should
seek to minimize during training. Which class of loss functions you
choose depends on your task.</p>
<p><strong>Loss for classification</strong></p>
<p>For classification purposes, there are a number of probabilistic
losses to choose from. We chose <code>CategoricalCrossentropy</code>
because we want to compute the crossentropy loss between our one-hot
encoded class labels and the model predictions. This loss function is
appropriate to use when the data has two or more label classes.</p>
<p>The loss function is defined by the
<code>tf.keras.losses.CategoricalCrossentropy</code> class.</p>
<p>More information about loss functions can be found in the Keras <a href="https://keras.io/api/losses/" class="external-link">loss documentation</a>.</p>
</div>
<div class="section level4">
<h4 id="optimizer">Optimizer<a class="anchor" aria-label="anchor" href="#optimizer"></a>
</h4>
<p>Somewhat coupled to the loss function is the
<strong>optimizer</strong>. The optimizer here refers to the algorithm
with which the model learns to optimize on the provided loss
function.</p>
<p>We need to choose which optimizer to use and, if this optimizer has
parameters, what values to use for those. Furthermore, we specify how
many times to present the training samples to the optimizer. In other
words, the optimizer is responsible for taking the output of the loss
function and then applying some changes to the weights within the
network. It is through this process that the “learning” (adjustment of
the weights) is achieved.</p>
<pre><code><span><span class="co">## compile the model</span></span>
<span><span class="co">#model_intro.compile(optimizer = 'adam', </span></span>
<span><span class="co">#                    loss = keras.losses.CategoricalCrossentropy(), </span></span>
<span><span class="co">#                    metrics = ['accuracy'])</span></span></code></pre>
<p><strong>Adam</strong></p>
<p>Here we picked one of the most common optimizers demonstrated to work
well for most tasks, the <strong>Adam</strong> optimizer. Similar to
other hyperparameters, the choice of optimizer depends on the problem
you are trying to solve, your model architecture, and your data. Adam is
a good starting point though, which is why we chose it. Adam has a
number of parameters, but the default values work well for most problems
so we will use it with its default parameters.</p>
<p>It is defined by the <code>keras.optimizers.Adam</code> class and
takes a single parameter <code>learning_rate=0.01</code></p>
<p><strong>Learning rate</strong> is a hyperparameter that determines
the step size at which the model’s weights are updated during training.
You can think of like the pace of learning for your model because it’s
basically how big (or small) a step your model takes to learn from its
mistakes. Too high, and it might overshoot the optimal values; too low,
and it might take forever to learn.</p>
<p>The learning rate may be fixed and remain constant throughout the
entire training process or it may be adaptive and dynamically change
during training.</p>
<p>The <a href="https://keras.io/api/optimizers/" class="external-link">optimizer
documentation</a> describes the optimizers to choose. A couple more
popular or famous ones include:</p>
<ul>
<li><p><strong>Stochastic Gradient Descent (sgd)</strong>: Stochastic
Gradient Descent (SGD) is one of the fundamental optimization algorithms
used to train machine learning models, especially neural networks. It is
a variant of the gradient descent algorithm, designed to handle large
datasets efficiently.</p></li>
<li>
<p><strong>Root Mean Square (rms)prop</strong>: RMSprop is widely
used in various deep learning frameworks and is one of the predecessors
of more advanced optimizers like Adam, which further refines the concept
of adaptive learning rates. It is an extension of the basic Stochastic
Gradient Descent (SGD) algorithm and addresses some of the challenges of
SGD.</p>
<ul>
<li>For example, one of the main issues with the basic SGD is that it
uses a fixed learning rate for all model parameters throughout the
training process. This fixed learning rate can lead to slow convergence
or divergence (over-shooting) in some cases. RMSprop introduces an
adaptive learning rate mechanism to address this problem.</li>
</ul>
</li>
</ul>
<div id="accordionSpoiler1" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler1" aria-expanded="false" aria-controls="collapseSpoiler1">
  <h3 class="accordion-header" id="headingSpoiler1">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>WANT TO KNOW MORE: Learning Rate</h3>
</button>
<div id="collapseSpoiler1" class="accordion-collapse collapse" aria-labelledby="headingSpoiler1" data-bs-parent="#accordionSpoiler1">
<div class="accordion-body">
<p>ChatGPT</p>
<p><strong>Learning rate</strong> is a hyperparameter that determines
the step size at which the model’s parameters are updated during
training. A higher learning rate allows for more substantial parameter
updates, which can lead to faster convergence, but it may risk
overshooting the optimal solution. On the other hand, a lower learning
rate leads to smaller updates, providing more cautious convergence, but
it may take longer to reach the optimal solution. Finding an appropriate
learning rate is crucial for effectively training machine learning
models.</p>
<p>The figure below illustrates a small learning rate will not traverse
toward the minima of the gradient descent algorithm in a timely manner,
i.e. number of epochs.</p>
<figure><img src="https://developers.google.com/static/machine-learning/crash-course/images/LearningRateTooSmall.svg" title="Small learning rate leads to inefficient approach to loss minima" alt="Plot of loss over weight value illustrating how a small learning rate takes a long time to reach the optimal solution." class="figure mx-auto d-block"><figcaption>Small learning rate leads to inefficient approach to
loss minima</figcaption></figure><p>On the other hand, specifying a learning rate that is <em>too
high</em> will result in a loss value that never approaches the minima.
That is, ‘bouncing between the sides’, thus never reaching a minima to
cease learning.</p>
<figure><img src="https://developers.google.com/static/machine-learning/crash-course/images/LearningRateTooLarge.svg" alt="Plot of loss over weight value illustrating how a large learning rate never approaches the optimal solution because it bounces between the sides." class="figure mx-auto d-block"><figcaption>A large learning rate results in overshooting the
gradient descent minima</figcaption></figure><p>Finally, a modest learning rate will ensure that the product of
multiplying the scalar gradient value and the learning rate does not
result in too small steps, nor a chaotic bounce between sides of the
gradient where steepness is greatest.</p>
<figure><img src="https://developers.google.com/static/machine-learning/crash-course/images/LearningRateJustRight.svg" alt="Plot of loss over weight value illustrating how a good learning rate gets to optimal solution gradually." class="figure mx-auto d-block"><figcaption>An optimal learning rate supports a gradual approach
to the minima</figcaption></figure><p>These images were obtained from <a href="https://developers.google.com/machine-learning/crash-course/reducing-loss/learning-rate" class="external-link">Google
Developers Machine Learning Crash Course</a> and is licenced under the
<a href="https://creativecommons.org/licenses/by/4.0/" class="external-link">Creative Commons
4.0 Attribution Licence</a>.</p>
</div>
</div>
</div>
</div>
</div>
<div class="section level4">
<h4 id="metrics">Metrics<a class="anchor" aria-label="anchor" href="#metrics"></a>
</h4>
<p>After we select the desired optimizer and loss function we specify
the metric(s) to be evaluated by the model during training and testing.
A <strong>metric</strong> is a function used to judge the performance of
your model.</p>
<pre><code><span><span class="co">## compile the model</span></span>
<span><span class="co">#model_intro.compile(optimizer = 'adam', </span></span>
<span><span class="co">#                    loss = keras.losses.CategoricalCrossentropy(), </span></span>
<span><span class="co">#                    metrics = ['accuracy']) </span></span></code></pre>
<p>Metric functions are similar to loss functions, except the results
from evaluating a metric are not used when training the model. Note you
are able to use any loss function as a metric.</p>
<p>Typically, for classification problems, you will use
<code>accuracy</code>, which calculates how often the model predictions
match the true labels.</p>
<p>The accuracy function creates two local variables, total and count,
that it uses to compute the frequency with which predictions matches
labels. This frequency is ultimately returned as accuracy: an operation
that divides the total by count.</p>
<p>The Keras <a href="https://keras.io/api/metrics/" class="external-link">metrics</a>
documentation provides a list of potential metrics.</p>
<p>Now that we selected which loss function, optimizer, and metric to
use, we compile the model using <code>model.compile</code>. Compiling
the model prepares it for training.</p>
</div>
</div>
<div class="section level3">
<h3 id="step-6--train-fit-model">Step 6. Train (Fit) model<a class="anchor" aria-label="anchor" href="#step-6--train-fit-model"></a>
</h3>
<p>We are ready to train the model.</p>
<p>Training the model is done using the <code>fit</code> method. It
takes the image data and target (label) data as inputs and has several
other parameters for certain options of the training. Here we only set a
different number of epochs.</p>
<p>A training <strong>epoch</strong> means that every sample in the
training data has been given to the neural network and used to update
its parameters. In general, CNN models improve with more epochs of
training, but only to a point.</p>
<p>We want to train our model for 10 epochs:</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the model</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>history_intro <span class="op">=</span> model_intro.fit(train_images, train_labels, </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>                                epochs <span class="op">=</span> <span class="dv">10</span>, </span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>                                validation_data <span class="op">=</span> (val_images, val_labels),</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>                                batch_size <span class="op">=</span> <span class="dv">32</span>)</span></code></pre>
</div>
<p>The <code>batch_size</code> parameter defaults to 32. The
<strong>batch size</strong> is an important hyperparameter that
determines the number of training samples processed together before
updating the model’s parameters during each iteration (or mini-batch) of
training.</p>
<p>In general, smaller batch sizes may require more iterations to cover
the entire dataset, which can lead to longer training times. Larger
batch sizes contribute to a smoother learning process, i.e. more
consistent updates to the model’s parameters, but might not generalise
well to new, unseen data.</p>
<p>Note we are also creating a new variable <code>history_intro</code>
to capture the history of the training in order to extract metrics we
will use for model evaluation.</p>
<p>Other arguments used to fit our model can be found in the
documentation for the <a href="https://keras.io/api/models/model_training_apis/" class="external-link">fit
method</a>.</p>
<div id="accordionSpoiler2" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler2" aria-expanded="false" aria-controls="collapseSpoiler2">
  <h3 class="accordion-header" id="headingSpoiler2">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>WANT TO KNOW MORE: Batch size</h3>
</button>
<div id="collapseSpoiler2" class="accordion-collapse collapse" aria-labelledby="headingSpoiler2" data-bs-parent="#accordionSpoiler2">
<div class="accordion-body">
<p>ChatGPT</p>
<p>The choice of batch size can have various implications, and there are
situations where using different batch sizes can be beneficial. There is
no one-size-fits-all answer.</p>
<p><strong>Large Datasets and Memory Constraints</strong>: If you have a
large dataset and limited memory, using a smaller batch size can help
fit the data into memory during training. This allows you to train
larger models or use more complex architectures that might not fit with
larger batch sizes.</p>
<p><strong>Training on GPUs</strong>: Modern deep learning frameworks
and libraries are optimized for parallel processing on GPUs. Using
larger batch sizes can fully leverage the parallelism of GPUs and lead
to faster training times. However, the choice of batch size should
consider the available GPU memory.</p>
<p><strong>Noise in Parameter Updates</strong>: Smaller batch sizes
introduce more noise in the gradients, which can help models escape
sharp minima and potentially find better solutions. This regularization
effect is similar to the impact of stochasticity in Stochastic Gradient
Descent (SGD).</p>
<p><strong>Generalization</strong>: Using smaller batch sizes may
improve the generalization of the model. It prevents the model from
overfitting to the training data, as it gets updated more frequently and
experiences more diverse samples during training.</p>
<p>It’s essential to consider the trade-offs of using different batch
sizes. While larger batch sizes may provide more stable gradients during
training, there could be a trade-off where the model might not
generalise as effectively to new, unseen data. It’s a common
consideration in machine learning to find a balance between stable
training and the ability to generalize well to diverse examples. You
should experiment with different batch sizes to find the best-performing
one for your specific model, architecture, and dataset.</p>
</div>
</div>
</div>
</div>
<div class="section level4">
<h4 id="monitor-training-progress-aka-model-evaluation-during-training">Monitor Training Progress (aka Model Evaluation during
Training)<a class="anchor" aria-label="anchor" href="#monitor-training-progress-aka-model-evaluation-during-training"></a>
</h4>
<p>We now know more about the compilation and fitting of CNNs. Let us
inspect the training metrics for our model.</p>
<p>Using seaborn, we can plot the training process using the
history:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># convert the history to a dataframe for plotting </span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>history_intro_df <span class="op">=</span> pd.DataFrame.from_dict(history_intro.history)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the loss and accuracy from the training process</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">'cifar_model_intro'</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>sns.lineplot(ax<span class="op">=</span>axes[<span class="dv">0</span>], data<span class="op">=</span>history_intro_df[[<span class="st">'loss'</span>, <span class="st">'val_loss'</span>]])</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>sns.lineplot(ax<span class="op">=</span>axes[<span class="dv">1</span>], data<span class="op">=</span>history_intro_df[[<span class="st">'accuracy'</span>, <span class="st">'val_accuracy'</span>]])</span></code></pre>
</div>
<figure><img src="fig/04_model_intro_accuracy_loss.png" alt="two panel figure; the figure on the left illustrates the training loss starting at 1.5 and decreasing to 0.7 and the validation loss decreasing from 1.3 to 1.0 before leveling out; the figure on the right illustrates the training accuracy increasing from 0.45 to 0.75 and the validation accuracy increasing from 0.53 to 0.65 before leveling off" class="figure mx-auto d-block"></figure><p>This plot is used to identify whether the training is well configured
or whether there are problems to address. The solid blue lines represent
the training loss and accuracy; the dashed orange lines represent the
validation loss and accuracy.</p>
<div id="challenge-inspect-the-training-curve" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-inspect-the-training-curve" class="callout-inner">
<h3 class="callout-title">CHALLENGE Inspect the Training Curve<a class="anchor" aria-label="anchor" href="#challenge-inspect-the-training-curve"></a>
</h3>
<div class="callout-content">
<p>Inspect the training curves we have just made and recall the
difference between the training and the validation datasets.</p>
<ol style="list-style-type: decimal">
<li>How does the training progress look?</li>
</ol>
<ul>
<li>Does the loss increase or decrease?</li>
<li>What about the accuracy?</li>
<li>Do either change fast or slowly?</li>
<li>Do the graphs lines fluctuate or go up and down frequently?</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Do you think the resulting trained network will work well on the
test set?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<ol style="list-style-type: decimal">
<li>Key things to look for for:</li>
</ol>
<ul>
<li>Loss
<ul>
<li>The loss curve should drop quickly in a relatively smooth line with
little to no fluctuations.</li>
<li>The val_loss curve should decrease along with the loss.</li>
</ul>
</li>
<li>Accuracy
<ul>
<li>The accuracy should increase quickly in a relatively smooth line
with little to no fluctuations.</li>
<li>The val_accuracy should behave similarly</li>
</ul>
</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>The results of the training give very little information on its
performance on a test set. You should be careful not to use it as an
indication of a well trained network.</li>
</ol>
</div>
</div>
</div>
</div>
<p>Note the training loss continues to decrease, while the validation
loss stagnates, and even starts to increase over the course of the
epochs. Similarly, the accuracy for the validation set does not improve
anymore after some epochs.</p>
<p>This is evidence of <strong>overfitting</strong> in these plots. If a
model is overfitting, it means the model performs exceptionally well on
the training data, but poorly on the validation data. Overfitting occurs
when the model has learned to memorize the noise and specific patterns
in the training data instead of generalizing the underlying
relationships. As a result, the model fails to perform well on new,
unseen, data because it has become too specialized to the training
set.</p>
<p>Key characteristics of an overfit model include:</p>
<ul>
<li><p>High Training Accuracy, Low Validation Accuracy: The model
achieves high accuracy on the training data but significantly lower
accuracy on the validation (or test) data.</p></li>
<li><p>Small Training Loss, Large Validation Loss: The training loss is
low, indicating the model’s predictions closely match the true labels in
the training set. However, the validation loss is high, indicating the
model’s predictions are far from the true labels in the validation
set.</p></li>
</ul>
<p>How to Address Overfitting:</p>
<ul>
<li>Reduce the model’s complexity by using fewer layers or units to make
it less prone to overfitting.</li>
<li>Collect more training data if possible to provide the model with a
diverse and representative dataset.</li>
<li>Perform data augmentation to artificially increase the size of the
training data and introduce variability.</li>
</ul>
<div id="accordionSpoiler3" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler3" aria-expanded="false" aria-controls="collapseSpoiler3">
  <h3 class="accordion-header" id="headingSpoiler3">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>WANT TO KNOW MORE: What is underfitting?</h3>
</button>
<div id="collapseSpoiler3" class="accordion-collapse collapse" aria-labelledby="headingSpoiler3" data-bs-parent="#accordionSpoiler3">
<div class="accordion-body">
<p>Underfitting occurs when the model is too simple or lacks the
capacity to capture the underlying patterns and relationships present in
the data. As a result, the model’s predictions are not accurate, and it
fails to generalize well to unseen data.</p>
<p>Key characteristics of an underfit model include:</p>
<ul>
<li>Large Training Loss: The training loss (error) is high, indicating
the model’s predictions are far from the true labels in the training
set.</li>
<li>Increasing validation loss.</li>
<li>Low Validation Accuracy: This indicates the model is not learning
from the data effectively.</li>
</ul>
<p>How to address underfitting:</p>
<ul>
<li>Perform data augmentation or feature engineering to provide the
model with more informative input features.</li>
<li>Train the model for more epochs to give it more time to learn from
the data.</li>
<li>Increase the model’s complexity by adding more layers or units to
the existing layers.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="improve-model-generalization-avoid-overfitting">Improve Model Generalization (avoid Overfitting)<a class="anchor" aria-label="anchor" href="#improve-model-generalization-avoid-overfitting"></a>
</h3>
<p>Techniques to avoid overfitting, or to improve model generalization,
are termed <strong>regularization techniques</strong>.</p>
<div id="accordionSpoiler4" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler4" aria-expanded="false" aria-controls="collapseSpoiler4">
  <h3 class="accordion-header" id="headingSpoiler4">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>WANT TO KNOW MORE: Regularization techniques for CNNs</h3>
</button>
<div id="collapseSpoiler4" class="accordion-collapse collapse" aria-labelledby="headingSpoiler4" data-bs-parent="#accordionSpoiler4">
<div class="accordion-body">
<p>ChatGPT</p>
<p><strong>Regularization</strong> methods introduce constraints or
penalties to the training process, encouraging the model to be simpler
and less prone to overfitting. Here are some common regularization
methods for CNNs:</p>
<p><strong>L1 and L2 Regularization</strong>: L1 and L2 regularization
are the two most common regularization techniques used in deep learning.
They add a penalty term to the loss function during training to restrict
the model’s weights.</p>
<ul>
<li><p>L1 regularization adds the absolute value of the weights to the
loss function. It tends to produce sparse weight vectors, forcing some
of the less important features to have exactly zero weights.</p></li>
<li><p>L2 regularization adds the square of the weights to the loss
function. It encourages the model to have smaller weights overall,
preventing extreme values and reducing the impact of individual
features.</p></li>
</ul>
<p>The regularization strength is controlled by a hyperparameter, often
denoted as lambda (λ), that determines how much weight should be given
to the regularization term. A larger λ value increases the impact of
regularization, making the model simpler and more regularized.</p>
<p><strong>Dropout</strong>: Involves randomly “dropping out” a fraction
of neurons during training. This means during each training iteration,
some neurons are temporarily removed from the network. Dropout
effectively reduces the interdependence between neurons, preventing the
network from relying too heavily on specific neurons, and making it more
robust.</p>
<p><strong>Batch Normalization</strong>: While not explicitly a
regularization technique, Batch Normalization has a regularizing effect
on the model. It normalizes the activations of each layer in the
network, reducing internal covariate shift. This can improve training
stability and reduce the need for aggressive dropout or weight
decay.</p>
<p><strong>Data Augmentation</strong>: Data augmentation is a technique
where the training data is artificially augmented by applying various
transformations like rotation, scaling, flipping, and cropping to create
new examples. This increases the diversity of the training data and
helps the model generalize better to unseen data.</p>
<p><strong>Early Stopping</strong>: Early stopping is a form of
regularization that stops the training process when the model’s
performance on a validation set starts to degrade. It prevents the model
from overfitting by avoiding further training after the point of best
validation performance.</p>
<p>Using regularization techniques improves the generalization
performance of CNNs and reduces the risk of overfitting. It’s essential
to experiment with different regularization methods and hyperparameters
to find the optimal combination for your specific CNN architecture and
dataset.</p>
</div>
</div>
</div>
</div>
<div class="section level4">
<h4 id="dropout">Dropout<a class="anchor" aria-label="anchor" href="#dropout"></a>
</h4>
<p>One of the most versatile regularization technique is
<strong>dropout</strong> (Srivastava et al., 2014). Dropout essentially
means that during each training cycle a random fraction of the dense
layer nodes are turned off. This is described with the dropout rate
between zero and one, which determines the fraction of nodes to silence
at a time.</p>
<figure><img src="fig/04-neural_network_sketch_dropout.png" alt="diagram of two neural networks; the first network is densely connected without dropout and the second network has some of the neurons dropped out of of the network" class="figure mx-auto d-block"></figure><p>The intuition behind dropout is that it enforces redundancies in the
network by constantly removing different elements of a network. The
model can no longer rely on individual nodes and instead must create
multiple “paths”.</p>
<p>In addition, the model has to make predictions with much fewer nodes
and weights (connections between the nodes). As a result, it becomes
much harder for a network to memorize particular features. At first this
might appear a quite drastic approach which affects the network
architecture strongly. In practice, however, dropout is computationally
a very elegant solution which does not affect training speed. And it
frequently works very well.</p>
<div id="callout1" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout1"></a>
</h3>
<div class="callout-content">
<p>Dropout layers will only randomly silence nodes during training!
During a predictions step, all nodes remain active (dropout is off).
During training, the sample of nodes that are silenced are different for
each training instance, to give all nodes a chance to observe enough
training data to learn its weights.</p>
</div>
</div>
</div>
<p>Dropout layers are defined by the
<code>tf.keras.layers.Dropout</code> class and have the following
definition:</p>
<pre><code>tf.keras.layers.Dropout(rate, noise_shape=None, seed=None, **kwargs)</code></pre>
<p>The <code>rate</code> parameter is a float between 0 and 1 and
represents the fraction of the input units to drop.</p>
<p>We want to add one Dropout Layer to our network that randomly drops
80 per cent of the input units but where should we put it?</p>
<p>The placement of the dropout layer matters. Adding dropout before or
after certain layers can have different effects. For example, it’s
common to place dropout after convolutional and dense layers but not
typically after pooling layers. Let us add a third convolutional layer
to our model and then the dropout layer.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define the inputs, layers, and outputs of a CNN model with dropout</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># CNN Part 1</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Input layer of 32x32 images with three channels (RGB)</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>inputs_dropout <span class="op">=</span> keras.Input(shape<span class="op">=</span>train_images.shape[<span class="dv">1</span>:])</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co"># CNN Part 2</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Convolutional layer with 32 filters, 3x3 kernel size, and ReLU activation</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>x_dropout <span class="op">=</span> keras.layers.Conv2D(<span class="dv">16</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(inputs_dropout)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Pooling layer with input window sized 2,2</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>x_dropout <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x_dropout)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Second Convolutional layer with 32 filters, 3x3 kernel size, and ReLU activation</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>x_dropout <span class="op">=</span> keras.layers.Conv2D(<span class="dv">32</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x_dropout)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Second Pooling layer with input window sized 2,2</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>x_dropout <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x_dropout)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Second Convolutional layer with 64 filters, 3x3 kernel size, and ReLU activation</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>x_dropout <span class="op">=</span> keras.layers.Conv2D(<span class="dv">64</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x_dropout) <span class="co"># This is new!</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Dropout layer andomly drops 60 per cent of the input units</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>x_dropout <span class="op">=</span> keras.layers.Dropout(<span class="fl">0.6</span>)(x_dropout) <span class="co"># This is new!</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Flatten layer to convert 2D feature maps into a 1D vector</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>x_dropout <span class="op">=</span> keras.layers.Flatten()(x_dropout)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Dense layer with 128 neurons and ReLU activation</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>x_dropout <span class="op">=</span> keras.layers.Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x_dropout)</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a><span class="co"># CNN Part 3</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Output layer with 10 units (one for each class) and softmax activation</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>outputs_dropout <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">'softmax'</span>)(x_dropout)</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="co"># create the dropout model</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>model_dropout <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs_dropout, outputs<span class="op">=</span>outputs_dropout, name<span class="op">=</span><span class="st">"cifar_model_dropout"</span>)</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>model_dropout.summary()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Model: "cifar_model_dropout"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 32, 32, 3)]       0         
                                                                 
 conv2d_2 (Conv2D)           (None, 30, 30, 16)        448       
                                                                 
 max_pooling2d_2 (MaxPoolin  (None, 15, 15, 16)        0         
 g2D)                                                            
                                                                 
 conv2d_3 (Conv2D)           (None, 13, 13, 32)        4640      
                                                                 
 max_pooling2d_3 (MaxPoolin  (None, 6, 6, 32)          0         
 g2D)                                                            
                                                                 
 conv2d_4 (Conv2D)           (None, 4, 4, 64)          18496     
                                                                 
 dropout (Dropout)           (None, 4, 4, 64)          0         
                                                                 
 flatten_1 (Flatten)         (None, 1024)              0         
                                                                 
 dense_2 (Dense)             (None, 128)               131200    
                                                                 
 dense_3 (Dense)             (None, 10)                1290      
                                                                 
=================================================================
Total params: 156074 (609.66 KB)
Trainable params: 156074 (609.66 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________</code></pre>
</div>
<p>Note the dropout does not alter the dimensions of the image and has
zero parameters.</p>
<div id="challenge-does-adding-a-dropout-layer-improve-our-model" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-does-adding-a-dropout-layer-improve-our-model" class="callout-inner">
<h3 class="callout-title">CHALLENGE Does adding a Dropout Layer improve
our model?<a class="anchor" aria-label="anchor" href="#challenge-does-adding-a-dropout-layer-improve-our-model"></a>
</h3>
<div class="callout-content">
<p>Write the code to compile and fit our new dropout model using the
same arguments we used for our model in the introduction. Then inspect
the training metrics to determine whether our model has improved or not
by adding a dropout layer.</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># compile the dropout model</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>model_dropout.<span class="bu">compile</span>(optimizer <span class="op">=</span> <span class="st">'adam'</span>,</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>              loss <span class="op">=</span> keras.losses.CategoricalCrossentropy(),</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>              metrics <span class="op">=</span> [<span class="st">'accuracy'</span>])</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the dropout model</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>history_dropout <span class="op">=</span> model_dropout.fit(train_images, train_labels, </span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>                                    epochs <span class="op">=</span> <span class="dv">10</span>,</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>                                    validation_data<span class="op">=</span>(val_images, val_labels),</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>                                    batch_size <span class="op">=</span> <span class="dv">32</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co"># save dropout model</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>model_dropout.save(<span class="st">'fit_outputs/model_dropout.keras'</span>)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="co"># inspect the training results</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="co"># convert the history to a dataframe for plotting </span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>history_dropout_df <span class="op">=</span> pd.DataFrame.from_dict(history_dropout.history)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the loss and accuracy from the training process</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">'cifar_model_dropout'</span>)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>sns.lineplot(ax<span class="op">=</span>axes[<span class="dv">0</span>], data<span class="op">=</span>history_dropout_df[[<span class="st">'loss'</span>, <span class="st">'val_loss'</span>]])</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>sns.lineplot(ax<span class="op">=</span>axes[<span class="dv">1</span>], data<span class="op">=</span>history_dropout_df[[<span class="st">'accuracy'</span>, <span class="st">'val_accuracy'</span>]])</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>val_loss_dropout, val_acc_dropout <span class="op">=</span> model_dropout.evaluate(val_images, val_labels, verbose<span class="op">=</span><span class="dv">2</span>)</span></code></pre>
</div>
<figure><img src="fig/04_model_dropout_accuracy_loss.png" alt="two panel figure; the figure on the left illustrates the training loss starting at 1.7 and decreasing to 1.0 and the validation loss decreasing from 1.4 to 0.9 before leveling out; the figure on the right illustrates the training accuracy increasing from 0.40 to 0.65 and the validation accuracy increasing from 0.5 to 0.67" class="figure mx-auto d-block"></figure><p>In this relatively uncommon situation, the training loss is higher
than our validation loss while the validation accuracy is higher than
the training accuracy. Using dropout, or other regularization techniques
during training, can lead to a lower training accuracy.</p>
<p>Dropout randomly “drops out” units during training, which can prevent
the model from fitting the training data too closely. This
regularization effect may lead to a situation where the model
generalizes better on the validation set.</p>
<p>The final accuracy on the validation set is higher than without
dropout.</p>
</div>
</div>
</div>
</div>
</div>
</div>
<section id="choose-the-best-model-and-use-it-to-predict"><h2 class="section-heading">Choose the best model and use it to predict<a class="anchor" aria-label="anchor" href="#choose-the-best-model-and-use-it-to-predict"></a>
</h2>
<hr class="half-width">
<p>Based on our evaluation of the loss and accuracy metrics, the
<code>model_dropout</code> appears to have the best performance
<strong>of the models we have examined thus far</strong>. The next step
is to use this model to predict the object classes on our test
dataset.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Use Model.compile() to compile a CNN.</li>
<li>The choice of loss function will depend on your data and aim.</li>
<li>The choice of optimizer often depends on experimentation and
empirical evaluation.</li>
<li>Use Model.fit() to make a train (fit) a CNN.</li>
<li>Training/validation loss and accuracy can be used to evaluate a
model during training.</li>
<li>Dropout is one way to prevent overfitting.</li>
</ul>
</div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-05-evaluate-predict-cnn"><p>Content from <a href="05-evaluate-predict-cnn.html">Evaluate a Convolutional Neural Network and Make Predictions (Classifications)</a></p>
<hr>
<p>Last updated on 2024-02-26 |
        
        <a href="https://https://github.com/erinmgraham/icwithcnn/edit/main/episodes/05-evaluate-predict-cnn.html" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How do you use a model to make a prediction?</li>
<li>How do you measure model prediction accuracy?</li>
<li>What can you do to improve model performance?</li>
<li>What is a hyperparameter?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Use a convolutional neural network (CNN) to make a prediction
(i.e. classify an image).</li>
<li>Explain how to measure the performance of a CNN.</li>
<li>Know what steps to take to improve model accuracy.</li>
<li>Explain hyperparameter tuning.</li>
<li>Be familiar with advantages and disadvantages of different
optimizers.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="step-7--perform-a-predictionclassification">Step 7. Perform a Prediction/Classification<a class="anchor" aria-label="anchor" href="#step-7--perform-a-predictionclassification"></a>
</h3>
<p>After you fully train the network to a satisfactory performance on
the training and validation sets, we use it to perform predictions on a
special hold-out set, the <strong>test</strong> set. The prediction
accuracy of the model on new images will be used in <strong>Step 8.
Measuring performance</strong> to measure the performance of the
network.</p>
<div class="section level4">
<h4 id="prepare-test-dataset">Prepare test dataset<a class="anchor" aria-label="anchor" href="#prepare-test-dataset"></a>
</h4>
<p>Recall in <a href="02-image-data.html">Episode 02 Introduction to
Image Data</a> we discussed how to split your data into training and
test datasets and why. In most cases, that means you already have a test
set on hand. For example, we are using
<code>keras.models.load_model</code> to create a training and test
set.</p>
<p>When creating and using a test set there are a few things to
check:</p>
<ul>
<li>It only contains images the model has never seen before.</li>
<li>It is sufficiently large to provide a meaningful evaluation of model
performance.</li>
<li>It should include images from every target label plus images of
classes not in your target set.</li>
<li>It is processed in the same way as your training set.</li>
</ul>
<p>Check to make sure you have a model in memory and a test dataset:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># check correct model is loaded</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>model_best <span class="op">=</span> keras.models.load_model(<span class="st">'fit_outputs/model_dropout.h5'</span>) <span class="co"># pick your best model</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'We are using'</span>, model_best.name)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># load the CIFAR-10 dataset included with the keras library</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>(train_images, train_labels), (test_images, test_labels) <span class="op">=</span> keras.datasets.cifar10.load_data()</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># normalize the RGB values to be between 0 and 1</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>train_images <span class="op">=</span> train_images <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>test_images <span class="op">=</span> test_images <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># we do not one hot encode here because our model predicts a class label</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># check correct model is loaded</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'We are using'</span>, model_best.name)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># check test image dataset is loaded - images and labels</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The number and shape of images in our test dataset is:'</span>, test_images.shape)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The number of labels in our test dataset is:'</span>, <span class="bu">len</span>(test_labels))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>We are using cifar_model_dropout
The number and shape of images in our test dataset is:  (10000, 32, 32, 3)
The number of labels in our test dataset is:  10000</code></pre>
</div>
<div id="challenge-how-big-should-our-test-data-set-be" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-how-big-should-our-test-data-set-be" class="callout-inner">
<h3 class="callout-title">CHALLENGE How big should our test data set
be?<a class="anchor" aria-label="anchor" href="#challenge-how-big-should-our-test-data-set-be"></a>
</h3>
<div class="callout-content">

</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>It depends! Recall in an <a href="02-image-data.html">Episode 02
Introduction to Image Data</a> Callout we talked about the different
ways to partition the data into training, validation and test data sets.
For example, using the <strong>Stratified Sampling</strong> technique,
we might split the data using these rations: 80-10-10 or 70-15-15.</p>
<p>The test set should be sufficiently large to provide a meaningful
evaluation of your model’s performance. Smaller datasets might not
provide a reliable estimate of how well your model generalizes.</p>
</div>
</div>
</div>
</div>
</div>
<div class="section level4">
<h4 id="predict">Predict<a class="anchor" aria-label="anchor" href="#predict"></a>
</h4>
<p>Armed with a test dataset, we will use our CNN to predict their class
labels using the <code>predict</code> function.</p>
<p>Recall our model will return a vector of probabilities, one for each
class. By finding the class with the highest probability, we can select
the most likely class name of the object.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># use our current best model to predict probability of each class on new test set</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> model_best.predict(test_images)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># create a list of classnames</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>class_names <span class="op">=</span> [<span class="st">'airplane'</span>, <span class="st">'automobile'</span>, <span class="st">'bird'</span>, <span class="st">'cat'</span>, <span class="st">'deer'</span>, <span class="st">'dog'</span>, <span class="st">'frog'</span>, <span class="st">'horse'</span>, <span class="st">'ship'</span>, <span class="st">'truck'</span>]</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># convert probability predictions to table using class names for column names</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>prediction_df <span class="op">=</span> pd.DataFrame(predictions, columns<span class="op">=</span>class_names)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># inspect </span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prediction_df.head())</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co"># convert predictions to class labels</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>predicted_labels <span class="op">=</span> np.argmax(predictions, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(predicted_labels)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>airplane  automobile      bird  ...     horse      ship     truck
0  0.165748   -0.118394  0.062156  ...  0.215477  0.013811 -0.047446
1  0.213530   -0.126139  0.052813  ...  0.264517  0.009097 -0.091710
2  0.211900   -0.099055  0.047890  ...  0.242345 -0.014492 -0.073153
3  0.187883   -0.085144  0.044609  ...  0.217864  0.007502 -0.055209
4  0.190110   -0.118892  0.054869  ...  0.252434 -0.030064 -0.061485

[5 rows x 10 columns]

Out  [5]: array([3, 8, 8, ..., 5, 1, 7], dtype=int64)</code></pre>
</div>
</div>
</div>
<div class="section level3">
<h3 id="step-8--measuring-performance">Step 8. Measuring performance<a class="anchor" aria-label="anchor" href="#step-8--measuring-performance"></a>
</h3>
<p>Once we trained the network we want to measure its performance. There
are many different methods available for measuring performance and which
one to use depends on the type of task we are attempting. These metrics
are often published as an indication of how well our network
performs.</p>
<p>An easy way to check the accuracy of our model on the test set is to
use the <code>accuracy_score()</code> from
<code>sklearn.metrics</code>:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate the model on the test data set</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>test_acc <span class="op">=</span> accuracy_score(test_labels, predicted_labels)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Accuracy:'</span>, <span class="bu">round</span>(test_acc,<span class="dv">2</span>))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="va">Accuracy</span><span class="op">:</span> <span class="fl">0.67</span></span></code></pre>
</div>
<p>To understand a bit more about how this accuracy is obtained, we
create a confusion matrix.</p>
<div class="section level4">
<h4 id="confusion-matrix">Confusion matrix<a class="anchor" aria-label="anchor" href="#confusion-matrix"></a>
</h4>
<p>In the case of multiclass classifications, each cell value
(C<sub>i,j</sub>) is equal to the number of observations known to be in
group <em>i</em> and predicted to be in group <em>j</em>. The diagonal
cells in the matrix are where the true class and predicted class
match.</p>
<figure><img src="fig/05_confusion_matrix_explained.png" alt="for ten classes an example confusion matrix has 10 rows and 10 columns where the value in each cell is the number of observations predicted in that class and known to be in that class. The diagonal cells are where the true and predicted classes match." class="figure mx-auto d-block"></figure><p>To create a confusion matrix, we use another convenient function from
sklearn called <code>confusion_matrix</code>. This function takes as a
first parameter the true labels of the test set. The second parameter is
the predicted labels from our model.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># create a confusion matrix</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> confusion_matrix(test_labels_values, predicted_labels)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(conf_matrix)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[[682  36  67  13  15   7  27  11 108  34]
 [ 12 837   3   2   6   3  32   0  29  76]
 [ 56   4 462  37 137  72 184  26  13   9]
 [ 10  13  48 341  87 217 221  27  17  19]
 [ 23   4  38  34 631  23 168  63  13   3]
 [  8   9  59 127  74 550 103  51  10   9]
 [  3   3  18  23  18  12 919   2   1   1]
 [ 14   8  24  28  98  75  34 693   2  24]
 [ 56  39  11  17   4   6  22   2 813  30]
 [ 23 118   6  12   6   5  33  15  35 747]]</code></pre>
</div>
<p>Unfortunately, this matrix is kinda hard to read. It’s not clear
which column and which row corresponds to which class. So let’s convert
it to a pandas dataframe with its index and columns set to the class
labels as follows:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to a pandas dataframe</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>confusion_df <span class="op">=</span> pd.DataFrame(conf_matrix, index<span class="op">=</span>class_names, columns<span class="op">=</span>class_names)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the names of the x and y axis, this helps with the readability of the heatmap.</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>confusion_df.index.name <span class="op">=</span> <span class="st">'True Label'</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>confusion_df.columns.name <span class="op">=</span> <span class="st">'Predicted Label'</span></span></code></pre>
</div>
<p>We can then use the <code>heatmap</code> function from seaborn to
create a nice visualization of the confusion matrix.</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>sns.heatmap(confusion_df, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">'3g'</span>)</span></code></pre>
</div>
<ul>
<li>The <code>annot=True</code> parameter here will put the numbers from
the confusion matrix in the heatmap.</li>
<li>The <code>fmt=3g</code> will display the values with three
significant digits.</li>
</ul>
<figure><img src="fig/05_pred_v_true_confusion_matrix.png" alt="Confusion matrix of model predictions where the colour scale goes from black to light to represent values from 0 to the total number of test observations in our test set of 1000. The diagonal has much lighter colours, indicating our model is predicting well, but a few non-diagonal cells also have a lighter colour to indicate where the model is making a large number of prediction errors." class="figure mx-auto d-block"></figure><div id="challenge-confusion-matrix" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-confusion-matrix" class="callout-inner">
<h3 class="callout-title">CHALLENGE Confusion Matrix<a class="anchor" aria-label="anchor" href="#challenge-confusion-matrix"></a>
</h3>
<div class="callout-content">
<p>Measure the performance of the neural network you trained and
visualized as a confusion matrix.</p>
<p>Q1. Did the neural network perform well on the test set?</p>
<p>Q2. Did you expect this from the training loss plot?</p>
<p>Q3. What could we do to improve the performance?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>Q1. The confusion matrix illustrates that the predictions are not bad
but can be improved.</p>
<p>Q2. I expected the performance to be better than average because the
accuracy of the model I chose was 67 per cent on the validation set.</p>
<p>Q3. We can try many things to improve the performance from here. One
of the first things we can try is to change the network architecture.
However, in the interest of time, and given we already learned how to
build a CNN, we will now change the training parameters.</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="step-9--tune-hyperparameters">Step 9. Tune hyperparameters<a class="anchor" aria-label="anchor" href="#step-9--tune-hyperparameters"></a>
</h3>
<p>Recall the following from <a href="01-introduction.html">Episode 01
Introduction to Deep Learning</a>:</p>
<div class="section level4">
<h4 id="what-are-hyperparameters">What are hyperparameters?<a class="anchor" aria-label="anchor" href="#what-are-hyperparameters"></a>
</h4>
<p>Hyperparameters are the parameters set by the person configuring the
model instead of those learned by the algorithm itself. Like the dials
on a radio which are <em>tuned</em> to the best frequency,
hyperparameters can be <em>tuned</em> to the best combination for a
given model and context.</p>
<p>These hyperparameters can include the learning rate, the number of
layers in the network, the number of neurons per layer, and many more.
The tuning process is systematic searching for the best combination of
hyperparameters to optimize the model’s performance.</p>
<p>In some cases, it might be necessary to adjust these and re-run the
training many times before we are happy with the result.</p>
<p>Table 1. List of some of the hyperparameters to tune and when.</p>
<table class="table">
<thead><tr class="header">
<th>During Build</th>
<th>When Compiling</th>
<th>During Training</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>number of neurons</td>
<td>loss function</td>
<td>epoch</td>
</tr>
<tr class="even">
<td>activation function</td>
<td>optimizer</td>
<td>batch size</td>
</tr>
<tr class="odd">
<td>dropout rate</td>
<td>learning rate</td>
<td></td>
</tr>
</tbody>
</table>
<p>There are a number of techniques used to tune hyperparameters. Let us
explore a few of them.</p>
<p>One common method for hyperparameter tuning is by using a
<code>for</code> loop to change a particular parameter.</p>
<div id="challenge-tune-dropout-rate-using-a-for-loop" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-tune-dropout-rate-using-a-for-loop" class="callout-inner">
<h3 class="callout-title">CHALLENGE Tune Dropout Rate using a For
Loop<a class="anchor" aria-label="anchor" href="#challenge-tune-dropout-rate-using-a-for-loop"></a>
</h3>
<div class="callout-content">
<p>Q1. What do you think would happen if you lower the dropout rate?
Write some code to vary the dropout rate and investigate how it affects
the model training.</p>
<p>Q2. You are varying the dropout rate and checking its effect on the
model performance, what is the term associated to this procedure?</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<p>Q1. Varying the dropout rate</p>
<p>The code below instantiates and trains a model with varying dropout
rates. The resulting plot indicates the ideal dropout rate in this case
is around 0.45. This is where the validation loss is lowest.</p>
<ul>
<li>NB1: It takes a while to train these five networks.</li>
<li>NB2: You should do this with a test set and not with the validation
set!</li>
</ul>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># one-hot encode labels</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>train_labels <span class="op">=</span> keras.utils.to_categorical(train_labels, <span class="bu">len</span>(class_names))</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># split the training data into training and validation sets</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>train_images, val_images, train_labels, val_labels <span class="op">=</span> train_test_split(train_images, train_labels, </span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>																	  test_size <span class="op">=</span> <span class="fl">0.2</span>, </span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>																	  random_state <span class="op">=</span> <span class="dv">42</span>)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co"># specify range of dropout rates</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>dropout_rates <span class="op">=</span> [<span class="fl">0.15</span>, <span class="fl">0.3</span>, <span class="fl">0.45</span>, <span class="fl">0.6</span>, <span class="fl">0.75</span>]</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co"># create empty list to hold losses</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>val_losses_vary <span class="op">=</span> [] </span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> dropout_rate <span class="kw">in</span> dropout_rates:</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Input layer of 32x32 images with three channels (RGB)</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    inputs_vary <span class="op">=</span> keras.Input(shape<span class="op">=</span>train_images.shape[<span class="dv">1</span>:])</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># CNN Part 2</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convolutional layer with 16 filters, 3x3 kernel size, and ReLU activation</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>    x_vary <span class="op">=</span> keras.layers.Conv2D(<span class="dv">16</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(inputs_vary)</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Pooling layer with input window sized 2,2</span></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>    x_vary <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x_vary)</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Second Convolutional layer with 32 filters, 3x3 kernel size, and ReLU activation</span></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    x_vary <span class="op">=</span> keras.layers.Conv2D(<span class="dv">32</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x_vary)</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Second Pooling layer with input window sized 2,2</span></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>    x_vary <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x_vary)</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Second Convolutional layer with 64 filters, 3x3 kernel size, and ReLU activation</span></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>    x_vary <span class="op">=</span> keras.layers.Conv2D(<span class="dv">64</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x_vary)</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Dropout layer randomly drops x per cent of the input units</span></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>    x_vary <span class="op">=</span> keras.layers.Dropout(dropout_rate)(x_vary) <span class="co"># This is new!</span></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Flatten layer to convert 2D feature maps into a 1D vector</span></span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>    x_vary <span class="op">=</span> keras.layers.Flatten()(x_vary)</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Dense layer with 128 neurons and ReLU activation</span></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>    x_vary <span class="op">=</span> keras.layers.Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x_vary)</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># CNN Part 3</span></span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Output layer with 10 units (one for each class) and softmax activation</span></span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>    outputs_vary <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">'softmax'</span>)(x_vary)</span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>    model_vary <span class="op">=</span> keras.Model(inputs <span class="op">=</span> inputs_vary, outputs <span class="op">=</span> outputs_vary, </span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>                             name <span class="op">=</span><span class="st">"cifar_model_vary_dropout"</span>)</span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a>    model_vary.<span class="bu">compile</span>(optimizer <span class="op">=</span> <span class="st">'adam'</span>,</span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a>                       loss <span class="op">=</span> keras.losses.CategoricalCrossentropy(),</span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a>                       metrics <span class="op">=</span> [<span class="st">'accuracy'</span>])</span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a>    model_vary.fit(train_images, train_labels, </span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a>                   epochs <span class="op">=</span> <span class="dv">20</span>,</span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a>                   validation_data <span class="op">=</span> (val_images, val_labels),</span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a>                   batch_size <span class="op">=</span> <span class="dv">32</span>)</span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a>    val_loss_vary, val_acc_vary <span class="op">=</span> model_vary.evaluate(val_images, val_labels)</span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a>    val_losses_vary.append(val_loss_vary)</span>
<span id="cb11-56"><a href="#cb11-56" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-57"><a href="#cb11-57" aria-hidden="true" tabindex="-1"></a>loss_df <span class="op">=</span> pd.DataFrame({<span class="st">'dropout_rate'</span>: dropout_rates, <span class="st">'val_loss_vary'</span>: val_losses_vary})</span>
<span id="cb11-58"><a href="#cb11-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-59"><a href="#cb11-59" aria-hidden="true" tabindex="-1"></a>sns.lineplot(data<span class="op">=</span>loss_df, x<span class="op">=</span><span class="st">'dropout_rate'</span>, y<span class="op">=</span><span class="st">'val_loss_vary'</span>)</span></code></pre>
</div>
<figure><img src="fig/05_vary_dropout_rate.png" alt="test loss plotted against five dropout rates ranging from 0.15 to 0.75 where the minimum test loss appears to occur between 0.4 and 0.5" class="figure mx-auto d-block"></figure><p>Q2. Term associated to this procedure</p>
<p>This is called hyperparameter tuning.</p>
</div>
</div>
</div>
</div>
<p>Another common method for hyperparameter tuning is <strong>grid
search</strong>.</p>
</div>
<div class="section level4">
<h4 id="what-is-grid-search">What is Grid Search?<a class="anchor" aria-label="anchor" href="#what-is-grid-search"></a>
</h4>
<p>Grid Search or <code>GridSearchCV</code> (as per the library function
call) is foundation method for hyperparameter tuning. The aim of
hyperparameter tuning is to define a grid of possible values for each
hyperparameter you want to tune. GridSearch will then evaluate the model
performance for each combination of hyperparameters in a brute-force
manner, iterating through every possible combination in the grid.</p>
<p>For instance, suppose you’re tuning two hyperparameters:</p>
<ul>
<li><p>Learning rate: with possible values [0.01, 0.1, 1]</p></li>
<li><p>Batch size: with possible values [10, 50, 100]</p></li>
<li><p>GridSearch will evaluate the model for all 3*3 = 9 combinations
(e.g., {0.01, 10}, {0.01, 50}, {0.1, 10}, and so on).</p></li>
</ul>
<div id="challenge-tune-optimizer-using-grid-search" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-tune-optimizer-using-grid-search" class="callout-inner">
<h3 class="callout-title">CHALLENGE Tune Optimizer using Grid
Search<a class="anchor" aria-label="anchor" href="#challenge-tune-optimizer-using-grid-search"></a>
</h3>
<div class="callout-content">
<p>In <a href="04-fit-cnn.html">Episode 04 Compile and Train a
Convolutional Neural Network</a> we talked briefly about the
<code>Adam</code> optimizer used in our <code>model.compile</code>
discussion. Recall the optimizer refers to the algorithm with which the
model learns to optimize on the provided loss function.</p>
<p>Here we will use our introductory model to demonstrate how GridSearch
is expressed in code to search for an optimizer.</p>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4">Show me the solution</h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" data-bs-parent="#accordionSolution4" aria-labelledby="headingSolution4">
<div class="accordion-body">
<p>First, we will define a <strong>build function</strong> to use during
GridSearch. This function will compile the model for each combination of
parameters prior to evaluation.</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_model():</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Input layer of 32x32 images with three channels (RGB)</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>train_images.shape[<span class="dv">1</span>:])</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convolutional layer with 50 filters, 3x3 kernel size, and ReLU activation</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(inputs)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Second Convolutional layer</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Flatten layer to convert 2D feature maps into a 1D vector</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Flatten()(x)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Output layer with 10 units (one for each class)</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>)(x)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># create the model</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    mode <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>outputs)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># compile the model</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(optimizer <span class="op">=</span> <span class="st">'adam'</span>, </span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>                  loss <span class="op">=</span> keras.losses.CategoricalCrossentropy(), </span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>                  metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span></code></pre>
</div>
<p>Secondly, we can define our GridSearch parameters and assign fit
results to a variable for output.</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scikeras.wrappers <span class="im">import</span> KerasClassifier</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Wrap the model</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> KerasClassifier(build_fn<span class="op">=</span>create_model, epochs<span class="op">=</span><span class="dv">2</span>, batch_size<span class="op">=</span><span class="dv">32</span>, verbose<span class="op">=</span><span class="dv">0</span>)  <span class="co"># epochs, batch_size, verbose can be adjusted as required. Using low epochs to save computation time and demonstration purposes only</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the grid search parameters</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> [<span class="st">'SGD'</span>, <span class="st">'RMSprop'</span>, <span class="st">'Adam'</span>]</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> <span class="bu">dict</span>(optimizer<span class="op">=</span>optimizer)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> GridSearchCV(estimator<span class="op">=</span>model, param_grid<span class="op">=</span>param_grid, n_jobs<span class="op">=</span><span class="dv">1</span>, cv<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>grid_result <span class="op">=</span> grid.fit(train_images, train_labels)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Summarize results</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best: </span><span class="sc">%f</span><span class="st"> using </span><span class="sc">%s</span><span class="st">"</span> <span class="op">%</span> (grid_result.best_score_, grid_result.best_params_))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Best: 0.586660 using {'optimizer': 'RMSprop'}</code></pre>
</div>
<p>Thus, we can interpret from this output that our best tested
optimiser is the <strong>root mean square propagation</strong>
optimiser, or RMSprop.</p>
<p>Curious about RMSprop? <a href="https://keras.io/api/optimizers/rmsprop/" class="external-link">RMSprop in Keras</a> and
<a href="https://optimization.cbe.cornell.edu/index.php?title=RMSProp" class="external-link">RMSProp,
Cornell University</a></p>
</div>
</div>
</div>
</div>
<p>A third way to tune hyperparameters is brute force.</p>
<div id="challenge-tune-activation-function-using-brute-force" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-tune-activation-function-using-brute-force" class="callout-inner">
<h3 class="callout-title">CHALLENGE Tune Activation Function using Brute
Force<a class="anchor" aria-label="anchor" href="#challenge-tune-activation-function-using-brute-force"></a>
</h3>
<div class="callout-content">
<p>In <a href="03-build-cnn.html">Episode 03 Build a Convolutional
Neural Network</a> we talked briefly about the <code>relu</code>
activation function passed as an argument to our <code>Conv2D</code>
hidden layers.</p>
<p>An activation function is like a switch, or a filter, that we use in
artificial neural networks, inspired by how our brains work. These
functions play a crucial role in determining whether a neuron (a small
unit in the neural network) should “fire” or become active.</p>
<p>Think of an activation function as a tiny decision-maker for each
neuron in a neural network. It helps determine whether the neuron should
‘fire’, or pass on information, or stay ‘off’ and remain silent, much
like a light switch controls whether the light should be ON or OFF.
Activation functions are crucial because they add non-linearity to the
neural network. Without them, the network would be like a simple linear
model, unable to learn complex patterns in data.</p>
<div class="section level3">
<h3 id="how-do-you-know-what-activation-function-to-choose">How do you know what activation function to choose?<a class="anchor" aria-label="anchor" href="#how-do-you-know-what-activation-function-to-choose"></a>
</h3>
<p>Neural networks can be tuned to leverage many different types of
activation functions. In fact, it is a crucial decision as the choice of
activation function will have a direct impact on the performance of the
model.</p>
<p>Table 2. Description of each activation function, its benefits, and
drawbacks.</p>
<table style="width:100%;" class="table">
<colgroup>
<col width="16%">
<col width="50%">
<col width="33%">
</colgroup>
<thead><tr class="header">
<th>Activation Function</th>
<th>Positives</th>
<th>Negatives</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>ReLU</td>
<td>- Addresses vanishing gradient problem <br> - Computationally
efficient</td>
<td>- Can cause “dying neurons” <br> - Not zero-centered</td>
</tr>
<tr class="even">
<td>Leaky ReLU</td>
<td>- Addresses the “dying ReLU” problem <br> - Computationally
efficient</td>
<td>- Empirical results can be inconsistent <br> - Not
zero-centered</td>
</tr>
<tr class="odd">
<td>Sigmoid</td>
<td>- Outputs between 0 and 1 <br> - Smooth gradient</td>
<td>- Can cause vanishing gradient problem <br> - Computationally more
expensive</td>
</tr>
<tr class="even">
<td>Tanh</td>
<td>- Outputs between -1 and 1 <br> - Zero-centered</td>
<td>- Can still suffer from vanishing gradients to some extent</td>
</tr>
<tr class="odd">
<td>Softmax</td>
<td>- Used for multi-class classification <br> - Outputs a probability
distribution</td>
<td>- Used only in the output layer for classification tasks</td>
</tr>
<tr class="even">
<td>SELU</td>
<td>- Self-normalizing properties <br> - Can outperform ReLU in deeper
networks</td>
<td>- Requires specific weight initialization <br> - May not perform
well outside of deep architectures</td>
</tr>
</tbody>
</table>
<p>Write some code to assessing activation function performance.</p>
</div>
</div>
</div>
</div>
<div id="accordionSolution5" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution5" aria-expanded="false" aria-controls="collapseSolution5">
  <h4 class="accordion-header" id="headingSolution5">Show me the solution</h4>
</button>
<div id="collapseSolution5" class="accordion-collapse collapse" data-bs-parent="#accordionSolution5" aria-labelledby="headingSolution5">
<div class="accordion-body">
<p>The code below serves as a practical means for exploring activation
performance on an image dataset.</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function to create a model with a given activation function</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_model(activation_function):</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Input layer of 32x32 images with three channels (RGB)</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>train_images.shape[<span class="dv">1</span>:])</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convolutional layer with 50 filters, 3x3 kernel size, and ReLU activation</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span>activation_function)(inputs)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Second Convolutional layer</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span>activation_function)(x)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Flatten layer to convert 2D feature maps into a 1D vector</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Flatten()(x)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Output layer with 10 units (one for each class)</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>)(x)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># create the model</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>outputs)</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># create the model</span></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(optimizer <span class="op">=</span> <span class="st">'adam'</span>, </span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>                  loss <span class="op">=</span> keras.losses.CategoricalCrossentropy(),</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>                  metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a><span class="co"># List of activation functions to try</span></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>activations <span class="op">=</span> [<span class="st">'relu'</span>, <span class="st">'sigmoid'</span>, <span class="st">'tanh'</span>, <span class="st">'selu'</span>, keras.layers.LeakyReLU()]</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>history_data <span class="op">=</span> {}</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Train a model with each activation function and store the history</span></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> activation <span class="kw">in</span> activations:</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> create_model(activation)</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>    history <span class="op">=</span> model.fit(train_images, train_labels, </span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>                        epochs<span class="op">=</span><span class="dv">10</span>, </span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>                        validation_data<span class="op">=</span>(val_images, val_labels),</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>                        batch_size <span class="op">=</span> <span class="dv">32</span>)</span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>    history_data[<span class="bu">str</span>(activation)] <span class="op">=</span> history</span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the validation accuracy for each activation function</span></span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> activation, history <span class="kw">in</span> history_data.items():</span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a>    plt.plot(history.history[<span class="st">'val_accuracy'</span>], label<span class="op">=</span>activation)</span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Validation accuracy for different activation functions'</span>)</span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epochs'</span>)</span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Validation Accuracy'</span>)</span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/05_tune_activation_results.png" alt="Validation accuracy plotted against ten epochs for five different activations functions. relu and Leaky relu have the highest accuracy atound 0.60; sigmoid and selu are next with accuracy around 0.45 and tanh has the lowest accuracy of 0.35" class="figure mx-auto d-block"></figure><p>In this figure, after 10 epochs, the <code>ReLU</code> and
<code>Leaky ReLU</code> activation functions appear to converge around
0.60 per cent validation accuracy. We recommend when tuning your model
to ensure you use enough epochs to be confident in your results.</p>
</div>
</div>
</div>
</div>
<div id="challenge-what-could-be-next-steps-to-further-improve-the-model" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-what-could-be-next-steps-to-further-improve-the-model" class="callout-inner">
<h3 class="callout-title">CHALLENGE What could be next steps to further
improve the model?<a class="anchor" aria-label="anchor" href="#challenge-what-could-be-next-steps-to-further-improve-the-model"></a>
</h3>
<div class="callout-content">
<p>With unlimited options to modify the model architecture or to play
with the training parameters, deep learning can trigger very extensive
hunting for better and better results. Usually models are “well
behaving” in the sense that small chances to the architectures also only
result in small changes of the performance (if any). It is often
tempting to hunt for some magical settings that will lead to much better
results. But do those settings exist? Applying common sense is often a
good first step to make a guess of how much better could results be.</p>
<ul>
<li>What changes to the model architecture might make sense to
explore?</li>
<li>Ignoring changes to the model architecture, what might notably
improve the prediction quality?</li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution6" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution6" aria-expanded="false" aria-controls="collapseSolution6">
  <h4 class="accordion-header" id="headingSolution6">Show me the solution</h4>
</button>
<div id="collapseSolution6" class="accordion-collapse collapse" data-bs-parent="#accordionSolution6" aria-labelledby="headingSolution6">
<div class="accordion-body">
<p>This is an open question.</p>
<p>Regarding the model architecture:</p>
<ul>
<li>In the present case we do not see a magical silver bullet to
suddenly boost the performance. But it might be worth testing if deeper
networks do better (more layers).</li>
</ul>
<p>Other changes that might impact the quality notably:</p>
<ul>
<li>The most obvious answer here would be: more data! Even this will not
always work (e.g., if data is very noisy and uncorrelated, more data
might not add much).</li>
<li>Related to more data: use data augmentation. By creating realistic
variations of the available data, the model might improve as well.</li>
<li>More data can mean more data points and also more features!</li>
</ul>
</div>
</div>
</div>
</div>
<p>By now you should have a well-trained, finely-tuned model that makes
accurate predictions and are ready to share the model with others.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Use model.predict to make a prediction with your model.</li>
<li>Model accuracy must be measured on a test dataset with images your
model has not seen before.</li>
<li>There are many hyperparameters to choose from to improve model
performance.</li>
<li>Fitting separate models with different hyperparameters and comparing
their performance is a common and good practice in deep learning.</li>
</ul>
</div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</div>
</div></section><section id="aio-06-conclusion"><p>Content from <a href="06-conclusion.html">Conclusion</a></p>
<hr>
<p>Last updated on 2024-02-13 |
        
        <a href="https://https://github.com/erinmgraham/icwithcnn/edit/main/episodes/06-conclusion.html" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How do I share my convolutional neural network (CNN)?</li>
<li>Where can I find pre-trained models?</li>
<li>What is a GPU?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Learn how to save and load models.</li>
<li>Know where to search for pretrained models.</li>
<li>Understand what a GPU is and what it can do for you.</li>
<li>Explain when to use a CNN and when not to.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="step-10--share-model">Step 10. Share model<a class="anchor" aria-label="anchor" href="#step-10--share-model"></a>
</h3>
<p>We now have a trained network that performs at a level we are happy
with and maintains high prediction accuracy on a test dataset. We should
consider publishing a file with both the architecture of our network and
the weights which it has learned (assuming we did not use a pre-trained
network). This will allow others to use it as as pre-trained network for
their own purposes and for them to (mostly) reproduce our result.</p>
<p>Use <code>model.save</code> to save a model:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># save best model</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>model.save(<span class="st">'model_best.keras'</span>)</span></code></pre>
</div>
<p>The <code>save</code> method is actually an alias for
<code>tf.keras.saving.save_model()</code> where the default
<code>save_format=NONE</code>.</p>
<p>This saved model can be loaded again by using the
<code>load_model</code> method:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load a saved model</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>pretrained_model <span class="op">=</span> keras.models.load_model(<span class="st">'model_best.keras'</span>)</span></code></pre>
</div>
<p>This loaded model can be used as before to predict.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># use the pretrained model to predict the class name of the first test image</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>result_pretrained <span class="op">=</span> model_intro.predict(test_images[<span class="dv">0</span>].reshape(<span class="dv">1</span>,<span class="dv">32</span>,<span class="dv">32</span>,<span class="dv">3</span>))</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The predicted probability of each class is: '</span>, result_pretrained.<span class="bu">round</span>(<span class="dv">4</span>))</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The class with the highest predicted probability is: '</span>, class_names[result_pretrained.argmax()])</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="va">cat</span></span></code></pre>
</div>
<p>The saved .keras file contains:</p>
<ul>
<li>The model’s configuration (architecture).</li>
<li>The model’s weights.</li>
<li>The model’s optimizer’s state (if any).</li>
</ul>
<p>Note that saving the model does not save the training history
(i.e. training and validation loss and accuracy). For that, you save the
model history dataframe we used for plotting.</p>
<p>The Keras documentation for <a href="https://keras.io/api/saving/" class="external-link">Saving and Serialization</a>
explains other ways to save your model.</p>
<p>To share your model with a wider audience it is recommended you
create git repository, such as <a href="https://github.com/" class="external-link">GitHub</a>,
and upload your code, images, and model outputs to the cloud. In some
cases, you may be able to offer up your model to an online repository of
pretrained models.</p>
<div class="section level4">
<h4 id="choosing-a-pretrained-model">Choosing a pretrained model<a class="anchor" aria-label="anchor" href="#choosing-a-pretrained-model"></a>
</h4>
<p>If your data and problem is very similar to what others have done, a
pre-trained network might be preferable. Even if your problem is
different, if the data type is common (for example images), you can use
a pre-trained network and fine-tune it for your problem. A large number
of openly available pre-trained networks can be found in the <a href="https://modelzoo.co/" class="external-link">Model Zoo</a>, <a href="https://pytorch.org/hub/" class="external-link">pytorch hub</a> or <a href="https://pytorch.org/hub/" class="external-link">tensorflow hub</a>.</p>
</div>
</div>
<div class="section level3">
<h3 id="what-else-do-i-need-to-know">What else do I need to know?<a class="anchor" aria-label="anchor" href="#what-else-do-i-need-to-know"></a>
</h3>
<div class="section level4">
<h4 id="how-to-choose-a-deep-learning-library">How to choose a Deep Learning Library<a class="anchor" aria-label="anchor" href="#how-to-choose-a-deep-learning-library"></a>
</h4>
<p>In this lesson we chose to use <a href="https://keras.io/" class="external-link">Keras</a>
because it was designed to be easy to use and usually requires fewer
lines of code than other libraries. Keras can actually work on top of
TensorFlow (and several other libraries), hiding away the complexities
of TensorFlow while still allowing you to make use of their
features.</p>
<p>The performance of Keras is sometimes not as good as other libraries
and if you are going to move on to create very large networks using very
large datasets then you might want to consider one of the other
libraries. But for many applications the performance difference will not
be enough to worry about and the time you will save with simpler code
will exceed what you will save by having the code run a little
faster.</p>
<p>Keras also benefits from a very good set of <a href="https://keras.io/guides/" class="external-link">online documentation</a> and a large
user community. You will find most of the concepts from Keras translate
very well across to the other libraries if you wish to learn them at a
later date.</p>
<p>A couple of those libraries include:</p>
<ul>
<li><p><a href="https://www.tensorflow.org/" class="external-link">TensorFlow</a> was
developed by Google and is one of the older Deep Learning libraries,
ported across many languages since it was first released to the public
in 2015. It is very versatile and capable of much more than Deep
Learning but as a result it often takes a lot more lines of code to
write Deep Learning operations in TensorFlow than in other libraries. It
offers (almost) seamless integration with GPU accelerators and Google’s
own TPU (Tensor Processing Unit) chips specially built for machine
learning.</p></li>
<li><p><a href="https://pytorch.org/" class="external-link">PyTorch</a> was developed by
Facebook in 2016 and is a popular choice for Deep Learning applications.
It was developed for Python from the start and feels a lot more
“pythonic” than TensorFlow. Like TensorFlow it was designed to do more
than just Deep Learning and offers some very low level interfaces. <a href="https://www.pytorchlightning.ai/" class="external-link">PyTorch Lightning</a> offers a
higher level interface to PyTorch to set up experiments. Like TensorFlow
it is also very easy to integrate PyTorch with a GPU. In many benchmarks
it outperforms the other libraries.</p></li>
<li><p>NEW <a href="https://keras.io/keras_core/announcement/?utm_source=ADSA&amp;utm_campaign=60c8d8b6cb-EMAIL_CAMPAIGN_2022_10_04_06_04_COPY_01&amp;utm_medium=email&amp;utm_term=0_5401c7226a-60c8d8b6cb-461545621" class="external-link">Keras
Core</a> In Fall 2023, this library will become Keras 3.0. Keras Core is
a full rewrite of the Keras codebase that rebases it on top of a modular
backend architecture. It makes it possible to run Keras workflows on top
of arbitrary frameworks — starting with TensorFlow, JAX, and
PyTorch.</p></li>
</ul>
</div>
<div class="section level4">
<h4 id="what-is-a-gpu-and-do-i-need-one">What is a GPU and do I need one?<a class="anchor" aria-label="anchor" href="#what-is-a-gpu-and-do-i-need-one"></a>
</h4>
<p>A <strong>GPU</strong>, or <strong>Graphics Processing Unit</strong>,
is a specialized electronic circuit designed to accelerate graphics
rendering and image processing in a computer. In the context of deep
learning and machine learning, GPUs have become essential due to their
ability to perform parallel computations at a much faster rate compared
to traditional central processing units (CPUs). This makes them
well-suited for the intensive matrix and vector operations that are
common in deep learning algorithms.</p>
<p>As you have experienced in this lesson, training CNN models can take
a long time. If you follow the steps presented here you will find you
are training multiple models to find the one best suited to your needs,
particularly when fine tuning hyperparameters. However you have also
seen that running on CPU only machines can be done! So while a GPU is
not an absolute requirement for deep learning, it can significantly
accelerate your deep learning work and make it more efficient,
especially for larger and more complex tasks.</p>
<p>If you don’t have access to a powerful GPU locally, there are cloud
services that provide GPU instances for deep learning. This may be the
most cost-effective option for many users.</p>
</div>
<div class="section level4">
<h4 id="it-this-the-bestonly-way-to-code-up-cnns-for-image-classification">It this the best/only way to code up CNNs for image
classification?<a class="anchor" aria-label="anchor" href="#it-this-the-bestonly-way-to-code-up-cnns-for-image-classification"></a>
</h4>
<p>Absolutely not! The code we used in today’s workshop might be
considered old fashioned. A lot of the data preprocessing we did by hand
can now be done by adding different layer types to your model. The <a href="https://keras.io/guides/preprocessing_layers/" class="external-link">preprocessing
layers</a> section fo the Keras documentation provides several
examples.</p>
<p>The point is that this technology, both hardware and software, is
dynamic and changing at exponentially increasing rates. It is essential
to stay curious and open to learning and follow up with continuous
education and practice. Other strategies to stay informed include:</p>
<ul>
<li>Online communications and forums, such as the Reddit’s <a href="https://www.reddit.com/r/MachineLearning/?rdt=58875" class="external-link">r/MachineLearning</a>
and <a href="https://datascience.stackexchange.com/" class="external-link">Data Science Stack
Exchange</a>
<ul>
<li>watch out for outdated threads!</li>
</ul>
</li>
<li>Academic journals and conferences
<ul>
<li>Unlike other sciences, computer science digital libraries like <a href="https://arxiv.org/" class="external-link">arXiv</a> enable researchers to publish their
preprints in advance and disseminates recent advances more quickly than
traditional methods of publishing</li>
</ul>
</li>
<li>
<a href="https://github.com/" class="external-link">GitHub</a> repositories</li>
<li>Practice
<ul>
<li>like any other language, you must use it or lose it!</li>
</ul>
</li>
</ul>
</div>
<div class="section level4">
<h4 id="what-other-uses-are-there-for-neural-networks">What other uses are there for neural networks?<a class="anchor" aria-label="anchor" href="#what-other-uses-are-there-for-neural-networks"></a>
</h4>
<p>In addition to image classification, <a href="01-introduction.html">Episode 01 Introduction to Deep Learning</a>
introduced other computer vision tasks, including object detection and
instance and semantic segmentation. These can all be done with CNNs and
are readily transferable to videos. Also included in these tasks is
medical imaging for diagnoses of disease and, of course, facial
recognition.</p>
<p>However, there are many other tasks which CNNs are well suited
for:</p>
<ul>
<li>Language tasks
<ul>
<li>Natural Language Processing (NLP) for text classification (sentiment
analysis, spam detection, topic classification)</li>
<li>Speech Recognition for speech to text conversion</li>
</ul>
</li>
<li>Drug Discovery</li>
<li>Time-series analysis (sensor readings, financial data, health
monitoring)</li>
<li>Robotics</li>
<li>Self-driving cars</li>
</ul>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Deep Learning is well suited to classification and prediction
problems such as image recognition.</li>
<li>To use Deep Learning effectively, go through a workflow of: defining
the problem, identifying inputs and outputs, preparing data, choosing
the type of network, choosing a loss function, training the model,
tuning Hyperparameters, measuring performance before we can classify
data.</li>
<li>Keras is a Deep Learning library that is easier to use than many of
the alternatives such as TensorFlow and PyTorch.</li>
<li>Graphical Processing Units are useful, though not essential, for
deep learning tasks.</li>
</ul>
</div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</div>
</div></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>
        
        <a href="https://https://github.com/erinmgraham/icwithcnn/edit/main/README.html" class="external-link">Edit on GitHub</a>
        
	
        | <a href="https://https://github.com/erinmgraham/icwithcnn/blob/main/CONTRIBUTING.html" class="external-link">Contributing</a>
        | <a href="https://https://github.com/erinmgraham/icwithcnn/" class="external-link">Source</a></p>
				<p><a href="https://https://github.com/erinmgraham/icwithcnn/blob/main/CITATION" class="external-link">Cite</a> | <a href="mailto:erin.graham@jcu.edu.au">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">
        
        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>
        
        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.2" class="external-link">sandpaper (0.16.2)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.3" class="external-link">pegboard (0.7.3)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.1" class="external-link">varnish (1.0.1)</a></p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://.github.io/github.com/aio.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://.github.io/github.com/aio.html",
  "identifier": "https://.github.io/github.com/aio.html",
  "dateCreated": "2023-05-03",
  "dateModified": "2024-02-26",
  "datePublished": "2024-02-26"
}

  </script><script>
		feather.replace();
	</script><!-- Matomo
    2022-11-07: we have gotten a notification that we have an overage for our
    tracking and I'm pretty sure this has to do with Workbench usage.
    Considering that I am not _currently_ using this tracking because I do not
    yet know how to access the data, I am turning this off for now.
  <script>
    var _paq = window._paq = window._paq || [];
    /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
    _paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
    _paq.push(["setDomains", ["*.preview.carpentries.org","*.datacarpentry.github.io","*.datacarpentry.org","*.librarycarpentry.github.io","*.librarycarpentry.org","*.swcarpentry.github.io", "*.carpentries.github.io"]]);
    _paq.push(["setDoNotTrack", true]);
    _paq.push(["disableCookies"]);
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
          var u="https://carpentries.matomo.cloud/";
          _paq.push(['setTrackerUrl', u+'matomo.php']);
          _paq.push(['setSiteId', '1']);
          var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
          g.async=true; g.src='https://cdn.matomo.cloud/carpentries.matomo.cloud/matomo.js'; s.parentNode.insertBefore(g,s);
        })();
  </script>
  End Matomo Code -->
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

