<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>Image Classification with Convolutional Neural Networks: Compile and Train (Fit) a Convolutional Neural Network</title><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" type="text/css" href="../assets/styles.css"><script src="../assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png"><link rel="manifest" href="../site.webmanifest"><link rel="mask-icon" href="../safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" content="#ffffff"></head><body>
    <header id="top" class="navbar navbar-expand-md navbar-light bg-white top-nav incubator"><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-6">
      <div class="large-logo">
        <img alt="Carpentries Incubator" src="../assets/images/incubator-logo.svg"><abbr class="badge badge-light" title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught." style="background-color: #FF4955; border-radius: 5px">
          <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#early-development-pre-alpha-through-alpha" class="external-link alert-link" style="color: #000">
            <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
            Pre-Alpha
          </a>
          <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
        </abbr>
        
      </div>
    </div>
    <div class="selector-container">
      
      
      <div class="dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1"><li><button class="dropdown-item" type="button" onclick="window.location.href='../04-fit-cnn.html';">Learner View</button></li>
        </ul></div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl navbar-light bg-white bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="../assets/images/incubator-logo-sm.svg"></div>
    <div class="lesson-title-md">
      Image Classification with Convolutional Neural Networks
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="search button" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item">
          <span class="lesson-title">
            Image Classification with Convolutional Neural Networks
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/instructor-notes.html">Instructor Notes</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/images.html">Extract All Images</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"><hr><li><a class="dropdown-item" href="reference.html">Reference</a></li>
          </ul></li>
      </ul></div>
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled><input class="form-control me-2 searchbox" type="search" placeholder="Search" aria-label="Search"><button class="btn btn-outline-success tablet-search-button" type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="search button"></i>
        </button>
      </fieldset></form>
  </div><!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Image Classification with Convolutional Neural Networks
</div>

<aside class="col-md-12 lesson-progress"><div style="width: 55%" class="percentage">
    55%
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: 55%" aria-valuenow="55" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="../04-fit-cnn.html">Learner View</a>
                      </div>
                    </div>
                  </div><!--/div.accordion-item-->
                </div><!--/div.accordion-flush-->
              </div><!--div.sidenav-view-selector -->
            </div><!--/div.col -->
      
            <hr></div><!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Schedule</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="setup-gpu.html">1. Setup - GPU</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="01-introduction.html">2. Introduction to Deep Learning</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="02-image-data.html">3. Introduction to Image Data</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="03-build-cnn.html">4. Build a Convolutional Neural Network</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlushcurrent">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-headingcurrent">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapsecurrent" aria-expanded="true" aria-controls="flush-collapsecurrent">
        <span class="visually-hidden">Current Chapter</span>
        <span class="current-chapter">
        5. Compile and Train (Fit) a Convolutional Neural Network
        </span>
      </button>
    </div><!--/div.accordion-header-->
        
    <div id="flush-collapsecurrent" class="accordion-collapse collapse show" aria-labelledby="flush-headingcurrent" data-bs-parent="#accordionFlushcurrent">
      <div class="accordion-body">
        <ul><li><a href="#choose-the-best-model-and-use-it-to-predict">Choose the best model and use it to predict</a></li>
        </ul></div><!--/div.accordion-body-->
    </div><!--/div.accordion-collapse-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="05-evaluate-predict-cnn.html">6. Evaluate a Convolutional Neural Network and Make Predictions (Classifications)</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush8">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading8">
        <a href="06-conclusion.html">7. Conclusion</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width"><div class="accordion accordion-flush resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul><li>
                        <a href="../instructor/key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="../instructor/instructor-notes.html">Instructor Notes</a>
                      </li>
                      <li>
                        <a href="../instructor/images.html">Extract All Images</a>
                      </li>
                      <hr><li><a class="dropdown-item" href="reference.html">Reference</a></li>
                    </ul></div>
                </div>
              </div>
            </div>
            <hr class="half-width resources"><a href="../instructor/aio.html">See all in one page</a>
            

            <hr class="d-none d-sm-block d-md-none"><div class="d-grid gap-1">
            
            </div>
          </div><!-- /div.accordion -->
        </div><!-- /div.sidebar-inner -->
      </nav></div><!-- /div.sidebar -->
  </div><!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-instructor.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <nav class="lesson-content mx-md-4" aria-label="Previous and Next Chapter"><!-- content for small screens --><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/03-build-cnn.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/05-evaluate-predict-cnn.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/03-build-cnn.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Build a
        </a>
        <a class="chapter-link float-end" href="../instructor/05-evaluate-predict-cnn.html" rel="next">
          Next: Evaluate a... 
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
      <hr></nav><main id="main-content" class="main-content"><div class="container lesson-content">
        <h1>Compile and Train (Fit) a Convolutional Neural Network</h1>
        <p>Last updated on 2023-12-12 |
        
        <a href="https://https://github.com/erinmgraham/icwithcnn/edit/main/episodes/04-fit-cnn.html" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
        
        
        
        <p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 12 minutes</p>
        
        <div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>

        

<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul><li>How do you compile a convolutional neural network (CNN)?</li>
<li>What is a loss function?</li>
<li>What is an optimizer?</li>
<li>How do you train (fit) a CNN?</li>
<li>How do you evaluate a model during training?</li>
<li>What is overfitting?</li>
</ul></div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul><li>Explain the difference between compiling and training (fitting) a
CNN.</li>
<li>Know how to select a loss function for your model.</li>
<li>Understand what an optimizer is.</li>
<li>Define the terms: learning rate, batch size, epoch.</li>
<li>Understand what loss and accuracy are and how to monitor them during
training.</li>
<li>Explain what overfitting is and what to do about it.</li>
</ul></div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="step-5--choose-a-loss-function-and-optimizer">Step 5. Choose a loss function and optimizer<a class="anchor" aria-label="anchor" href="#step-5--choose-a-loss-function-and-optimizer"></a></h3>
<p>We have designed a convolutional neural network (CNN) that in theory
we should be able to train to classify images.</p>
<p>We now select an appropriate optimizer and loss function to use
during training (fitting).</p>
<p>Recall how we compiled our model in the introduction:</p>
<pre><code><span><span class="co">## compile the model</span></span>
<span><span class="co">#model_intro.compile(optimizer = 'adam', </span></span>
<span><span class="co">#                    loss = keras.losses.CategoricalCrossentropy(), </span></span>
<span><span class="co">#                    metrics = ['accuracy'])</span></span></code></pre>
<div class="section level4">
<h4 id="loss-function">Loss function<a class="anchor" aria-label="anchor" href="#loss-function"></a></h4>
<p>The <strong>loss function</strong> tells the training algorithm how
wrong, or how ‘far away’ from the true value the predicted value is. The
purpose of loss functions is to compute the quantity that a model should
seek to minimize during training. Which class of loss functions you
choose depends on your task.</p>
<p><strong>Loss for classification</strong></p>
<p>For classification purposes, there are a number of probabilistic
losses to choose from. We chose <code>CategoricalCrossentropy</code>
because we want to compute the crossentropy loss between our one-hot
encoded class labels and the model predictions. This loss function is
appropriate to use when the data has two or more label classes.</p>
<p>The loss function is defined by the
<code>tf.keras.losses.CategoricalCrossentropy</code> class.</p>
<p>More information about loss functions can be found in the Keras <a href="https://keras.io/api/losses/" class="external-link">loss documentation</a>.</p>
</div>
<div class="section level4">
<h4 id="optimizer">Optimizer<a class="anchor" aria-label="anchor" href="#optimizer"></a></h4>
<p>Somewhat coupled to the loss function is the
<strong>optimizer</strong>. The optimizer here refers to the algorithm
with which the model learns to optimize on the provided loss
function.</p>
<p>We need to choose which optimizer to use and, if this optimizer has
parameters, what values to use for those. Furthermore, we specify how
many times to present the training samples to the optimizer. In other
words, the optimizer is responsible for taking the output of the loss
function and then applying some changes to the weights within the
network. It is through this process that the “learning” (adjustment of
the weights) is achieved.</p>
<pre><code><span><span class="co">## compile the model</span></span>
<span><span class="co">#model_intro.compile(optimizer = 'adam', </span></span>
<span><span class="co">#                    loss = keras.losses.CategoricalCrossentropy(), </span></span>
<span><span class="co">#                    metrics = ['accuracy'])</span></span></code></pre>
<p><strong>Adam</strong></p>
<p>Here we picked one of the most common optimizers demonstrated to work
well for most tasks, the <strong>Adam</strong> optimizer. Similar to
activation functions, the choice of optimizer depends on the problem you
are trying to solve, your model architecture, and your data. Adam is a
good starting point though, which is why we chose it. Adam has a number
of parameters, but the default values work well for most problems so we
will use it with its default parameters.</p>
<p>It is defined by the <code>keras.optimizers.Adam</code> class and
takes a single parameter <code>learning_rate=0.01</code></p>
<p>The <a href="https://keras.io/api/optimizers/" class="external-link">optimizer
documentation</a> describes the optimizers to choose. A couple more
popular or famous ones include:</p>
<ul><li><p><strong>Stochastic Gradient Descent (sgd)</strong>: Stochastic
Gradient Descent (SGD) is one of the fundamental optimization algorithms
used to train machine learning models, especially neural networks. It is
a variant of the gradient descent algorithm, designed to handle large
datasets efficiently.</p></li>
<li>
<p><strong>Root Mean Square (rms)prop</strong>: RMSprop is widely
used in various deep learning frameworks and is one of the predecessors
of more advanced optimizers like Adam, which further refines the concept
of adaptive learning rates. It is an extension of the basic Stochastic
Gradient Descent (SGD) algorithm and addresses some of the challenges of
SGD.</p>
<ul><li>For example, one of the main issues with the basic SGD is that it
uses a fixed learning rate for all model parameters throughout the
training process. This fixed learning rate can lead to slow convergence
or divergence (over-shooting) in some cases. RMSprop introduces an
adaptive learning rate mechanism to address this problem.</li>
</ul></li>
</ul><div id="accordionSpoiler1" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler1" aria-expanded="false" aria-controls="collapseSpoiler1">
  <h3 class="accordion-header" id="headingSpoiler1">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>WANT TO KNOW MORE: Learning Rate</h3>
</button>
<div id="collapseSpoiler1" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler1" aria-labelledby="headingSpoiler1">
<div class="accordion-body">
<p>ChatGPT</p>
<p><strong>Learning rate</strong> is a hyperparameter that determines
the step size at which the model’s parameters are updated during
training. A higher learning rate allows for more substantial parameter
updates, which can lead to faster convergence, but it may risk
overshooting the optimal solution. On the other hand, a lower learning
rate leads to smaller updates, providing more cautious convergence, but
it may take longer to reach the optimal solution. Finding an appropriate
learning rate is crucial for effectively training machine learning
models.</p>
<p>The figure below illustrates a small learning rate will not traverse
toward the minima of the gradient descent algorithm in a timely manner,
i.e. number of epochs.</p>
<figure><img src="https://developers.google.com/static/machine-learning/crash-course/images/LearningRateTooSmall.svg" title="Small learning rate leads to inefficient approach to loss minima" alt="Plot of loss over weight value illustrating how a small learning rate takes a long time to reach the optimal solution." class="figure mx-auto d-block"><figcaption>Small learning rate leads to inefficient approach to
loss minima</figcaption></figure><p>On the other hand, specifying a learning rate that is <em>too
high</em> will result in a loss value that never approaches the minima.
That is, ‘bouncing between the sides’, thus never reaching a minima to
cease learning.</p>
<figure><img src="https://developers.google.com/static/machine-learning/crash-course/images/LearningRateTooLarge.svg" alt="Plot of loss over weight value illustrating how a large learning rate never approaches the optimal solution because it bounces between the sides." class="figure mx-auto d-block"><figcaption>A large learning rate results in overshooting the
gradient descent minima</figcaption></figure><p>Finally, a modest learning rate will ensure that the product of
multiplying the scalar gradient value and the learning rate does not
result in too small steps, nor a chaotic bounce between sides of the
gradient where steepness is greatest.</p>
<figure><img src="https://developers.google.com/static/machine-learning/crash-course/images/LearningRateJustRight.svg" alt="Plot of loss over weight value illustrating how a good learning rate gets to optimal solution gradually." class="figure mx-auto d-block"><figcaption>An optimal learning rate supports a gradual approach
to the minima</figcaption></figure><p>These images were obtained from <a href="https://developers.google.com/machine-learning/crash-course/reducing-loss/learning-rate" class="external-link">Google
Developers Machine Learning Crash Course</a> and is licenced under the
<a href="https://creativecommons.org/licenses/by/4.0/" class="external-link">Creative Commons
4.0 Attribution Licence</a>.</p>
</div>
</div>
</div>
</div>
</div>
<div class="section level4">
<h4 id="metrics">Metrics<a class="anchor" aria-label="anchor" href="#metrics"></a></h4>
<p>After we select the desired optimizer and loss function we specify
the metric(s) to be evaluated by the model during training and testing.
A <strong>metric</strong> is a function used to judge the performance of
your model.</p>
<pre><code><span><span class="co">## compile the model</span></span>
<span><span class="co">#model_intro.compile(optimizer = 'adam', </span></span>
<span><span class="co">#                    loss = keras.losses.CategoricalCrossentropy(), </span></span>
<span><span class="co">#                    metrics = ['accuracy']) </span></span></code></pre>
<p>Metric functions are similar to loss functions, except the results
from evaluating a metric are not used when training the model. Note you
are able to use any loss function as a metric.</p>
<p>Typically you will use <code>accuracy</code>, which calculates how
often the model predictions match the true labels.</p>
<p>The accuracy function creates two local variables, total and count,
that it uses to compute the frequency with which predictions matches
labels. This frequency is ultimately returned as accuracy: an operation
that divides the total by count.</p>
<p>The Keras <a href="https://keras.io/api/metrics/" class="external-link">metrics</a>
documentation provides a list of potential metrics.</p>
<p>Now that we selected which loss function, optimizer, and metric to
use, we compile the model using <code>model.compile</code>. Compiling
the model prepares it for training.</p>
</div>
</div>
<div class="section level3">
<h3 id="step-6--train-fit-model">Step 6. Train (Fit) model<a class="anchor" aria-label="anchor" href="#step-6--train-fit-model"></a></h3>
<p>We are ready to train the model.</p>
<p>Training the model is done using the <code>fit</code> method. It
takes the image data and target (label) data as inputs and has several
other parameters for certain options of the training. Here we only set a
different number of epochs.</p>
<p>A training <strong>epoch</strong> means that every sample in the
training data has been given to the neural network and used to update
its parameters. In general, CNN models improve with more epochs of
training, but only to a point.</p>
<p>We want to train our model for 10 epochs:</p>
<pre><code>history_intro = model_intro.fit(train_images, train_labels, 
                                epochs = 10, 
                                validation_data = (val_images, val_labels),
                                batch_size = 32)</code></pre>
<p>The <code>batch_size</code> parameter defaults to 32. The
<strong>batch size</strong> is an important hyperparameter that
determines the number of training samples processed together before
updating the model’s parameters during each iteration (or mini-batch) of
training.</p>
<p>Note we are also creating a new variable <code>history_intro</code>
to capture the history of the training in order to extract metrics we
will use for model evaluation.</p>
<p>Other arguments used to fit our model can be found in the
documentation for the <a href="https://keras.io/api/models/model_training_apis/" class="external-link">fit
method</a>.</p>
<div id="accordionSpoiler2" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler2" aria-expanded="false" aria-controls="collapseSpoiler2">
  <h3 class="accordion-header" id="headingSpoiler2">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>WANT TO KNOW MORE: Batch size</h3>
</button>
<div id="collapseSpoiler2" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler2" aria-labelledby="headingSpoiler2">
<div class="accordion-body">
<p>ChatGPT</p>
<p>The choice of batch size can have various implications, and there are
situations where using different batch sizes can be beneficial.</p>
<p><strong>Large Datasets and Memory Constraints</strong>: If you have a
large dataset and limited memory, using a smaller batch size can help
fit the data into memory during training. This allows you to train
larger models or use more complex architectures that might not fit with
larger batch sizes.</p>
<p><strong>Training on GPUs</strong>: Modern deep learning frameworks
and libraries are optimized for parallel processing on GPUs. Using
larger batch sizes can fully leverage the parallelism of GPUs and lead
to faster training times. However, the choice of batch size should
consider the available GPU memory.</p>
<p><strong>Noise in Parameter Updates</strong>: Smaller batch sizes
introduce more noise in the gradients, which can help models escape
sharp minima and potentially find better solutions. This regularization
effect is similar to the impact of stochasticity in Stochastic Gradient
Descent (SGD).</p>
<p><strong>Generalization</strong>: Using smaller batch sizes may
improve the generalization of the model. It prevents the model from
overfitting to the training data, as it gets updated more frequently and
experiences more diverse samples during training.</p>
<p>However, it’s essential to consider the trade-offs of using different
batch sizes. Smaller batch sizes may require more iterations to cover
the entire dataset, which can lead to longer training times. Larger
batch sizes can provide more stable gradients, but might suffer from
generalization issues. There is no one-size-fits-all answer. You should
experiment with different batch sizes to find the best-performing one
for your specific model, architecture, and dataset.</p>
</div>
</div>
</div>
</div>
<div class="section level4">
<h4 id="monitor-training-progress-aka-model-evaluation-during-training">Monitor Training Progress (aka Model Evaluation during
Training)<a class="anchor" aria-label="anchor" href="#monitor-training-progress-aka-model-evaluation-during-training"></a></h4>
<p>We now know more about the compilation and fitting of CNNs. Let us
inspect the training metrics for our model.</p>
<p>Using seaborn, we can plot the training process using the
history:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># convert the history to a dataframe for plotting </span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>history_intro_df <span class="op">=</span> pd.DataFrame.from_dict(history_intro.history)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the loss and accuracy from the training process</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">'cifar_model_pool'</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>sns.lineplot(ax<span class="op">=</span>axes[<span class="dv">0</span>], data<span class="op">=</span>history_intro_df[[<span class="st">'loss'</span>, <span class="st">'val_loss'</span>]])</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>sns.lineplot(ax<span class="op">=</span>axes[<span class="dv">1</span>], data<span class="op">=</span>history_intro_df[[<span class="st">'accuracy'</span>, <span class="st">'val_accuracy'</span>]])</span></code></pre>
</div>
<figure><img src="../fig/04_model_intro_accuracy_loss.png" alt="two panel figure; the figure on the left illustrates the training loss starting at 1.5 and decreasing to 0.7 and the validation loss decreasing from 1.3 to 1.0 before leveling out; the figure on the right illustrates the training accuracy increasing from 0.45 to 0.75 and the validation accuracy increasing from 0.53 to 0.65 before leveling off" class="figure mx-auto d-block"></figure><p>This plot is used to identify whether the training is well configured
or whether there are problems to address. The solid blue lines represent
the training loss and accuracy; the dashed orange lines represent the
validation loss and accuracy.</p>
<div id="inspect-the-training-curve" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="inspect-the-training-curve" class="callout-inner">
<h3 class="callout-title">Inspect the Training Curve<a class="anchor" aria-label="anchor" href="#inspect-the-training-curve"></a>
</h3>
<div class="callout-content">
<p>Inspect the training curves we have just made and recall the
difference between the training and the validation datasets.</p>
<ol style="list-style-type: decimal"><li>How does the training progress?</li>
</ol><ul><li>Does the loss increase or decrease?</li>
<li>What about the accuracy?</li>
<li>Do either change fast or slowly?</li>
<li>Do the graphs lines go up and down frequently (i.e. jitter)?</li>
</ul><ol start="2" style="list-style-type: decimal"><li>Do you think the resulting trained network will work well on the
test set?</li>
</ol></div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<ol style="list-style-type: decimal"><li>The loss curve should drop quite quickly in a smooth line with
little jitter. The accuracy should increase quite quickly in a smooth
line also wtih little jitter.</li>
<li>The results of the training give very little information on its
performance on a test set. You should be careful not to use it as an
indication of a well trained network.</li>
</ol></div>
</div>
</div>
</div>
<p>These is evidence of <strong>overfitting</strong> in these plots. If
a model is overfitting, it means the model performs exceptionally well
on the training data, but poorly on the validation data. Overfitting
occurs when the model has learned to memorize the noise and specific
patterns in the training data instead of generalizing the underlying
relationships. As a result, the model fails to perform well on new,
unseen, data because it has become too specialized to the training
set.</p>
<p>Key characteristics of an overfit model include:</p>
<ul><li><p>High Training Accuracy, Low Validation Accuracy: The model
achieves high accuracy on the training data but significantly lower
accuracy on the validation (or test) data.</p></li>
<li><p>Small Training Loss, Large Validation Loss: The training loss is
low, indicating the model’s predictions closely match the true labels in
the training set. However, the validation loss is high, indicating the
model’s predictions are far from the true labels in the validation
set.</p></li>
</ul><p>How to Address Overfitting:</p>
<ul><li>Reduce the model’s complexity by using fewer layers or units to make
it less prone to overfitting.</li>
<li>Collect more training data if possible to provide the model with a
diverse and representative dataset.</li>
<li>Perform data augmentation to artificially increase the size of the
training data and introduce variability.</li>
</ul><div id="accordionSpoiler3" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler3" aria-expanded="false" aria-controls="collapseSpoiler3">
  <h3 class="accordion-header" id="headingSpoiler3">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>WANT TO KNOW MORE: What is underfitting?</h3>
</button>
<div id="collapseSpoiler3" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler3" aria-labelledby="headingSpoiler3">
<div class="accordion-body">
<p>Underfitting occurs when the model is too simple or lacks the
capacity to capture the underlying patterns and relationships present in
the data. As a result, the model’s predictions are not accurate, and it
fails to generalize well to unseen data.</p>
<p>Key characteristics of an underfit model include:</p>
<ul><li>Low Validation Accuracy: This indicates the model is not learning
from the data effectively.</li>
<li>Large Training Loss: The training loss (error) is high, indicating
the model’s predictions are far from the true labels in the training
set.</li>
<li>Increasing validation loss.</li>
</ul><p>How to address underfitting:</p>
<ul><li>Increase the model’s complexity by adding more layers or units to
the existing layers.</li>
<li>Train the model for more epochs to give it more time to learn from
the data.</li>
<li>Perform data augmentation or feature engineering to provide the
model with more informative input features.</li>
</ul></div>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="improve-model-generalization-avoid-overfitting">Improve Model Generalization (avoid Overfitting)<a class="anchor" aria-label="anchor" href="#improve-model-generalization-avoid-overfitting"></a></h3>
<div class="section level4">
<h4 id="dropout">Dropout<a class="anchor" aria-label="anchor" href="#dropout"></a></h4>
<p>Note the training loss continues to decrease, while the validation
loss stagnates, and even starts to increase over the course of the
epochs. Similarly, the accuracy for the validation set does not improve
anymore after some epochs. This means we are overfitting on our training
data set.</p>
<p>Techniques to avoid overfitting, or to improve model generalization,
are termed <strong>regularization techniques</strong>. One of the most
versatile regularization technique is <strong>dropout</strong>
(Srivastava et al., 2014). Dropout essentially means that during each
training cycle a random fraction of the dense layer nodes are turned
off. This is described with the dropout rate between zero and one, which
determines the fraction of nodes to silence at a time.</p>
<figure><img src="../fig/04-neural_network_sketch_dropout.png" alt="diagram of two neural networks; the first network is densely connected without dropout and the second network has some of the neurons dropped out of of the network" class="figure mx-auto d-block"></figure><p>The intuition behind dropout is that it enforces redundancies in the
network by constantly removing different elements of a network. The
model can no longer rely on individual nodes and instead must create
multiple “paths”. In addition, the model has to make predictions with
much fewer nodes and weights (connections between the nodes). As a
result, it becomes much harder for a network to memorize particular
features. At first this might appear a quite drastic approach which
affects the network architecture strongly. In practice, however, dropout
is computationally a very elegant solution which does not affect
training speed. And it frequently works very well.</p>
<div id="callout1" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout1"></a>
</h3>
<div class="callout-content">
<p>Dropout layers will only randomly silence nodes during training!
During a predictions step, all nodes remain active (dropout is off).
During training, the sample of nodes that are silenced are different for
each training instance, to give all nodes a chance to observe enough
training data to learn its weights.</p>
</div>
</div>
</div>
<p>Dropout layers are defined by the `tf.keras.layers.Dropout class and
have the following definition:</p>
<pre><code>tf.keras.layers.Dropout(rate, noise_shape=None, seed=None, **kwargs)</code></pre>
<p>The <code>rate</code> parameter is a float between 0 and 1 and
represents the fraction of the input units to drop.</p>
<p>We want to add one Dropout Layer to our network that randomly drops
80 per cent of the input units but where should we put it?</p>
<p>The placement of the dropout layer matters. Adding dropout before or
after certain layers can have different effects. For example, it’s
common to place dropout after convolutional and dense layers but not
typically after pooling layers. Let us add a third convolutional layer
to our model and then the dropout layer.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define the inputs, layers, and outputs of a CNN model with dropout</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># CNN Part 1</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Input layer of 32x32 images with three channels (RGB)</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>inputs_dropout <span class="op">=</span> keras.Input(shape<span class="op">=</span>train_images.shape[<span class="dv">1</span>:])</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co"># CNN Part 2</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Convolutional layer with 32 filters, 3x3 kernel size, and ReLU activation</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>x_dropout <span class="op">=</span> keras.layers.Conv2D(<span class="dv">16</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(inputs_dropout)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Pooling layer with input window sized 2,2</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>x_dropout <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x_dropout)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Second Convolutional layer with 32 filters, 3x3 kernel size, and ReLU activation</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>x_dropout <span class="op">=</span> keras.layers.Conv2D(<span class="dv">32</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x_dropout)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Second Pooling layer with input window sized 2,2</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>x_dropout <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x_dropout)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Second Convolutional layer with 64 filters, 3x3 kernel size, and ReLU activation</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>x_dropout <span class="op">=</span> keras.layers.Conv2D(<span class="dv">64</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x_dropout) <span class="co"># This is new!</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Dropout layer andomly drops 60 per cent of the input units</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>x_dropout <span class="op">=</span> keras.layers.Dropout(<span class="fl">0.6</span>)(x_dropout) <span class="co"># This is new!</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Flatten layer to convert 2D feature maps into a 1D vector</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>x_dropout <span class="op">=</span> keras.layers.Flatten()(x_dropout)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Dense layer with 128 neurons and ReLU activation</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>x_dropout <span class="op">=</span> keras.layers.Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x_dropout)</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a><span class="co"># CNN Part 3</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Output layer with 10 units (one for each class) and softmax activation</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>outputs_dropout <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">'softmax'</span>)(x_dropout)</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="co"># create the dropout model</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>model_dropout <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs_dropout, outputs<span class="op">=</span>outputs_dropout, name<span class="op">=</span><span class="st">"cifar_model_dropout"</span>)</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>model_dropout.summary()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Model: "cifar_model_dropout"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 32, 32, 3)]       0         
                                                                 
 conv2d_2 (Conv2D)           (None, 30, 30, 16)        448       
                                                                 
 max_pooling2d_2 (MaxPoolin  (None, 15, 15, 16)        0         
 g2D)                                                            
                                                                 
 conv2d_3 (Conv2D)           (None, 13, 13, 32)        4640      
                                                                 
 max_pooling2d_3 (MaxPoolin  (None, 6, 6, 32)          0         
 g2D)                                                            
                                                                 
 conv2d_4 (Conv2D)           (None, 4, 4, 64)          18496     
                                                                 
 dropout (Dropout)           (None, 4, 4, 64)          0         
                                                                 
 flatten_1 (Flatten)         (None, 1024)              0         
                                                                 
 dense_2 (Dense)             (None, 128)               131200    
                                                                 
 dense_3 (Dense)             (None, 10)                1290      
                                                                 
=================================================================
Total params: 156074 (609.66 KB)
Trainable params: 156074 (609.66 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________</code></pre>
</div>
<p>Note the dropout does not alter the dimensions of the image and has
zero parameters.</p>
<div id="does-adding-a-dropout-layer-improve-our-model" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="does-adding-a-dropout-layer-improve-our-model" class="callout-inner">
<h3 class="callout-title">Does adding a Dropout Layer improve our
model?<a class="anchor" aria-label="anchor" href="#does-adding-a-dropout-layer-improve-our-model"></a>
</h3>
<div class="callout-content">
<p>Write the code to compile and fit our new dropout model using the
same arguments we used for our model in the introduction. Then inspect
the training metrics to determine whether our model has improved or not
by adding a dropout layer.</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># compile the dropout model</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>model_dropout.<span class="bu">compile</span>(optimizer <span class="op">=</span> <span class="st">'adam'</span>,</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>              loss <span class="op">=</span> keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>              metrics <span class="op">=</span> [<span class="st">'accuracy'</span>])</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the dropout model</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>history_dropout <span class="op">=</span> model_dropout.fit(train_images, train_labels, </span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>                                    epochs<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>                                    validation_data<span class="op">=</span>(val_images, val_labels),</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>                                    batch_size <span class="op">=</span> <span class="dv">32</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co"># save dropout model</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>model_dropout.save(<span class="st">'fit_outputs/model_dropout.keras'</span>)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="co"># inspect the training results</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="co"># convert the history to a dataframe for plotting </span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>history_dropout_df <span class="op">=</span> pd.DataFrame.from_dict(history_dropout.history)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the loss and accuracy from the training process</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">'cifar_model_dropout'</span>)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>sns.lineplot(ax<span class="op">=</span>axes[<span class="dv">0</span>], data<span class="op">=</span>history_dropout_df[[<span class="st">'loss'</span>, <span class="st">'val_loss'</span>]])</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>sns.lineplot(ax<span class="op">=</span>axes[<span class="dv">1</span>], data<span class="op">=</span>history_dropout_df[[<span class="st">'accuracy'</span>, <span class="st">'val_accuracy'</span>]])</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>val_loss_dropout, val_acc_dropout <span class="op">=</span> model_dropout.evaluate(val_images, val_labels, verbose<span class="op">=</span><span class="dv">2</span>)</span></code></pre>
</div>
<figure><img src="../fig/04_model_dropout_accuracy_loss.png" alt="two panel figure; the figure on the left illustrates the training loss starting at 1.7 and decreasing to 1.0 and the validation loss decreasing from 1.4 to 0.9 before leveling out; the figure on the right illustrates the training accuracy increasing from 0.40 to 0.65 and the validation accuracy increasing from 0.5 to 0.67" class="figure mx-auto d-block"></figure><p>In this relatively uncommon situation, the training loss is higher
than our validation loss while the validation accuracy is higher than
the training accuracy. Using dropout, or other regularization techniques
during training, can lead to a lower training accuracy.</p>
<p>Dropout randomly “drops out” units during training, which can prevent
the model from fitting the training data too closely. This
regularization effect may lead to a situation where the model
generalizes better on the validation set.</p>
<p>The final accuracy on the validation set is higher than without
dropout.</p>
</div>
</div>
</div>
</div>
<div id="accordionSpoiler4" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler4" aria-expanded="false" aria-controls="collapseSpoiler4">
  <h3 class="accordion-header" id="headingSpoiler4">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>WANT TO KNOW MORE: Regularization methods for Convolutional Neural Networks (CNNs)</h3>
</button>
<div id="collapseSpoiler4" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler4" aria-labelledby="headingSpoiler4">
<div class="accordion-body">
<p>ChatGPT</p>
<p><strong>Regularization</strong> methods introduce constraints or
penalties to the training process, encouraging the model to be simpler
and less prone to overfitting. Here are some common regularization
methods for CNNs:</p>
<p><strong>L1 and L2 Regularization</strong>: L1 and L2 regularization
are the two most common regularization techniques used in deep learning.
They add a penalty term to the loss function during training to restrict
the model’s weights.</p>
<ul><li><p>L1 regularization adds the absolute value of the weights to the
loss function. It tends to produce sparse weight vectors, forcing some
of the less important features to have exactly zero weights.</p></li>
<li><p>L2 regularization adds the square of the weights to the loss
function. It encourages the model to have smaller weights overall,
preventing extreme values and reducing the impact of individual
features.</p></li>
</ul><p>The regularization strength is controlled by a hyperparameter, often
denoted as lambda (λ), that determines how much weight should be given
to the regularization term. A larger λ value increases the impact of
regularization, making the model simpler and more regularized.</p>
<ol start="2" style="list-style-type: lower-alpha"><li>randomly “dropping out” a fraction of neurons during training. This
means during each training iteration, some neurons are temporarily
removed from the network. Dropout effectively reduces the
interdependence between neurons, preventing the network from relying too
heavily on specific neurons, and making it more robust.</li>
</ol><p><strong>Batch Normalization</strong>: While not explicitly a
regularization technique, Batch Normalization has a regularizing effect
on the model. It normalizes the activations of each layer in the
network, reducing internal covariate shift. This can improve training
stability and reduce the need for aggressive dropout or weight
decay.</p>
<p><strong>Data Augmentation</strong>: Data augmentation is a technique
where the training data is artificially augmented by applying various
transformations like rotation, scaling, flipping, and cropping to create
new examples. This increases the diversity of the training data and
helps the model generalize better to unseen data.</p>
<p><strong>Early Stopping</strong>: Early stopping is a form of
regularization that stops the training process when the model’s
performance on a validation set starts to degrade. It prevents the model
from overfitting by avoiding further training after the point of best
validation performance.</p>
<p>Using regularization techniques improves the generalization
performance of CNNs and reduces the risk of overfitting. It’s essential
to experiment with different regularization methods and hyperparameters
to find the optimal combination for your specific CNN architecture and
dataset.</p>
</div>
</div>
</div>
</div>
</div>
</div>
<section id="choose-the-best-model-and-use-it-to-predict"><h2 class="section-heading">Choose the best model and use it to predict<a class="anchor" aria-label="anchor" href="#choose-the-best-model-and-use-it-to-predict"></a>
</h2>
<hr class="half-width"><p>Based on our evaluation of the loss and accuracy metrics, the
<code>model_dropout</code> appears to have the best performance
<strong>of the models we have examined thus far</strong>. The next step
is to use this model to predict the object classes on our test
dataset.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul><li>Use model.compile to compile a CNN.</li>
<li>The choice of loss function will depend on your data and aim.</li>
<li>The choice of optimizer often depends on experimentation and
empirical evaluation.</li>
<li>Use model.fit to make a train (fit) a CNN.</li>
<li>Training/validation loss and accuracy can be used to evaluate a
model during training.</li>
<li>Dropout is one way to prevent overfitting.</li>
</ul></div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></div> <!-- / div.lesson-content -->
    </main><!-- / main#main-content.main-content --><nav class="bottom-pagination mx-md-4" aria-label="Previous and Next Chapter"><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/03-build-cnn.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/05-evaluate-predict-cnn.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/03-build-cnn.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Build a
        </a>
        <a class="chapter-link float-end" href="../instructor/05-evaluate-predict-cnn.html" rel="next">
          Next: Evaluate a... 
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
    </nav></div> <!-- / div.primary-content.col-xs-12 -->
<!-- END:   inst/pkgdown/templates/content-instructor.html-->

      </div><!--/div.row-->
      		<footer class="row footer mx-md-3"><hr><div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>
        
        <a href="https://https://github.com/erinmgraham/icwithcnn/edit/main/episodes/04-fit-cnn.html" class="external-link">Edit on GitHub</a>
        
	
        | <a href="https://https://github.com/erinmgraham/icwithcnn/blob/main/CONTRIBUTING.html" class="external-link">Contributing</a>
        | <a href="https://https://github.com/erinmgraham/icwithcnn/" class="external-link">Source</a></p>
				<p><a href="https://https://github.com/erinmgraham/icwithcnn/blob/main/CITATION" class="external-link">Cite</a> | <a href="mailto:erin.graham@jcu.edu.au">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">
        
        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>
        
        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.2" class="external-link">sandpaper (0.16.2)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.3" class="external-link">pegboard (0.7.3)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.1" class="external-link">varnish (1.0.1)</a></p>
			</div>
		</footer></div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://.github.io/github.com/instructor/04-fit-cnn.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries",
  "name": "Compile and Train (Fit) a Convolutional Neural Network",
  "creativeWorkStatus": "active",
  "url": "https://.github.io/github.com/instructor/04-fit-cnn.html",
  "identifier": "https://.github.io/github.com/instructor/04-fit-cnn.html",
  "dateCreated": "2023-05-03",
  "dateModified": "2023-12-12",
  "datePublished": "2024-01-23"
}

  </script><script>
		feather.replace();
	</script><!-- Matomo
    2022-11-07: we have gotten a notification that we have an overage for our
    tracking and I'm pretty sure this has to do with Workbench usage.
    Considering that I am not _currently_ using this tracking because I do not
    yet know how to access the data, I am turning this off for now.
  <script>
    var _paq = window._paq = window._paq || [];
    /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
    _paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
    _paq.push(["setDomains", ["*.preview.carpentries.org","*.datacarpentry.github.io","*.datacarpentry.org","*.librarycarpentry.github.io","*.librarycarpentry.org","*.swcarpentry.github.io", "*.carpentries.github.io"]]);
    _paq.push(["setDoNotTrack", true]);
    _paq.push(["disableCookies"]);
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
          var u="https://carpentries.matomo.cloud/";
          _paq.push(['setTrackerUrl', u+'matomo.php']);
          _paq.push(['setSiteId', '1']);
          var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
          g.async=true; g.src='https://cdn.matomo.cloud/carpentries.matomo.cloud/matomo.js'; s.parentNode.insertBefore(g,s);
        })();
  </script>
  End Matomo Code --></body></html><!-- END:   inst/pkgdown/templates/layout.html-->

