<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>Image Classification with Convolutional Neural Networks: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="../assets/styles.css">
<script src="../assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="manifest" href="../site.webmanifest">
<link rel="mask-icon" href="../safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md navbar-light bg-white top-nav incubator"><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-6">
      <div class="large-logo">
        <img alt="Carpentries Incubator" src="../assets/images/incubator-logo.svg"><abbr class="icon" title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught." style="text-decoration: unset">
           
          <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#early-development-pre-alpha-through-alpha" class="external-link alert-link" style="color: #383838">Pre-Alpha
            <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="color: #FF4955; border-radius: 5px"></i>
          </a>
          <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
        </abbr>
        
      </div>
    </div>
    <div class="selector-container">
      
      
      <div class="dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='../aio.html';">Learner View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl navbar-light bg-white bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="../assets/images/incubator-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      Image Classification with Convolutional Neural Networks
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
      <i role="img" aria-label="search button" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            Image Classification with Convolutional Neural Networks
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/instructor-notes.html">Instructor Notes</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/images.html">Extract All Images</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"></ul>
</li>
      </ul>
</div>
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
<input class="form-control me-2 searchbox" type="search" placeholder="Search" aria-label="Search"><button class="btn btn-outline-success tablet-search-button" type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="search button"></i>
        </button>
      </fieldset>
</form>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Image Classification with Convolutional Neural Networks
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="../aio.html">Learner View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->
      
            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Schedule</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="setup-gpu.html">1. Setup - GPU</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="01-introduction.html">2. Introduction to Deep Learning</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="02-image-data.html">3. Introduction to Image Data</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="03-build-cnn.html">4. Build a Convolutional Neural Network</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="04-fit-cnn.html">5. Compile and Train a Convolutional Neural Network</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="05-evaluate-predict-cnn.html">6. Evaluate a Convolutional Neural Network and Make Predictions (Classifications)</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush8">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading8">
        <a href="06-conclusion.html">7. Conclusion</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="../instructor/key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="../instructor/instructor-notes.html">Instructor Notes</a>
                      </li>
                      <li>
                        <a href="../instructor/images.html">Extract All Images</a>
                      </li>
                      
                    </ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width resources">
<a href="../instructor/aio.html">See all in one page</a>
            

            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">
            
            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-setup-gpu"><p>Content from <a href="setup-gpu.html">Setup - GPU</a></p>
<hr>
<p> Last updated on 2023-10-12 | 
        
        <a href="https://https://github.com/erinmgraham/icwithcnn/edit/main/episodes/setup-gpu.html" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right"> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<p>These instructions are for setting up tensorflow in a
<strong>GPU</strong> capable environment. Because this is a more
advanced topic and installation varies depending on your computer’s
architecture, please make sure you set up and test this up ahead of
time. We will not be able to spend class time assisting on GPU
setups.</p>
<section id="software-setup"><h2 class="section-heading">Software Setup<a class="anchor" aria-label="anchor" href="#software-setup"></a>
</h2>
<hr class="half-width">
<div id="installing-python-using-anaconda" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="installing-python-using-anaconda" class="callout-inner">
<h3 class="callout-title">Installing Python using Anaconda<a class="anchor" aria-label="anchor" href="#installing-python-using-anaconda"></a>
</h3>
<div class="callout-content">
<p><a href="https://python.org" class="external-link">Python</a> is a popular language for
scientific computing, and a frequent choice for machine learning as
well. Installing all of its scientific packages individually can be a
bit difficult, however, so we recommend the installer <a href="https://www.anaconda.com/products/individual" class="external-link">Anaconda</a> which
includes most (but not all) of the software you will need.</p>
<p>Regardless of how you choose to install it, please make sure you
install Python version 3.x (e.g., 3.9 is fine).</p>
<p>Also, please set up your python environment at least a day in advance
of the workshop. If you encounter problems with the installation
procedure, ask your workshop organizers via e-mail for assistance so you
are ready to go as soon as the workshop begins.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">
  Windows
  </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>Check out the <a href="https://www.youtube.com/watch?v=xxQ0mzZ8UvA" class="external-link">Windows - Video
tutorial</a> or:</p>
<ol style="list-style-type: decimal">
<li><p>Open [<a href="https://www.anaconda.com/products/distribution" class="external-link uri">https://www.anaconda.com/products/distribution</a>] with
your web browser.</p></li>
<li><p>Download the Python 3 installer for Windows.</p></li>
<li><p>Double-click the executable and install Python 3 using
<em>MOST</em> of the default settings. The only exception is to check
the <strong>Make Anaconda the default Python</strong> option. (Note this
may already be checked.)</p></li>
</ol>
</div>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">
  MacOS
  </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>Check out the <a href="https://www.youtube.com/watch?v=TcSAln46u9U" class="external-link">Mac OS X - Video
tutorial</a> or:</p>
<ol style="list-style-type: decimal">
<li><p>Open [<a href="https://www.anaconda.com/products/distribution" class="external-link uri">https://www.anaconda.com/products/distribution</a>] with
your web browser.</p></li>
<li><p>Download the Python 3 installer for Mac.</p></li>
<li><p>Install Python 3 using all of the defaults for
installation.</p></li>
</ol>
</div>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">
  Linux
  </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<p>Note that the following installation steps require you to work from
the shell. If you run into any difficulties, please request help before
the workshop begins.</p>
<ol style="list-style-type: decimal">
<li><p>Open [<a href="https://www.anaconda.com/products/distribution" class="external-link uri">https://www.anaconda.com/products/distribution</a>] with
your web browser.</p></li>
<li><p>Download the Python 3 installer for Linux.</p></li>
<li>
<p>Install Python 3 using all of the defaults for installation.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Open a terminal window.</p></li>
<li><p>Navigate to the folder where you downloaded the
installer</p></li>
<li><p>Type</p></li>
</ol>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">bash</span> Anaconda3-</span></code></pre>
</div>
<p>and press tab. The name of the file you just downloaded should
appear.</p>
<ol start="4" style="list-style-type: lower-alpha">
<li><p>Press enter.</p></li>
<li><p>Follow the text-only prompts. When the license agreement appears
(a colon will be present at the bottom of the screen) hold the down
arrow until the bottom of the text. Type <code>yes</code> and press
enter to approve the license. Press enter again to approve the default
location for the files. Type <code>yes</code> and press enter to prepend
Anaconda to your <code>PATH</code> (this makes the Anaconda distribution
the default Python).</p></li>
</ol>
</li>
</ol>
</div>
</div>
</div>
</div>
</section><section id="installing-the-required-packages"><h2 class="section-heading">Installing the required packages<a class="anchor" aria-label="anchor" href="#installing-the-required-packages"></a>
</h2>
<hr class="half-width">
<div id="callout1" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout1"></a>
</h3>
<div class="callout-content">
<p><a href="https://docs.conda.io/projects/conda/en/latest/" class="external-link">Conda</a>
is the package management system associated with <a href="https://www.anaconda.com/products/individual" class="external-link">Anaconda</a> and
runs on Windows, macOS and Linux.</p>
<p>Conda should already be available in your system once you installed
Anaconda successfully. Conda thus works regardless of the operating
system. Make sure you have an up-to-date version of Conda running. See
<a href="https://docs.anaconda.com/anaconda/install/update-version/" class="external-link">these
instructions</a> for updating Conda if required.</p>
</div>
</div>
</div>
<p>To create a conda environment called <code>cnn_workshop_gpu</code>
with the required packages, launch an Anaconda Prompt (terminal) and
type the command:</p>
<pre class="code"><code>(base) C:\Users\Lab&gt; conda create --name cnn_workshop_gpu python=3.9 spyder seaborn  scikit-learn pandas scikeras</code></pre>
<p>Activate the newly created environment:</p>
<pre class="code"><code>(base) C:\Users\Lab&gt; conda activate cnn_workshop_gpu</code></pre>
<p>NOTE the prompt should change to reflect you are in the
(cnn_workshop_gpu) environment.</p>
<div id="callout2" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout2"></a>
</h3>
<div class="callout-content">
<p>To set up a GPU environment you need to make sure that you have the
appropriate hardware, system, and software necessary for GPU support.
Here we are following the <a href="https://www.tensorflow.org/install/pip#windows-native_1" class="external-link">Windows
TensorFlow installation instructions</a> starting at <strong>Step 5. GPU
setup</strong> but using Anaconda instead of Miniconda. Specific
instructions can also be found there for <a href="https://www.tensorflow.org/install/pip#macos_1" class="external-link">MacOS</a> and <a href="https://www.tensorflow.org/install/pip#linux_1" class="external-link">Linux</a>
environments.</p>
</div>
</div>
</div>
<div class="section level4">
<h4 id="nvidia-gpu">NVIDIA GPU<a class="anchor" aria-label="anchor" href="#nvidia-gpu"></a>
</h4>
<p>First install NVIDIA GPU driver [<a href="https://www.nvidia.com/download/index.aspx" class="external-link uri">https://www.nvidia.com/download/index.aspx</a>] if you have
not.</p>
<p>Then install the CUDA, cuDNN with conda.</p>
<pre class="code"><code>(cnn_workshop_gpu) C:\Users\Lab&gt; conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0</code></pre>
<p>Install tensorflow using <a href="" title="https://pip.pypa.io/en/stable/">pip</a> (python’s package
manager), first making sure you have a recent version of pip:</p>
<pre class="code"><code>(cnn_workshop_gpu) C:\Users\Lab&gt;pip install --upgrade pip

# Anything above 2.10 is not supported on the GPU on Windows Native
(cnn_workshop_gpu) C:\Users\Lab&gt;pip install "tensorflow&lt;2.11" </code></pre>
</div>
<div class="section level4">
<h4 id="amd-gpu">AMD GPU<a class="anchor" aria-label="anchor" href="#amd-gpu"></a>
</h4>
<p>First install AMD GPU driver [<a href="https://www.amd.com/en/support" class="external-link uri">https://www.amd.com/en/support</a>] if you have not.</p>
<p>Note that modern versions of Tensorflow make Keras available as a
module: <code>from tensorflow import keras</code></p>
<p>TODO Check if this troublshooting section is still relevant:</p>
</div>
<div class="section level3">
<h3 id="troubleshooting-for-windows">Troubleshooting for Windows<a class="anchor" aria-label="anchor" href="#troubleshooting-for-windows"></a>
</h3>
<p>It is possible that Windows users will run into version conflicts. If
you are on Windows and get errors running the command, you can try
installing the packages using pip within a conda environment:</p>
<pre class="code"><code>conda create -n cnn_workshop_gpu python spyder
conda activate cnn_workshop
pip install tensorflow&gt;=2.5 seaborn scikit-learn pandas</code></pre>
<p><a href="" title="https://pip.pypa.io/en/stable/">pip</a> is the
package management system for Python software packages. It is integrated
into your local Python installation and runs regardless of your
operating system too.</p>
</div>
<div class="section level3">
<h3 id="troubleshooting-for-macs-with-apple-silicon-chip">Troubleshooting for Macs with Apple silicon chip<a class="anchor" aria-label="anchor" href="#troubleshooting-for-macs-with-apple-silicon-chip"></a>
</h3>
<p>Newer Macs (from 2020 onwards) often have a different kind of chip,
manufactured by Apple instead of Intel. This can lead to problems
installing Tensorflow . If you get errors running the installation
command or conda hangs endlessly, you can try installing Tensorflow for
Mac with pip:</p>
<pre class="conda"><code>pip install tensorflow-macos # TODO check if diff GPU version</code></pre>
</div>
</section><section id="starting-spyder"><h2 class="section-heading">Starting Spyder<a class="anchor" aria-label="anchor" href="#starting-spyder"></a>
</h2>
<hr class="half-width">
<p>We will teach using Python in <a href="https://www.spyder-ide.org/" class="external-link">Spyder</a> (Scientific Python
Development Environment) , a free integrated development environment
(IDE) written in Python that comes with Anaconda.Editing, interactive
testing, debugging, and introspection tools are all included in Spyder.
If you installed Python using Anaconda, Spyder should already be on your
system. If you did not use Anaconda, use the Python package manager pip
(see the <a href="https://docs.spyder-ide.org/current/installation.html" class="external-link">Spyder
website</a> for details.)</p>
<p>To start Spyder, open an Anaconda prompt (terminal), activate the
tensorflow environment for this workshop of not already, and launch the
app:</p>
<pre class="conda"><code>(cnn_workshop_gpu) C:\Users\Lab&gt;conda activate cnn_workshop_gpu
(cnn_workshop_gpu) C:\Users\Lab&gt;spyder</code></pre>
</section><section id="check-your-setup"><h2 class="section-heading">Check your setup<a class="anchor" aria-label="anchor" href="#check-your-setup"></a>
</h2>
<hr class="half-width">
<p>To check whether all packages installed correctly, go to the
interactive <code>IPython Console</code> in Spyder (lower right hand
side panel) and type in the following commands:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'sklearn version: '</span>, sklearn.__version__)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'seaborn version: '</span>, seaborn.__version__)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'pandas version: '</span>, pandas.__version__)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> keras</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Keras version: '</span>, keras.__version__)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Tensorflow version: '</span>, tensorflow.__version__)</span></code></pre>
</div>
<p>This should output the versions of all required packages without
giving errors. Most versions will work fine with this lesson, but:</p>
<ul>
<li>For Keras and Tensorflow, the minimum version is 2.2.4</li>
<li>For sklearn, the minimum version is 0.22.</li>
</ul></section><section id="fallback-option-cloud-environment"><h2 class="section-heading">Fallback option: cloud environment<a class="anchor" aria-label="anchor" href="#fallback-option-cloud-environment"></a>
</h2>
<hr class="half-width">
<p>TODO</p>
</section><section id="downloading-the-required-datasets"><h2 class="section-heading">Downloading the required datasets<a class="anchor" aria-label="anchor" href="#downloading-the-required-datasets"></a>
</h2>
<hr class="half-width">
<p>Download the <a href="https://www.cs.toronto.edu/~kriz/cifar.html" class="external-link">The CIFAR-10
dataset</a>.</p>
<p>TODO cifar comes with keras; need to work out if we want to provide
ahead of time - might need to be munged</p>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-01-introduction"><p>Content from <a href="01-introduction.html">Introduction to Deep Learning</a></p>
<hr>
<p> Last updated on 2023-10-12 | 
        
        <a href="https://https://github.com/erinmgraham/icwithcnn/edit/main/episodes/01-introduction.html" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time <i aria-hidden="true" data-feather="clock"></i> 10 minutes </p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right"> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is machine learning and what is it used for?</li>
<li>What is deep learning?</li>
<li>How do I use a neural network for image classification?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Explain the difference between artificial intelligence, machine
learning and deep learning</li>
<li>Explain how machine learning is used for regression and
classification tasks</li>
<li>Understand what algorithms are used for image classification</li>
<li>Know difference between training, testing, and validation
datasets</li>
<li>Perform an image classification using a convolutional neural network
(CNN)</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section id="what-is-machine-learning"><h2 class="section-heading">What is machine learning?<a class="anchor" aria-label="anchor" href="#what-is-machine-learning"></a>
</h2>
<hr class="half-width">
<p>Machine learning is a set of tools and techniques which let us find
patterns in data. This lesson will introduce you to only one of these
techniques, <strong>Deep Learning</strong> with <strong>Convolutional
Neural Network</strong>, abbreviated as <strong>CNN</strong>, but there
are many more.</p>
<p>The techniques breakdown into two broad categories, predictors and
classifiers. Predictors are used to predict a value (or set of values)
given a set of inputs, for example trying to predict the cost of
something given the economic conditions and the cost of raw materials or
predicting a country’s GDP given its life expectancy. Classifiers try to
classify data into different categories, or assign a label; for example,
deciding what characters are visible in a picture of some writing or if
a message is spam or not.</p>
</section><section id="training-data"><h2 class="section-heading">Training Data<a class="anchor" aria-label="anchor" href="#training-data"></a>
</h2>
<hr class="half-width">
<p>Many (but not all) machine learning systems “learn” by taking a
series of input data and output data and using it to form a model. The
maths behind the machine learning doesn’t care what the data is as long
as it can represented numerically or categorised. Some examples might
include:</p>
<ul>
<li>predicting a person’s weight based on their height</li>
<li>predicting house prices given stock market prices</li>
<li>classifying if an email is spam or not</li>
<li>classifying an image as eg, person, place, or particular object</li>
</ul>
<p>Typically we will need to train our models with hundreds, thousands
or even millions of examples before they work well enough to do any
useful predictions or classifications with them.</p>
</section><section id="deep-learning-machine-learning-and-artificial-intelligence"><h2 class="section-heading">Deep Learning, Machine Learning and Artificial Intelligence<a class="anchor" aria-label="anchor" href="#deep-learning-machine-learning-and-artificial-intelligence"></a>
</h2>
<hr class="half-width">
<p>Deep Learning (DL) is just one of many machine learning techniques,
in which people often talk about machine learning being a form of
artificial intelligence (AI). Definitions of artificial intelligence
vary, but usually involve having computers mimic the behaviour of
intelligent biological systems. Since the 1950s many works of science
fiction have dealt with the idea of an artificial intelligence which
matches (or exceeds) human intelligence in all areas. Although there
have been great advances in AI and ML research recently, we can only
come close to human like intelligence in a few specialist areas and are
still a long way from a general purpose AI. The image below shows some
differences between artificial intelligence, Machine Learning and Deep
Learning.</p>
<figure><img src="../fig/01_AI_ML_DL_differences.png" alt="Three nested circles describing AI as the largest circle in dark blue; enclosing machine learning in medium blue; enclosing deep learning in even lighter blue" class="figure mx-auto d-block"><figcaption>The image above is by Tukijaaliwa, CC BY-SA 4.0, via
Wikimedia Commons, <a href="https://en.wikipedia.org/wiki/File:AI-ML-DL.svg" class="external-link">original
source</a></figcaption></figure><div id="callout1" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout1"></a>
</h3>
<div class="callout-content">
<p>Concept: Differentiation between classical ML models and Deep
Learning models:</p>
<p><strong>Traditional ML algorithms</strong> can only use one (possibly
two layers) of data transformation to calculate an output (shallow
models). With high dimensional data and growing feature space (possible
set of values for any given feature), shallow models quickly run out of
layers to calculate outputs.</p>
<p><strong>Deep neural networks</strong> (constructed with multiple
layers of neurons) are the extension of shallow models with three
layers: input, hidden, and outputs layers. The hidden layer is where
learning takes place. As a result, deep learning is best applied to
large datasets for training and prediction. As observations and feature
inputs decrease, shallow ML approaches begin to perform noticeably
better.</p>
</div>
</div>
</div>
</section><section id="preparing-the-code"><h2 class="section-heading">Preparing the code<a class="anchor" aria-label="anchor" href="#preparing-the-code"></a>
</h2>
<hr class="half-width">
<p>It is the goal of this training workshop to produce a Deep Learning
program, using a Convolutional Neural Network. At the end of this
workshop, we hope that this code can be used as a “starting point”. We
will be creating an “initial program” for this introduction chapter,
that will be copied and used as a foundation for the rest of the
episodes.</p>
<p>The code that is being created will load the following libraries, so
the program will be initialised with the following:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load the keras package, which includes the CIFAR-10 dataset, that will be used later.</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> keras</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code></pre>
</div>
</section><section id="what-is-image-classification"><h2 class="section-heading">What is image classification?<a class="anchor" aria-label="anchor" href="#what-is-image-classification"></a>
</h2>
<hr class="half-width">
<p>Image classification is a fundamental task in computer vision, which
is a field of artificial intelligence focused on teaching computers to
interpret and understand visual information from the world. Image
classification specifically involves the process of assigning a label or
category to an input image. The goal is to enable computers to recognize
and categorize objects, scenes, or patterns within images, just as a
human would. Image classification can refer to one of several tasks:</p>
<figure><img src="../fig/01_Fei-Fei_Li_Justin_Johnson_Serena_Young__CS231N_2017.png" alt="Four types of image classification tasks include semantic segmentation where every pixel is labelled; classification and localization that detects a single object like a cat; object detection that detects multiple objects like cats and dogs; and instance segmentation that detects each pixel of multiple objects" class="figure mx-auto d-block"></figure><p>Image classification has numerous practical applications,
including:</p>
<ul>
<li>
<strong>Object Recognition</strong>: Identifying objects within
images, such as cars, animals, or household items.</li>
<li>
<strong>Medical Imaging</strong>: Diagnosing diseases from medical
images like X-rays or MRIs.</li>
<li>
<strong>Quality Control</strong>: Inspecting products for defects on
manufacturing lines.</li>
<li>
<strong>Autonomous Vehicles</strong>: Identifying pedestrians,
traffic signs, and other vehicles in self-driving cars.</li>
<li>
<strong>Security and Surveillance</strong>: Detecting anomalies or
unauthorized objects in security footage.</li>
</ul>
<p>Convolutional Neural Networks (CNNs) have become a cornerstone in
image classification due to their ability to automatically learn
hierarchical features from images and achieve remarkable performance on
a wide range of tasks.</p>
</section><section id="deep-learning-workflow"><h2 class="section-heading">Deep Learning Workflow<a class="anchor" aria-label="anchor" href="#deep-learning-workflow"></a>
</h2>
<hr class="half-width">
<p>To apply Deep Learning to a problem there are several steps we need
to go through:</p>
<div class="section level3">
<h3 id="step-1--formulate-outline-the-problem">Step 1. Formulate / Outline the problem<a class="anchor" aria-label="anchor" href="#step-1--formulate-outline-the-problem"></a>
</h3>
<p>Firstly we must decide what it is we want our Deep Learning system to
do. This lesson is all about image classification so our aim is to put
an image into one of a few categories. Specifically in our case, we will
be looking at 10 categories: airplane, automobile, bird, cat, deer, dog,
frog, horse, ship, truck’</p>
</div>
<div class="section level3">
<h3 id="step-2--identify-inputs-and-outputs">Step 2. Identify inputs and outputs<a class="anchor" aria-label="anchor" href="#step-2--identify-inputs-and-outputs"></a>
</h3>
<p>Next we need to identify what the inputs and outputs of the neural
network will be. In our case, the data is images and the inputs could be
the individual pixels of the images. We are performing a classification
problem and we will have one output for each potential class.</p>
</div>
<div class="section level3">
<h3 id="step-3--prepare-data">Step 3. Prepare data<a class="anchor" aria-label="anchor" href="#step-3--prepare-data"></a>
</h3>
<p>Many datasets are not ready for immediate use in a neural network and
will require some preparation. Neural networks can only really deal with
numerical data, so any non-numerical data (eg images) will have to be
somehow converted to numerical data. Information on how this is done and
what the data looks like will be explored in the next episode <a href="02-image-data">Introduction to Image Data</a>.</p>
<p>Next we will need to divide the data into multiple sets. One of these
will be used by the training process and we will call it the
<strong>training set</strong>. Another set, called the
<strong>validation set</strong>, will be used during the training
process to tune hyperparameters. A third <strong>test set</strong> is
used to assess the final performance of the trained model.</p>
<p>For this lesson, we will be using an existing image dataset known as
CIFAR-10 that we will discuss in more depth in the next episode.</p>
</div>
<div class="section level3">
<h3 id="load-data">Load data<a class="anchor" aria-label="anchor" href="#load-data"></a>
</h3>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load the cifar dataset included with the keras packages</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>(train_images, train_labels), (val_images, val_labels) <span class="op">=</span> keras.datasets.cifar10.load_data()</span></code></pre>
</div>
<div id="challenge-load-the-cifar-10-dataset" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-load-the-cifar-10-dataset" class="callout-inner">
<h3 class="callout-title">Challenge Load the CIFAR-10 dataset<a class="anchor" aria-label="anchor" href="#challenge-load-the-cifar-10-dataset"></a>
</h3>
<div class="callout-content">
<p>Explain the output of these commands?</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Train: Images=</span><span class="sc">%s</span><span class="st">, Labels=</span><span class="sc">%s</span><span class="st">'</span> <span class="op">%</span> (train_images.shape, train_labels.shape))</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Validate: Images=</span><span class="sc">%s</span><span class="st">, Labels=</span><span class="sc">%s</span><span class="st">'</span> <span class="op">%</span> (val_images.shape, val_labels.shape))</span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">
  Output
  </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Train: Images=(50000, 32, 32, 3), Labels=(50000, 1)
Validate: Images=(10000, 32, 32, 3), Labels=(10000, 1)</code></pre>
</div>
<p>The training set consists of 50000 images of 32x32 pixels and 3
channels (RGB values) and labels. The validation set consists of 10000
images of 32x32 pixels and 3 channels (RGB values) and labels.</p>
</div>
</div>
</div>
</div>
<p>Image RGB values are between 0 and 255. For input of neural networks,
it is better to have small input values. We will talk more about why
this important in the episode on image data but for now we will
normalize our data between 0 and 1.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># normalize the RGB values to be between 0 and 1</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>train_images <span class="op">=</span> train_images <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>val_images <span class="op">=</span> val_images <span class="op">/</span> <span class="fl">255.0</span></span></code></pre>
</div>
<p>The labels are a set of single numbers denoting the class and we map
the class numbers back to the class names, taken from the
documentation:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a list of classnames</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>class_names <span class="op">=</span> [<span class="st">'airplane'</span>, <span class="st">'automobile'</span>, <span class="st">'bird'</span>, <span class="st">'cat'</span>, <span class="st">'deer'</span>, <span class="st">'dog'</span>, <span class="st">'frog'</span>, <span class="st">'horse'</span>, <span class="st">'ship'</span>, <span class="st">'truck'</span>]</span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="visualize-a-subset-of-the-cifar-10-dataset">Visualize a subset of the CIFAR-10 dataset<a class="anchor" aria-label="anchor" href="#visualize-a-subset-of-the-cifar-10-dataset"></a>
</h3>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a figure object and specify width, height in inches</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">10</span>))</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co"># plot a subset of the images </span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">25</span>):</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">5</span>,<span class="dv">5</span>,i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    plt.imshow(train_images[i], cmap<span class="op">=</span>plt.cm.binary)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    plt.title(class_names[train_labels[i,<span class="dv">0</span>]])</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="../fig/01_cifar10.png" alt="Subset of 25 CIFAR-10 images displayed in five rows and five columns " class="figure mx-auto d-block"></figure>
</div>
<div class="section level3">
<h3 id="step-4--choose-a-pre-trained-model-or-build-a-new-architecture-from-scratch">Step 4. Choose a pre-trained model or build a new architecture from
scratch<a class="anchor" aria-label="anchor" href="#step-4--choose-a-pre-trained-model-or-build-a-new-architecture-from-scratch"></a>
</h3>
<p>Often we can use an existing neural network instead of designing one
from scratch. Training a network can take a lot of time and
computational resources. There are a number of well publicised networks
which have been shown to perform well at certain tasks, if you know of
one which already does a similar task well then it makes sense to use
one of these.</p>
<p>If instead we decide we do want to design our own network then we
need to think about how many input neurons it will have, how many hidden
layers and how many outputs, what types of layers we use (we will
explore the different types later on). This will probably need some
experimentation and we might have to try tweaking the network design a
few times before we see acceptable results.</p>
<div class="section level4">
<h4 id="define-the-model">Define the Model<a class="anchor" aria-label="anchor" href="#define-the-model"></a>
</h4>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Input layer of 32x32 images with three channels (RGB)</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>inputs_intro <span class="op">=</span> keras.Input(shape<span class="op">=</span>train_images.shape[<span class="dv">1</span>:])</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Convolutional layer with 50 filters, 3x3 kernel size, and ReLU activation</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>x_intro <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(inputs_intro)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Second Convolutional layer</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>x_intro <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x_intro)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Flatten layer to convert 2D feature maps into a 1D vector</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>x_intro <span class="op">=</span> keras.layers.Flatten()(x_intro)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Output layer with 10 units (one for each class)</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>outputs_intro <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>)(x_intro)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="co"># create the model</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>model_intro <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs_intro, outputs<span class="op">=</span>outputs_intro, name<span class="op">=</span><span class="st">"cifar_model_intro"</span>)</span></code></pre>
</div>
</div>
</div>
<div class="section level3">
<h3 id="step-5--choose-a-loss-function-and-optimizer">Step 5. Choose a loss function and optimizer<a class="anchor" aria-label="anchor" href="#step-5--choose-a-loss-function-and-optimizer"></a>
</h3>
<p>The loss function tells the training algorithm how far away the
predicted value was from the true value. We will look at choosing a loss
function in more detail later on.</p>
<p>The optimizer is responsible for taking the output of the loss
function and then applying some changes to the weights within the
network. It is through this process that the “learning” (adjustment of
the weights) is achieved.</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># compile the model</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>model_intro.<span class="bu">compile</span>(optimizer <span class="op">=</span> <span class="st">'adam'</span>,</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>                    loss <span class="op">=</span> keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>                    metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="step-6--train-the-model">Step 6. Train the model<a class="anchor" aria-label="anchor" href="#step-6--train-the-model"></a>
</h3>
<p>We can now go ahead and start training our neural network. We will
probably keep doing this for a given number of iterations through our
training dataset (referred to as epochs) or until the loss function
gives a value under a certain threshold.</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the model</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>history_intro <span class="op">=</span> model_intro.fit(train_images, train_labels, epochs <span class="op">=</span> <span class="dv">10</span>, </span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>                                validation_data <span class="op">=</span> (val_images, val_labels))</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co"># save the model</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>model_intro.save(<span class="st">'fit_outputs/model_intro.h5'</span>)</span></code></pre>
</div>
<p>Your output will begin to print similar to the output below:</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Epoch 1/10

1563/1563 [==============================] - 5s 3ms/step - loss: 1.4011 - accuracy: 0.5046 - val_loss: 1.3644 - val_accuracy: 0.5243</code></pre>
</div>
<div id="callout2" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout2"></a>
</h3>
<div class="callout-content">
<p><strong>What does this output mean?</strong></p>
<p>This output printed during the fit phase i.e. training the model
against known image labels, can be broken down as follows: -
<code>Epoch</code> descibes the number of full passes over all
<em>training data</em>. In the Output above there are <strong>1563
training observations</strong>. An epoch will conclude and move to the
next epoch after a training pass over all 1563 observations. -
<code>loss</code> and <code>val_loss</code> can be considered as
related. Where <code>loss</code> is a value the model will attempt to
minimise, and is the distance between the true label of an image and the
models prediction. Minimising this distance is where <em>learning</em>
occurs to adjust weights and bias which reduce <code>loss</code>. On the
other hand <code>val_loss</code> is a value calculated against the
validation data and is a measurement of the models performance against
<strong>unseen data</strong>. Both values are a summation of errors made
for each example when fitting to the training or validation sets. -
<code>accuracy</code> and <code>val_accuracy</code> can also be
considered as related. Unlike <code>loss</code> and
<code>val_loss</code>, these values are a percentage and are only
revelent to <strong>classification problems</strong>. The
<code>val_accuracy</code> score can be used to communicate a percentage
value of model effectiveness on unseen data.</p>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="step-7--perform-a-predictionclassification">Step 7. Perform a Prediction/Classification<a class="anchor" aria-label="anchor" href="#step-7--perform-a-predictionclassification"></a>
</h3>
<p>After training the network we can use it to perform predictions. This
is the mode you would use the network in after you have fully trained it
to a satisfactory performance. Doing predictions on a special hold-out
set is used in the next step to measure the performance of the
network.</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># specify a new image and prepare it to match CIFAR-10 dataset</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> icwithcnn_functions <span class="im">import</span> prepare_image_icwithcnn</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>new_img_path <span class="op">=</span> <span class="st">"../data/Jabiru_TGS.JPG"</span> <span class="co"># path to image</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>new_img_prepped <span class="op">=</span> prepare_image_icwithcnn(new_img_path)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co"># predict the class name</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>result_intro <span class="op">=</span> model_intro.predict(new_img_prepped) <span class="co"># make prediction</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">' The predicted probability of each class is: </span><span class="ch">\n</span><span class="st">'</span>, result_intro.<span class="bu">round</span>(<span class="dv">4</span>))</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The class with the highest predicted probability is: '</span>, class_names[result_intro.argmax()])</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code> The predicted probability of each class is: 
 [[-50.365475 -44.362732 -36.384968 -48.386 -32.87043 -66.691696 -29.284208 -51.06697  -36.475967 -43.751118]]
The class with the highest predicted probability is:  frog</code></pre>
</div>
<div id="callout3" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout3"></a>
</h3>
<div class="callout-content">
<p>My result is different!</p>
<p>While the neural network itself is deterministic, various factors in
the training process, system setup, and data variability can lead to
small variations in the output. These variations are usually minor and
should not significantly impact the overall performance or behavior of
the model.</p>
<p>If you are finding significant differences in the model predictions,
this could be a sign that the model is not fully converged, where
“convergence” refers to the point where the model has reached an optimal
or near-optimal state in terms of learning from the training data.</p>
</div>
</div>
</div>
<p>Congratulations, you just created your first image classification
model and used it to classify an image!</p>
<p>Unfortunately the classification was incorrect. Why might that be?
and What can we do about?</p>
<p>There are many ways we can try to improve the accuracy of our model,
such as adding or removing layers to the model definition and
fine-tuning the hyperparameters, which takes us to the next steps in our
workflow.</p>
</div>
<div class="section level3">
<h3 id="step-8--measure-performance">Step 8. Measure Performance<a class="anchor" aria-label="anchor" href="#step-8--measure-performance"></a>
</h3>
<p>Once we trained the network we want to measure its performance. To do
this we use some additional data that was <strong>not</strong> part of
the training; this is known as a test set. There are many different
methods available for measuring performance and which one is best
depends on the type of task we are attempting. These metrics are often
published as an indication of how well our network performs.</p>
</div>
<div class="section level3">
<h3 id="step-9--tune-hyperparameters">Step 9. Tune Hyperparameters<a class="anchor" aria-label="anchor" href="#step-9--tune-hyperparameters"></a>
</h3>
<p>When building image recognition models in Python, especially using
libraries like TensorFlow or Keras, the process involves not only
designing a neural network but also choosing the best values for various
hyperparameters that govern the training process.</p>
<div class="section level4">
<h4 id="what-are-hyperparameters">What are hyperparameters?<a class="anchor" aria-label="anchor" href="#what-are-hyperparameters"></a>
</h4>
<p>Hyperparameters are all the parameters set by the person configuring
the machine learning instead of those learned by the algorithm itself.
These hyperparameters can include the learning rate, the number of
layers in the network, the number of neurons per layer, and many more.
Hyperparameter tuning refers to the process of systematically searching
for the best combination of hyperparameters that will optimize the
model’s performance. One common method for hyperparameter tuning is
<strong>grid search</strong>.</p>
</div>
<div class="section level4">
<h4 id="what-is-grid-search">What is Grid Search?<a class="anchor" aria-label="anchor" href="#what-is-grid-search"></a>
</h4>
<p>Grid Search or <strong>GridSearch</strong> (as per the library
function call) is foundation method for hyperparameter tuning. The aim
of hyperparameter tuning is to define a grid of possible values for each
hyperparameter you want to tune. GridSearch will then evaluate the model
performance for each combination of hyperparameters in a brute-force
manner, iterating through every possible combination in the grid.</p>
<p>These concepts will be continued, with practical examples in Episode
05.</p>
</div>
</div>
<div class="section level3">
<h3 id="step-10--share-model">Step 10. Share Model<a class="anchor" aria-label="anchor" href="#step-10--share-model"></a>
</h3>
<p>Now that we have a trained network that performs at a level we are
happy with we can go and use it on real data to perform a prediction. At
this point we might want to consider publishing a file with both the
architecture of our network and the weights which it has learned
(assuming we did not use a pre-trained network). This will allow others
to use it as as pre-trained network for their own purposes and for them
to (mostly) reproduce our result.</p>
<p>We will return to these workflow steps throughout this lesson and
discuss each component in more detail.</p>
<div id="accordionInstructor1" class="accordion instructor-note accordion-flush">
<div class="accordion-item">
<button class="accordion-button instructor-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseInstructor1" aria-expanded="false" aria-controls="collapseInstructor1">
  <h3 class="accordion-header" id="headingInstructor1">
  <div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="edit-2"></i></div>
  Instructor Note
  </h3>
</button>
<div id="collapseInstructor1" class="accordion-collapse collapse" aria-labelledby="headingInstructor1" data-bs-parent="#accordionInstructor1">
<div class="accordion-body">
<p>Inline instructor notes can help inform instructors of timing
challenges associated with the lessons. They appear in the “Instructor
View”</p>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Keypoints<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Machine learning is the process where computers learn to recognise
patterns of data</li>
<li>Machine learning is used for regression and classification
tasks</li>
<li>Deep learning is a subset of machine learning, which is a subset of
artificial intelligence</li>
<li>Convolutional neural networks are well suited for image
classification</li>
<li>To use Deep Learning effectively we need to go through a workflow
of: defining the problem, identifying inputs and outputs, preparing
data, choosing the type of network, training the model, tuning
hyperparameters, measuring performance before we can classify data.</li>
</ul>
</div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</div>
</section></section><section id="aio-02-image-data"><p>Content from <a href="02-image-data.html">Introduction to Image Data</a></p>
<hr>
<p> Last updated on 2023-10-12 | 
        
        <a href="https://https://github.com/erinmgraham/icwithcnn/edit/main/episodes/02-image-data.html" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time <i aria-hidden="true" data-feather="clock"></i> 12 minutes </p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right"> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How much data do you need for Deep Learning?</li>
<li>Where can I find image data to train my model?</li>
<li>How do I plot image data in python?</li>
<li>How do I prepare image data for use in a convolutional neural
network (CNN)?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Identify sources of image data</li>
<li>Write code to plot image data</li>
<li>Understand the properties of image data</li>
<li>Prepare an image data set to train a convolutional neural network
(CNN)</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section id="deep-learning-workflow"><h2 class="section-heading">Deep Learning Workflow<a class="anchor" aria-label="anchor" href="#deep-learning-workflow"></a>
</h2>
<hr class="half-width">
<p>Let’s start over with the first steps in our workflow.</p>
<div class="section level3">
<h3 id="step-1--formulate-outline-the-problem">Step 1. Formulate/ Outline the problem<a class="anchor" aria-label="anchor" href="#step-1--formulate-outline-the-problem"></a>
</h3>
<p>Firstly we must decide what it is we want our Deep Learning system to
do. This lesson is all about image classification and our aim is to put
an image into one of ten categories: airplane, automobile, bird, cat,
deer, dog, frog, horse, ship, or truck</p>
</div>
<div class="section level3">
<h3 id="step-2--identify-inputs-and-outputs">Step 2. Identify inputs and outputs<a class="anchor" aria-label="anchor" href="#step-2--identify-inputs-and-outputs"></a>
</h3>
<p>Next we need to identify what the inputs and outputs of the neural
network will be. In our case, the data is images and the inputs could be
the individual pixels of the images.</p>
<p>We are performing a classification problem and we want to output one
category for each image.</p>
</div>
<div class="section level3">
<h3 id="step-3--prepare-data">Step 3. Prepare data<a class="anchor" aria-label="anchor" href="#step-3--prepare-data"></a>
</h3>
<p>Deep Learning requires extensive training using example data which
shows the network what output it should produce for a given input. In
this workshop our network will be trained by being “shown” a series of
images and told what they contain. Once the network is trained it should
be able to take another image and correctly classify its contents.</p>
<p>You can use pre-existing data or prepare your own.</p>
<div class="section level4">
<h4 id="pre-existing-image-data">Pre-existing image data<a class="anchor" aria-label="anchor" href="#pre-existing-image-data"></a>
</h4>
<p>In some cases you will be able to download an image dataset that is
already labelled and can be used to classify a number of different
object like we see with the CIFAR-10 dataset. Other examples
include:</p>
<ul>
<li>
<a href="https://en.wikipedia.org/wiki/MNIST_database" class="external-link">MNIST
database</a> - 60,000 training images of handwritten digits (0-9)</li>
<li>
<a href="https://www.image-net.org/" class="external-link">ImageNet</a> - 14 million
hand-annotated images indicating objects from more than 20,000
categories. ImageNet sponsors an <a href="https://www.image-net.org/challenges/LSVRC/#:~:text=The%20ImageNet%20Large%20Scale%20Visual,image%20classification%20at%20large%20scale." class="external-link">annual
software contest</a> where programs compete to achieve the highest
accuracy. When choosing a pretrained network, the winners of these sorts
of competitions are generally a good place to start.</li>
<li>
<a href="https://cocodataset.org/#home" class="external-link">MS COCO</a> - &gt;200,000
labelled images used for object detection, instance segmentation,
keypoint analysis, and captioning</li>
</ul>
<p>Where labelled data exists, in most cases the data provider or other
users will have created functions that you can use to load the data. We
already saw an example of this in the introduction:</p>
<pre><code><span><span class="co"># load the CIFAR-10 dataset included with the keras packages</span></span>
<span><span class="co">#from tensorflow import keras</span></span>
<span></span>
<span><span class="co"># commented out in case these are already be in memory</span></span>
<span><span class="co">#(train_images, train_labels), (val_images, val_labels) = keras.datasets.cifar10.load_data()</span></span></code></pre>
<p>In this instance the data is likely already prepared for use in a
CNN. However, it is always a good idea to first read any associated
documentation to find out what steps the data providers took to prepare
the images and second to take a closer at the images once loaded and
query their attributes.</p>
<div id="callout1" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout1"></a>
</h3>
<div class="callout-content">
<p>How much data do you need for Deep Learning?</p>
<p>The rise of Deep Learning is partially due to the increased
availability of very large datasets. But how much data do you actually
need to train a Deep Learning model? Unfortunately, this question is not
easy to answer. It depends, among other things, on the complexity of the
task (which you often do not know beforehand), the quality of the
available dataset and the complexity of the network. For complex tasks
with large neural networks, we often see that adding more data continues
to improve performance. However, this is also not a generic truth: if
the data you add is too similar to the data you already have, it will
not give much new information to the neural network.</p>
<p>In case you have too little data available to train a complex network
from scratch, it is sometimes possible to use a pretrained network that
was trained on a similar problem. Another trick is data augmentation,
where you expand the dataset with artificial data points that could be
real. An example of this is mirroring images when trying to classify
cats and dogs. An horizontally mirrored animal retains the label, but
exposes a different view.</p>
</div>
</div>
</div>
</div>
<div class="section level4">
<h4 id="custom-image-data">Custom image data<a class="anchor" aria-label="anchor" href="#custom-image-data"></a>
</h4>
<p>In other cases, you will need to create your own set of labelled
images.</p>
<p><strong>Custom data i. Data collection and Labeling:</strong></p>
<p>For image classification the label applies to the entire image;
object detection requires bounding boxes around objects of interest, and
instance or semantic segmentation requires each pixel to be
labelled.</p>
<p>There are a number of open source software that can be used to label
your dataset, including:</p>
<ul>
<li>(Visual Geometry Group) <a href="https://www.robots.ox.ac.uk/~vgg/software/via/" class="external-link">VGG Image
Annotator</a> (VIA)</li>
<li>
<a href="https://imagej.net/" class="external-link">ImageJ</a> can be extended with
plugins for annotation</li>
<li>
<a href="https://github.com/jsbroks/coco-annotator" class="external-link">COCO
Annotator</a> is designed specifically for creating annotations
compatible with Common Objects in Context (COCO) format</li>
</ul>
<p><strong>Custom data ii. Data preprocessing:</strong></p>
<p>This step involves various tasks to enhance the quality and
consistency of the data:</p>
<ul>
<li><p><strong>Resizing</strong>: Resize images to a consistent
resolution to ensure uniformity and reduce computational load.</p></li>
<li><p><strong>Normalization</strong>: Scale pixel values to a common
range, often between 0 and 1 or -1 and 1. Normalization helps the model
converge faster during training.</p></li>
<li><p><strong>Data Augmentation</strong>: Apply random transformations
(e.g., rotations, flips, shifts) to create new variations of the same
image. This helps improve the model’s robustness and generalization by
exposing it to more diverse data.</p></li>
<li><p><strong>Color Channels</strong>: Depending on the model and
library you use, you might need to handle different color channel orders
(RGB, BGR, etc.).</p></li>
</ul>
<p>Before we look at some of these tasks in more detail we need to
understand that the images we see on hard copy, view with our electronic
devices, or process with our programs are represented and stored in the
computer as numeric abstractions, or approximations of what we see with
our eyes in the real world. And before we begin to learn how to process
images with Python programs, we need to spend some time understanding
how these abstractions work.</p>
</div>
</div>
<div class="section level3">
<h3 id="pixels">Pixels<a class="anchor" aria-label="anchor" href="#pixels"></a>
</h3>
<p>It is important to realise that images are stored as rectangular
arrays of hundreds, thousands, or millions of discrete “picture
elements,” otherwise known as pixels. Each pixel can be thought of as a
single square point of coloured light.</p>
<p>For example, consider this image of a Jabiru, with a square area
designated by a red box:</p>
<figure><img src="../fig/02_Jabiru_TGS_marked.jpg" alt="Original size image of a Jabiru with a red square surrounding an area to zoom in on" class="figure mx-auto d-block"></figure><p>Now, if we zoomed in close enough to see the pixels in the red box,
we would see something like this:</p>
<figure><img src="../fig/02_Jabiru_TGS_marked_zoom_enlarged.jpg" alt="Enlarged image area of Jabiru" class="figure mx-auto d-block"></figure><p>Note that each square in the enlarged image area - each pixel - is
all one colour, but that each pixel can have a different colour from its
neighbors. Viewed from a distance, these pixels seem to blend together
to form the image we see.</p>
</div>
<div class="section level3">
<h3 id="working-with-pixels">Working with Pixels<a class="anchor" aria-label="anchor" href="#working-with-pixels"></a>
</h3>
<p>As noted, in practice, real world images will typically be made up of
a vast number of pixels, and each of these pixels will be one of
potentially millions of colours. In python, an image can be represented
as a multidimensional array, also known as a <code>tensor</code>, where
each element in the array corresponds to a pixel value in the image. In
the context of images, these arrays often have dimensions for height,
width, and color channels (if applicable).</p>
<div id="callout2" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout2"></a>
</h3>
<div class="callout-content">
<p>Matrices, arrays, images and pixels</p>
<p>The matrix is mathematical concept - numbers evenly arranged in a
rectangle. This can be a two dimensional rectangle, like the shape of
the screen you’re looking at now. Or it could be a three dimensional
equivalent, a cuboid, or have even more dimensions, but always keeping
the evenly spaced arrangement of numbers. In computing, array refers to
a structure in the computer’s memory where data is stored in
evenly-spaced elements. This is strongly analogous to a matrix. A NumPy
array is a type of variable (a simpler example of a type is an integer).
For our purposes, the distinction between matrices and arrays is not
important, we don’t really care how the computer arranges our data in
its memory. The important thing is that the computer stores values
describing the pixels in images, as arrays. And the terms matrix and
array can be used interchangeably.</p>
</div>
</div>
</div>
<div id="callout3" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout3"></a>
</h3>
<div class="callout-content">
<p>Python image libraries</p>
<p>Two of the most commonly used libraries for image representation and
manipulation are NumPy and Pillow (PIL). Additionally, when working with
deep learning frameworks like TensorFlow and PyTorch, images are often
represented as tensors within these frameworks.</p>
<ul>
<li>NumPy is a powerful library for numerical computing in Python. It
provides support for creating and manipulating arrays, which can be used
to represent images as multidimensional arrays.
<ul>
<li><code>import numpy as np</code></li>
</ul>
</li>
<li>The Pillow library (PIL fork) provides functions to open,
manipulate, and save various image file formats. It represents images
using its own Image class.
<ul>
<li><code>from PIL import Image</code></li>
<li>see <a href="https://pillow.readthedocs.io/en/latest/reference/Image.html" class="external-link">PIL
Image Module</a>
</li>
</ul>
</li>
<li>TensorFlow images are often represented as tensors that have
dimensions for batch size, height, width, and color channels. This
framework provide tools to load, preprocess, and work with image data
seamlessly.
<ul>
<li><code>from tensorflow import keras</code></li>
<li>see <a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image" class="external-link">image
preprocessing</a> documentation</li>
<li>Note Keras image functions also use PIL</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
<p>Let us start by looking at the image we used in the introduction.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load the libraries required</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.utils <span class="im">import</span> img_to_array</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.utils <span class="im">import</span> load_img</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># specify the image path</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>new_img_path <span class="op">=</span> <span class="st">"../data/Jabiru_TGS.JPG"</span> <span class="co"># path to image</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co"># read in the image with default arguments</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>new_img_pil <span class="op">=</span> load_img(new_img_path)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co"># confirm the data class and size</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The new image is of type :'</span>, new_img_pil.__class__, <span class="st">'and has the size'</span>, new_img_pil.size)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The new image is of type : &lt;class 'PIL.JpegImagePlugin.JpegImageFile'&gt; and has the size (552, 573)</code></pre>
</div>
</div>
<div class="section level3">
<h3 id="image-dimensions---resizing">Image Dimensions - Resizing<a class="anchor" aria-label="anchor" href="#image-dimensions---resizing"></a>
</h3>
<p>Here we see our new image has shape <code>(573, 552, 3)</code>,
meaning it is much larger in size, 573x552 pixels; a rectangle instead
of a square; and consists of 3 colour channels (RGB).</p>
<p>Recall from the introduction that our training data set consists of
50000 images of 32x32 pixels and 3 channels.</p>
<p>To reduce the computational load and ensure all of our images have a
uniform size, we need to choose an image resolution (or size in pixels)
and ensure that all of the images we use are resized to that shape to be
consistent.</p>
<p>There are a couple of ways to do this in python but one way is to
specify the size you want using an argument to the
<code>load_img()</code> function from <code>keras.utils</code>.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># read in the new image and specify the target size to be the same as our training images</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>new_img_pil_small <span class="op">=</span> load_img(new_img_path, target_size<span class="op">=</span>(<span class="dv">32</span>,<span class="dv">32</span>))</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># confirm the data class and shape</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The new image is still of type:'</span>, new_img_pil_small.__class__, <span class="st">'but now has the same size'</span>, new_img_pil_small.size, <span class="st">'as our training data'</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The new image is still of type: &lt;class 'PIL.Image.Image'&gt; but now has the same size (32, 32) as our training data.</code></pre>
</div>
</div>
<div class="section level3">
<h3 id="normalization">Normalization<a class="anchor" aria-label="anchor" href="#normalization"></a>
</h3>
<p>Image RGB values are between 0 and 255. As input for neural networks,
it is better to have small input values. The process of converting the
RGB values to be between 0 and 1 is called
<strong>normalization</strong>.</p>
<div id="callout4" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout4"></a>
</h3>
<div class="callout-content">
<p>ChatGPT</p>
<p>Normalizing the RGB values to be between 0 and 1 is a common
pre-processing step in machine learning tasks, especially when dealing
with image data. This normalization has several benefits:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Numerical Stability</strong>: By scaling the RGB values
to a range between 0 and 1, you avoid potential numerical instability
issues that can arise when working with large values. Neural networks
and many other machine learning algorithms are sensitive to the scale of
input features, and normalizing helps to keep the values within a
manageable range.</p></li>
<li><p><strong>Faster Convergence</strong>: Normalizing the RGB values
often helps in faster convergence during the training process. Neural
networks and other optimization algorithms rely on gradient descent
techniques, and having inputs in a consistent range aids in smoother and
faster convergence.</p></li>
<li><p><strong>Equal Weightage for All Channels</strong>: In RGB images,
each channel (Red, Green, Blue) represents different color intensities.
By normalizing to the range [0, 1], you ensure that each channel is
treated with equal weightage during training. This is important because
some machine learning algorithms could assign more importance to larger
values.</p></li>
<li><p><strong>Generalization</strong>: Normalization helps the model to
generalize better to unseen data. When the input features are in the
same range, the learned weights and biases can be more effectively
applied to new examples, making the model more robust.</p></li>
<li><p><strong>Compatibility</strong>: Many image-related libraries,
algorithms, and models expect pixel values to be in the range of [0, 1].
By normalizing the RGB values, you ensure compatibility and seamless
integration with these tools.</p></li>
</ol>
<p>The normalization process is typically done by dividing each RGB
value (ranging from 0 to 255) by 255, which scales the values to the
range [0, 1].</p>
<p>For example, if you have an RGB image with pixel values (100, 150,
200), after normalization, the pixel values would become (100/255,
150/255, 200/255) ≈ (0.39, 0.59, 0.78).</p>
<p>Remember that normalization is not always mandatory, and there could
be cases where other scaling techniques might be more suitable based on
the specific problem and data distribution. However, for most
image-related tasks in machine learning, normalizing RGB values to [0,
1] is a good starting point.</p>
</div>
</div>
</div>
<p>Before we can normalize our image values we must convert the image to
an numpy array.</p>
<p>We saw how to do this in the introduction but what you may not have
noticed is that the <code>keras.datasets.cifar10.load_data</code>
function did the conversion for us whereas now we will do it
ourselves.</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># convert the Image into an array for normalization</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>new_img_arr <span class="op">=</span> img_to_array(new_img_pil_small)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># confirm the data class and shape</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The new image is now of type :'</span>, new_img_arr.__class__, <span class="st">'and has the shape'</span>, new_img_arr.shape)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The new image is now of type : &lt;class 'numpy.ndarray'&gt; and has the shape (32, 32, 3)</code></pre>
</div>
<p>Now we can normalize the values. Let us also investigate the image
values before and after we normalize them.</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># extract the min, max, and mean pixel values</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The min, max, and mean pixel values are'</span>, new_img_arr.<span class="bu">min</span>(), <span class="st">','</span>, new_img_arr.<span class="bu">max</span>(), <span class="st">', and'</span>, new_img_arr.mean().<span class="bu">round</span>(), <span class="st">'respectively.'</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># normalize the RGB values to be between 0 and 1</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>new_img_arr_norm <span class="op">=</span> new_img_arr <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># extract the min, max, and mean pixel values AFTER</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'After normalization, the min, max, and mean pixel values are'</span>, new_img_arr_norm.<span class="bu">min</span>(), <span class="st">','</span>, new_img_arr_norm.<span class="bu">max</span>(), <span class="st">', and'</span>, new_img_arr_norm.mean().<span class="bu">round</span>(), <span class="st">'respectively.'</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The min, max, and mean pixel values are 0.0 , 255.0 , and 87.0 respectively.
After normalization, the min, max, and mean pixel values are 0.0 , 1.0 , and 0.0 respectively.</code></pre>
</div>
<p>Of course, if there are a large number of images to prepare you do
not want to copy and paste these steps for each image we have in our
dataset.</p>
<p>Here we will use <code>for</code> loops to find all the images in a
directory and prepare them to create a test dataset that aligns with our
training dataset.</p>
</div>
</section><section id="cinic-10-test-dataset-preparation"><h2 class="section-heading">CINIC-10 Test Dataset Preparation<a class="anchor" aria-label="anchor" href="#cinic-10-test-dataset-preparation"></a>
</h2>
<hr class="half-width">
<p>Our test dataset is a sample of images from an existing image dataset
known as <a href="https://github.com/BayesWatch/cinic-10/" class="external-link">CINIC-10</a>
(CINIC-10 Is Not ImageNet or CIFAR-10) that was designed to be used as a
drop-in alternative to the CIFAR-10 dataset we used in the
introduction.</p>
<p>The test image directory was set up to have the following
structure:</p>
<pre><code><span><span class="va">main_directory</span><span class="op">/</span></span>
<span><span class="va">...class_a</span><span class="op">/</span></span>
<span><span class="va">......image_1.jpg</span></span>
<span><span class="va">......image_2.jpg</span></span>
<span><span class="va">...class_b</span><span class="op">/</span></span>
<span><span class="va">......image_1.jpg</span></span>
<span><span class="va">......image_2.jpg</span></span></code></pre>
<p>We will use this structure to create two lists: one for images in
those folders and one for the image label. We can then use the lists to
resize, convert to arrays, and normalize the images.</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># set the mian directory  </span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>test_image_dir <span class="op">=</span> <span class="st">'D:/20230724_CINIC10/test_images'</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co"># make two lists of the subfolders (ie class or label) and filenames</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>test_filenames <span class="op">=</span> []</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>test_labels <span class="op">=</span> []</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> dn <span class="kw">in</span> os.listdir(test_image_dir):</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> fn <span class="kw">in</span> os.listdir(os.path.join(test_image_dir, dn)):</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>        test_filenames.append(fn)</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>        test_labels.append(dn)</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="co"># prepare the images</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="co"># create an empty numpy array to hold the processed images</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>test_images <span class="op">=</span> np.empty((<span class="bu">len</span>(test_filenames), <span class="dv">32</span>, <span class="dv">32</span>, <span class="dv">3</span>), dtype<span class="op">=</span>np.float32)</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="co"># use the dirnames and filenanes to process each </span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(test_filenames)):</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># set the path to the image</span></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    img_path <span class="op">=</span> os.path.join(test_image_dir, test_labels[i], test_filenames[i])</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># load the image and resize at the same time</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> load_img(img_path, target_size<span class="op">=</span>(<span class="dv">32</span>,<span class="dv">32</span>))</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># convert to an array</span></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>    img_arr <span class="op">=</span> img_to_array(img)</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># normalize</span></span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>    test_images[i] <span class="op">=</span> img_arr<span class="op">/</span><span class="fl">255.0</span></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(test_images.shape)</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(test_images.__class__)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(10000, 32, 32, 3)
&lt;class 'numpy.ndarray'&gt;</code></pre>
</div>
<div id="challenge1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge1"></a>
</h3>
<div class="callout-content">
<p>Training and Test sets</p>
<p>Take a look at the training and test set we created.</p>
<p>Q1. How many samples does the training set have and are the classes
well balanced?</p>
<p>Q2. How many samples does the test set have and are the classes well
balanced?</p>
<p>Hint1: Check the object class to understand what methods are
available.</p>
<p>Hint2: Use the `train_labels’ object to find out if the classes are
well balanced.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">
  Show me the solution
  </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>Q1. Training Set</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load training images into memory if not already</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="co"># (train_images, train_labels), (val_images, val_labels) = keras.datasets.cifar10.load_data()</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The training set is of type'</span>, train_images.__class__)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The training set has'</span>, train_images.shape[<span class="dv">0</span>] <span class="st">'samples.</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The number of labels in our training set and the number images in each class are:</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.unique(train_labels, return_counts<span class="op">=</span><span class="va">True</span>))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The training set is of type &lt;class 'numpy.ndarray'&gt;
The training set has 50000 samples.

The number of labels in our training set and the number images in each class are:

(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),
 array([5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000], dtype=int64))</code></pre>
</div>
<p>Q2. Test Set (we can use the same code as the training set)</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The test set is of type'</span>, test_images.__class__)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The test set has'</span>, test.shape[<span class="dv">0</span>] <span class="st">'samples.</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The number of labels in our test set and the number images in each class are:</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.unique(test_labels, return_counts<span class="op">=</span><span class="va">True</span>))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The test set is of type &lt;class 'numpy.ndarray'&gt;
The test set has 10000 samples.

The number of labels in our test set and the number images in each class are:

(array(['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog',
       'horse', 'ship', 'truck'], dtype='&lt;U10'), array([1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000],
      dtype=int64))</code></pre>
</div>
</div>
</div>
</div>
</div>
<p>TODO lables are strings, need to be numeric for predict; here or ep
05?</p>
<p>There are other preprocessing steps you might need to take for your
particular problem. We will discuss a few common ones briefly before
getting back to our model.</p>
<div class="section level3">
<h3 id="data-splitting">Data Splitting<a class="anchor" aria-label="anchor" href="#data-splitting"></a>
</h3>
<p>In the previous episode we saw that the keras installation includes
the Cifar-10 dataset and that by using the ‘cifar10.load_data()’ method
the returned data is split into two (train and validations sets). There
was not a test dataset which is why we just created one.</p>
<p>When using a different dataset, or loading your own set of images,
you may need to do the splitting yourself.</p>
<div id="callout5" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout5"></a>
</h3>
<div class="callout-content">
<p>ChatGPT</p>
<p>Data Splitting Techniques</p>
<p>Data is typically split into the training, validation, and test data
sets using a process called data splitting or data partitioning. There
are various methods to perform this split, and the choice of technique
depends on the specific problem, dataset size, and the nature of the
data. Here are some common approaches:</p>
<p><strong>Hold-Out Method:</strong></p>
<ul>
<li><p>In the hold-out method, the dataset is divided into two parts
initially: a training set and a test set.</p></li>
<li><p>The training set is used to train the model, and the test set is
kept completely separate to evaluate the model’s final
performance.</p></li>
<li><p>This method is straightforward and widely used when the dataset
is sufficiently large.</p></li>
</ul>
<p><strong>Train-Validation-Test Split:</strong></p>
<ul>
<li><p>The dataset is split into three parts: the training set, the
validation set, and the test set.</p></li>
<li><p>The training set is used to train the model, the validation set
is used to tune hyperparameters and prevent overfitting during training,
and the test set is used to assess the final model performance.</p></li>
<li><p>This method is commonly used when fine-tuning model
hyperparameters is necessary.</p></li>
</ul>
<p><strong>K-Fold Cross-Validation:</strong></p>
<ul>
<li><p>In k-fold cross-validation, the dataset is divided into k subsets
(folds) of roughly equal size.</p></li>
<li><p>The model is trained and evaluated k times, each time using a
different fold as the test set while the remaining k-1 folds are used as
the training set.</p></li>
<li><p>The final performance metric is calculated as the average of the
k evaluation results, providing a more robust estimate of model
performance.</p></li>
<li><p>This method is particularly useful when the dataset size is
limited, and it helps in better utilizing available data.</p></li>
</ul>
<p><strong>Stratified Sampling:</strong></p>
<ul>
<li><p>Stratified sampling is used when the dataset is imbalanced,
meaning some classes or categories are underrepresented.</p></li>
<li><p>The data is split in such a way that each subset (training,
validation, or test) maintains the same class distribution as the
original dataset.</p></li>
<li><p>This ensures that all classes are well-represented in each
subset, which is important to avoid biased model evaluation.</p></li>
</ul>
<p>It’s important to note that the exact split ratios (e.g., 80-10-10 or
70-15-15) may vary depending on the problem, dataset size, and specific
requirements. Additionally, data splitting should be performed randomly
to avoid introducing any biases into the model training and evaluation
process.</p>
</div>
</div>
</div>
<div id="challenge2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge2"></a>
</h3>
<div class="callout-content">
<p>Data Splitting Example</p>
<p>To split a dataset into training and test sets there is a very
convenient function from sklearn called <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" class="external-link">train_test_split</a>.</p>
<p><code>train_test_split</code> takes a number of parameters:</p>
<p><code>sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)</code></p>
<p>Take a look at the help and write a function to split an imaginary
dataset into a train/test split of 80/20 using stratified sampling.</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">
  Show me the solution
  </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p>Noting there are a couple ways to do this, here is one example:</p>
<pre><code>from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(image_dataset, target, test_size=0.2, random_state=42, shuffle=True, stratify=target)</code></pre>
<ul>
<li>The first two parameters are the dataset (X) and the corresponding
targets (y) (i.e. class labels)</li>
<li>Next is the named parameter <code>test_size</code> this is the
fraction of the dataset that is used for testing, in this case
<code>0.2</code> means 20% of the data will be used for testing.</li>
<li>
<code>random_state</code> controls the shuffling of the dataset,
setting this value will reproduce the same results (assuming you give
the same integer) every time it is called.</li>
<li>
<code>shuffle</code> which can be either <code>True</code> or
<code>False</code>, it controls whether the order of the rows of the
dataset is shuffled before splitting. It defaults to
<code>True</code>.</li>
<li>
<code>stratify</code> is a more advanced parameter that controls how
the split is done. By setting it to <code>target</code> the train and
test sets the function will return will have roughly the same
proportions (with regards to the number of images of a certain class) as
the dataset.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="image-colours">Image Colours<a class="anchor" aria-label="anchor" href="#image-colours"></a>
</h3>
<p><strong>RGB</strong> Images:</p>
<ul>
<li><p>For image classification tasks, RGB images are used because they
capture the full spectrum of colors that human vision can perceive,
allowing the model to learn intricate features and patterns present in
the images.</p></li>
<li><p>RGB (Red, Green, Blue) images have three color channels: red,
green, and blue, with each channel having an intensity value that ranges
from 0 to 255. Each channel represents the intensity of the
corresponding color for each pixel. This results in a 3D array, where
the dimensions are height, width, and color channel.</p></li>
</ul>
<p>While RGB is the most common representation, there are scenarios
where other color palettes might be considered, such as:</p>
<p><strong>Grayscale</strong> Images:</p>
<ul>
<li><p>Grayscale images have only one channel, representing the
intensity of the pixels. Each pixel’s intensity is usually represented
by a single numerical value that ranges from 0 (black) to 255 (white).
The image is essentially a 2D array where each element holds the
intensity value of the corresponding pixel.</p></li>
<li><p>In cases where color information isn’t critical, you might
convert RGB images to grayscale to reduce the computational
load.</p></li>
</ul>
</div>
<div class="section level3">
<h3 id="one-hot-encoding">One-hot encoding<a class="anchor" aria-label="anchor" href="#one-hot-encoding"></a>
</h3>
<p>A neural network can only take numerical inputs and outputs, and
learns by calculating how “far away” the class predicted by the neural
network is from the true class. When the target (label) is a categorical
data, or strings, it is very difficult to determine this “distance” or
error. Therefore we will transform this column into a more suitable
format. There are many ways to do this, however we will be using
<strong>one-hot encoding</strong>.</p>
<p>One-hot encoding is a technique to represent categorical data as
binary vectors, making it compatible with machine learning algorithms.
Each category becomes a separate feature, and the presence or absence of
a category is indicated by 1s and 0s in the respective columns.</p>
<p>Let’s say you have a dataset with a “Color” column containing three
categories: Red, Blue, Green.</p>
<p>Table 1. Original Data.</p>
<table class="table">
<thead><tr class="header">
<th>color</th>
<th align="right"></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>red</td>
<td align="right"><span class="emoji" data-emoji="red_square">🟥</span></td>
</tr>
<tr class="even">
<td>green</td>
<td align="right"><span class="emoji" data-emoji="green_square">🟩</span></td>
</tr>
<tr class="odd">
<td>blue</td>
<td align="right"><span class="emoji" data-emoji="blue_square">🟦</span></td>
</tr>
<tr class="even">
<td>red</td>
<td align="right"><span class="emoji" data-emoji="red_square">🟥</span></td>
</tr>
</tbody>
</table>
<p>Table 2. After One-Hot Encoding.</p>
<table class="table">
<thead><tr class="header">
<th>Color_red</th>
<th align="center">Color_blue</th>
<th align="right">Color_green</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>1</td>
<td align="center">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td>0</td>
<td align="center">1</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td>0</td>
<td align="center">0</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td>1</td>
<td align="center">0</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p>Each category has its own binary column, and the value is set to 1 in
the corresponding column for each row that matches that category.</p>
<p>Our image data is not one-hot encoded because image data is
continuous and typically represented as arrays of pixel values. While in
some cases you may want to one-hot encode the labels, here we are using
integer labels (0,1,2) instead. Many machine learning libraries and
frameworks handle this label encoding internally.</p>
</div>
<div class="section level3">
<h3 id="image-augmentation">Image augmentation<a class="anchor" aria-label="anchor" href="#image-augmentation"></a>
</h3>
<p>There are several ways to augment your data to increase the diversity
of the training data and improve model robustness.</p>
<ul>
<li>Geometric Transformations
<ul>
<li>rotation, translation, scaling, zooming, cropping</li>
</ul>
</li>
<li>Flipping or Mirroring
<ul>
<li>some classes, like horse, have a different shape when facing left or
right and you want your model to recognize both</li>
</ul>
</li>
<li>Color properties
<ul>
<li>brightness, contrast, or hue</li>
<li>these changes simulate variations in lighting conditions</li>
</ul>
</li>
</ul>
<div class="section level4">
<h4 id="finally">Finally!<a class="anchor" aria-label="anchor" href="#finally"></a>
</h4>
<p>Our test dataset is ready to go and we can move on to how to build an
architecture.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Keypoints<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Image datasets can be found online or created uniquely for your
research question</li>
<li>Images consist of pixels arranged in a particular order</li>
<li>Image data is usually preprocessed before use in a CNN for
efficiency, consistency, and robustness</li>
</ul>
</div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</div>
</div>
</section></section><section id="aio-03-build-cnn"><p>Content from <a href="03-build-cnn.html">Build a Convolutional Neural Network</a></p>
<hr>
<p> Last updated on 2023-10-11 | 
        
        <a href="https://https://github.com/erinmgraham/icwithcnn/edit/main/episodes/03-build-cnn.html" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time <i aria-hidden="true" data-feather="clock"></i> 12 minutes </p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right"> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is a (artificial) neural network (ANN)?</li>
<li>How is a convolutional neural network (CNN) different from an
ANN?</li>
<li>What are the types of layers used to build a CNN?</li>
<li>How do you monitor the training process?</li>
<li>What is underfitting?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understand how a convolutional neural network (CNN) differs from an
artificial neural network (ANN)</li>
<li>Explain the terms: kernel, filter</li>
<li>Know the different layers: dense, convolutional, pooling,
flatten</li>
<li>Explain what underfitting is; how you detect underfitting; and ways
to address underfitting</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section id="neural-networks"><h2 class="section-heading">Neural Networks<a class="anchor" aria-label="anchor" href="#neural-networks"></a>
</h2>
<hr class="half-width">
<p>A neural network is an artificial intelligence technique loosely
based on the way neurons in the brain work. A neural network consists of
connected computational units called neurons. Each neuron …</p>
<ul>
<li>has one or more inputs, e.g. input data expressed as floating point
numbers</li>
<li>most of the time, each neuron conducts 3 main operations:
<ul>
<li>take the weighted sum of the inputs</li>
<li>add an extra constant weight (i.e. a bias term) to this weighted
sum</li>
<li>apply a non-linear function to the output so far (using a predefined
activation function)</li>
</ul>
</li>
<li>return one output value, again a floating point number</li>
</ul>
<figure><img src="../fig/03_neuron.png" alt="" class="figure mx-auto d-block"></figure><p>Multiple neurons can be joined together by connecting the output of
one to the input of another. These connections are associated with
weights that determine the ‘strength’ of the connection, the weights are
adjusted during training. In this way, the combination of neurons and
connections describe a computational graph, an example can be seen in
the image below. In most neural networks neurons are aggregated into
layers. Signals travel from the input layer to the output layer,
possibly through one or more intermediate layers called hidden layers.
The image below shows an example of a neural network with three layers,
each circle is a neuron, each line is an edge and the arrows indicate
the direction data moves in.</p>
<figure><img src="../fig/03_neural_net.png" alt="" class="figure mx-auto d-block"><figcaption>The image above is by Glosser.ca, <a href="https://creativecommons.org/licenses/by-sa/3.0" class="external-link">CC BY-SA 3.0</a>,
via Wikimedia Commons, <a href="https://commons.wikimedia.org/wiki/File:Colored_neural_network.svg" class="external-link">original
source</a></figcaption></figure><p>Neural networks aren’t a new technique, they have been around since
the late 1940s. But until around 2010 neural networks tended to be quite
small, consisting of only 10s or perhaps 100s of neurons. This limited
them to only solving quite basic problems. Around 2010 improvements in
computing power and the algorithms for training the networks made much
larger and more powerful networks practical. These are known as deep
neural networks or Deep Learning</p>
<div id="callout1" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout1"></a>
</h3>
<div class="callout-content">
<p>Concept: Why deep learning is possible and what infrastructure is
best suited to deep learning Systems with high quality GPUs and/or HPCs
if available. [Comment: I feel this is important to note, in order to
make it clear that anyone attempting to run neural networks on a
standard laptop will quickly reach the upper limit of capacity. By
setting this expectation clearly in the course, it could help prevent
people from trying to do everything neural net related on their machines
and becoming disenfranchise with ML as a result]</p>
</div>
</div>
</div>
</section><section id="convolutional-neural-networks"><h2 class="section-heading">Convolutional Neural Networks<a class="anchor" aria-label="anchor" href="#convolutional-neural-networks"></a>
</h2>
<hr class="half-width">
<p>A convolutional neural network (CNN) is a type of artificial neural
network (ANN) that is most commonly applied to analyze visual imagery.
They are designed to recognize the spatial structure of images when
extracting features.</p>
<div class="section level3">
<h3 id="step-4--build-an-architecture-from-scratch-or-choose-a-pretrained-model">Step 4. Build an architecture from scratch or choose a pretrained
model<a class="anchor" aria-label="anchor" href="#step-4--build-an-architecture-from-scratch-or-choose-a-pretrained-model"></a>
</h3>
<p>Now we will build a neural network from scratch, and although this
sounds like a daunting task, with Keras it is actually surprisingly
straightforward. With Keras you compose a neural network by creating
layers and linking them together.</p>
<p>Let’s look at our network from the introduction:</p>
<pre><code><span><span class="co"># # CNN Part 1</span></span>
<span><span class="co"># # Input layer of 32x32 images with three channels (RGB)</span></span>
<span><span class="co"># inputs_intro = keras.Input(shape=train_images.shape[1:])</span></span>
<span></span>
<span><span class="co"># # CNN Part 2</span></span>
<span><span class="co"># # Convolutional layer with 50 filters, 3x3 kernel size, and ReLU activation</span></span>
<span><span class="co"># x_intro = keras.layers.Conv2D(50, (3, 3), activation='relu')(inputs_intro)</span></span>
<span><span class="co"># # Second Convolutional layer</span></span>
<span><span class="co"># x_intro = keras.layers.Conv2D(50, (3, 3), activation='relu')(x_intro)</span></span>
<span><span class="co"># # Flatten layer to convert 2D feature maps into a 1D vector</span></span>
<span><span class="co"># x_intro = keras.layers.Flatten()(x_intro)</span></span>
<span></span>
<span><span class="co"># # CNN Part 3</span></span>
<span><span class="co"># # Output layer with 10 units (one for each class)</span></span>
<span><span class="co"># outputs_intro = keras.layers.Dense(10, activation='softmax')(x_intro)</span></span></code></pre>
</div>
<div class="section level3">
<h3 id="parts-of-a-neural-network">Parts of a neural network<a class="anchor" aria-label="anchor" href="#parts-of-a-neural-network"></a>
</h3>
<p>Here we can see there are three main components of a neural
network:</p>
<ul>
<li>CNN Part 1. Input Layer</li>
<li>CNN Part 2. Hidden Layers</li>
<li>CNN Part 3. Output Layer</li>
</ul>
<div class="section level4">
<h4 id="cnn-part-1--input-layer">CNN Part 1. Input Layer<a class="anchor" aria-label="anchor" href="#cnn-part-1--input-layer"></a>
</h4>
<p>The Input in Keras gets special treatment when images are used. Keras
automatically calculates the number of inputs and outputs a specific
layer needs and therefore how many edges need to be created. This means
we need to let Keras know how big our input is going to be. We do this
by instantiating a <code>keras.Input</code> class and pass it a tuple
that indicates the dimensionality of the input data.</p>
<p>In our case, the shape of an image is defined by its pixel dimensions
and number of channels:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># recall the shape of the images in our dataset</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(train_images.shape)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(50000, 32, 32, 3) # number of images, image width in pixels, image height in pixels, number of channels (RGB)</code></pre>
</div>
<pre><code><span><span class="co"># Input layer of 32x32 images with three channels (RGB)</span></span>
<span><span class="co">#inputs_intro = keras.Input(shape=train_images.shape[1:])</span></span></code></pre>
</div>
<div class="section level4">
<h4 id="cnn-part-2--hidden-layers">CNN Part 2. Hidden Layers<a class="anchor" aria-label="anchor" href="#cnn-part-2--hidden-layers"></a>
</h4>
<p>The next component consists of the so-called hidden layers of the
network. The reason they are referred to as hidden is because the true
values of their nodes are unknown - this is the black box.</p>
<p>In a CNN, the hidden layers typically consist of dense,
convolutional, reshaping (e.g., Flatten), and pooling layers. Check out
the <a href="https://keras.io/api/layers/" class="external-link">Layers API</a> section of the
Keras documentation.</p>
<div class="section level5">
<h5 id="dense-layers">
<strong>Dense layers</strong><a class="anchor" aria-label="anchor" href="#dense-layers"></a>
</h5>
<p>A <strong>dense</strong> layer has a number of neurons, which is a
parameter you can choose when you create the layer. When connecting the
layer to its input and output layers every neuron in the dense layer
gets an edge (i.e. connection) to <strong>all</strong> of the input
neurons and <strong>all</strong> of the output neurons.</p>
<ul>
<li>
<strong>Dense</strong>: Just your regular densely-connected NN
layer</li>
<li>defined by the keras.layers.Dense class</li>
</ul>
<figure><img src="../fig/03-neural_network_sketch_dense.png" alt="" class="figure mx-auto d-block"></figure><p>This layer is called fully connected, because all input neurons are
taken into account by each output neuron. The number of parameters that
need to be learned by the network is thus in the order of magnitude of
the number of input neurons times the number of hidden neurons.</p>
</div>
<div class="section level5">
<h5 id="convolutional-layers">
<strong>Convolutional Layers</strong><a class="anchor" aria-label="anchor" href="#convolutional-layers"></a>
</h5>
<p>A <strong>convolutional</strong> layer transforms the input image in
order to extract features from it.</p>
<ul>
<li>
<strong>Conv2D</strong>: 2D convolution layer (e.g. spatial
convolution over images)</li>
<li>defined by the keras.layers.Conv2D class</li>
</ul>
<p>With image classification, note that our input dimension is now quite
high (even with small pictures of 32x32 pixels), we have:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> train_images.shape[<span class="dv">1</span>] <span class="op">*</span> train_images.shape[<span class="dv">2</span>] <span class="op">*</span> train_images.shape[<span class="dv">3</span>]</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dim)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="fl">3072</span></span></code></pre>
</div>
<div id="challenge1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge1"></a>
</h3>
<div class="callout-content">
<p>Number of parameters</p>
<p>Suppose we create a single Dense (fully connected) layer with 100
hidden units that connect to the input pixels, how many parameters does
this layer have?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">
  Show me the solution
  </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>Each entry of the input dimensions, i.e. the shape of one single data
point, is connected with 100 neurons of our hidden layer, and each of
these neurons has a bias term associated to it. So we have 307300
parameters to learn.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>width, height <span class="op">=</span> (<span class="dv">32</span>, <span class="dv">32</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>n_hidden_neurons <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>n_bias <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>n_input_items <span class="op">=</span> width <span class="op">*</span> height <span class="op">*</span> <span class="dv">3</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>n_parameters <span class="op">=</span> (n_input_items <span class="op">*</span> n_hidden_neurons) <span class="op">+</span> n_bias</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(n_parameters)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="fl">307300</span></span></code></pre>
</div>
<p>We can also check this by building the layer in Keras:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>inputs_ex <span class="op">=</span> keras.Input(shape<span class="op">=</span>dim)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>outputs_ex <span class="op">=</span> keras.layers.Dense(<span class="dv">100</span>)(inputs_ex)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>model_ex <span class="op">=</span> keras.models.Model(inputs<span class="op">=</span>inputs_ex, outputs<span class="op">=</span>outputs_ex)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>model_ex.summary()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         [(None, 3072)]            0
_________________________________________________________________
dense (Dense)                (None, 100)               307300
=================================================================
Total params: 307,300
Trainable params: 307,300
Non-trainable params: 0</code></pre>
</div>
</div>
</div>
</div>
</div>
<p>We can decrease the number of units in our hidden layer, but this
also decreases the number of patterns our network can remember.
Moreover, if we increase the image size, the number of weights will
‘explode’, even though the task of recognizing large images is not
necessarily more difficult than the task of recognizing small
images.</p>
<p>The solution is that we make the network learn in a ‘smart’ way. The
features that we learn should be similar both for small and large
images, and similar features (e.g. edges, corners) can appear anywhere
in the image (in mathematical terms: translation invariant). We do this
by making use of a concepts from image processing that precede Deep
Learning.</p>
<p>A <strong>convolution matrix</strong>, or <strong>kernel</strong>, is
a matrix transformation that we ‘slide’ over the image to calculate
features at each position of the image. For each pixel, we calculate the
matrix product between the kernel and the pixel with its surroundings. A
kernel is typically small, between 3x3 and 7x7 pixels. We can for
example think of the 3x3 kernel:</p>
<pre><code>[[-1, -1, -1],
 [0,   0,  0]
 [1,   1,  1]]</code></pre>
<p>This kernel will give a high value to a pixel if it is on a
horizontal border between dark and light areas. Note that for RGB
images, the kernel should also have a depth of 3.</p>
<p>In the following image, we see the effect of such a kernel on the
values of a single-channel image. The red cell in the output matrix is
the result of multiplying and summing the values of the red square in
the input, and the kernel. Applying this kernel to a real image shows
that it indeed detects horizontal edges.</p>
<figure><img src="../fig/03_conv_matrix.png" alt="" class="figure mx-auto d-block"></figure><figure><img src="../fig/03_conv_image.png" alt="" class="figure mx-auto d-block"></figure><p>In our convolutional layer our hidden units are a number of
convolutional matrices (or kernels), where the values of the matrices
are the weights that we learn in the training process. The output of a
convolutional layer is an ‘image’ for each of the kernels, that gives
the output of the kernel applied to each pixel.</p>
<div id="playing-with-convolutions" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="playing-with-convolutions" class="callout-inner">
<h3 class="callout-title">Playing with convolutions<a class="anchor" aria-label="anchor" href="#playing-with-convolutions"></a>
</h3>
<div class="callout-content">
<p>Convolutions applied to images can be hard to grasp at first.
Fortunately, there are resources out there that enable users to
interactively play around with images and convolutions:</p>
<ul>
<li><p><a href="https://setosa.io/ev/image-kernels/" class="external-link">Image kernels
explained</a> shows how different convolutions can achieve certain
effects on an image, like sharpening and blurring.</p></li>
<li><p>The <a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks" class="external-link">convolutional
neural network cheat sheet</a> shows animated examples of the different
components of convolutional neural nets.</p></li>
</ul>
</div>
</div>
</div>
<div id="challenge2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge2"></a>
</h3>
<div class="callout-content">
<p>Border pixels</p>
<p>What do you think happens to the border pixels when applying a
convolution?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">
  Show me the solution
  </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>There are different ways of dealing with border pixels. You can
ignore them, which means that your output image is slightly smaller then
your input. It is also possible to ‘pad’ the borders, e.g. with the same
value or with zeros, so that the convolution can also be applied to the
border pixels. In that case, the output image will have the same size as
the input image.</p>
</div>
</div>
</div>
</div>
<div id="challenge3" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge3"></a>
</h3>
<div class="callout-content">
<p>Number of model parameters</p>
<p>Suppose we apply a convolutional layer with 100 kernels of size 3 * 3
* 3 (the last dimension applies to the rgb channels) to our images of 32
* 32 * 3 pixels. How many parameters do we have? Assume, for simplicity,
that the kernels do not use bias terms. Compare this to the answer of
the previous ‘Number of parameters’ challenge.</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">
  Show me the solution
  </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<p>We have 100 matrices with 3 * 3 * 3 = 27 values each so that gives 27
* 100 = 2700 weights. This is a magnitude of 100 less than the fully
connected layer with 100 units! Nevertheless, as we will see,
convolutional networks work very well for image data. This illustrates
the expressiveness of convolutional layers.</p>
</div>
</div>
</div>
</div>
</div>
<div class="section level5">
<h5 id="shaping-layers-flatten">
<strong>Shaping Layers: Flatten</strong><a class="anchor" aria-label="anchor" href="#shaping-layers-flatten"></a>
</h5>
<p>The third type of hidden layer used in our introductory model is a
<strong>Flatten</strong> layer. Let’s hold off on discussion this for
just a moment.</p>
</div>
</div>
<div class="section level4">
<h4 id="cnn-part-3--output-layer">CNN Part 3. Output Layer<a class="anchor" aria-label="anchor" href="#cnn-part-3--output-layer"></a>
</h4>
<p>Recall for the outputs we will need to look at what we want to
identify from the data. If we are performing a classification problem
then typically we will have one output for each potential class. We need
to finish with a Dense layer to connect the output cells of the
convolutional layer to the outputs for our 10 classes.</p>
<pre><code><span><span class="co"># Output layer with 10 units (one for each class)</span></span>
<span><span class="co">#outputs = keras.layers.Dense(10)(x)</span></span></code></pre>
</div>
</div>
</section><section id="putting-it-all-together"><h2 class="section-heading">Putting it all together<a class="anchor" aria-label="anchor" href="#putting-it-all-together"></a>
</h2>
<hr class="half-width">
<p>So let us look again at the small network used in our
introduction:</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Input layer of 32x32 images with three channels (RGB)</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>inputs_intro <span class="op">=</span> keras.Input(shape<span class="op">=</span>train_images.shape[<span class="dv">1</span>:])</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Convolutional layer with 50 filters, 3x3 kernel size, and ReLU activation</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>x_intro <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(inputs_intro)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Second Convolutional layer with 50 filters, 3x3 kernel size, and ReLU activation</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>x_intro <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x_intro)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Flatten layer to convert 2D feature maps into a 1D vector</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>x_intro <span class="op">=</span> keras.layers.Flatten()(x_intro)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Output layer with 10 units (one for each class)</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>outputs_intro <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>)(x_intro)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="co"># create the model</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>model_intro <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs_intro, outputs<span class="op">=</span>outputs_intro, name<span class="op">=</span><span class="st">"cifar_model_intro"</span>)</span></code></pre>
</div>
<p>We first store a reference to the input class in a variable
‘inputs_intro’ so we can pass it to the creation of our first hidden
layer. Creating the convolutional layers can then be done as
follows:</p>
<pre><code><span><span class="co">#x_intro = keras.layers.Conv2D(50, (3, 3), activation='relu')(inputs_intro)</span></span></code></pre>
<p>The instantiation here has 3 parameters and a seemingly strange
combination of parentheses, so let us take a closer look.</p>
<ul>
<li><p>The first parameter 50 is the number of neurons we want in this
layer, this is one of the hyperparameters of our system and needs to be
chosen carefully.</p></li>
<li><p>The second parameter is the kernel size.</p></li>
<li><p>The third parameter is the activation function to use; here we
choose <strong>relu</strong> which is 0 for inputs that are 0 and below
and the identity function (returning the same value) for inputs above 0.
This is a commonly used activation function in deep neural networks that
is proven to work well. We will discuss activation functions later in
<strong>Step 9. Tune hyperparameters</strong>.</p></li>
<li><p>Next we see an extra set of parenthenses with inputs in them,
this means that after creating an instance of the Conv2D layer we call
it as if it was a function. This tells the Conv2D layer to connect the
layer passed as a parameter, in this case the inputs.</p></li>
<li><p>Finally we store a reference so we can pass it to the next
layer.</p></li>
</ul>
<p>Adding a second Conv2D layer we use the same arguments but change the
input to be the output of the first Conv2D layer.</p>
<pre><code><span><span class="co">#x_intro = keras.layers.Conv2D(50, (3, 3), activation='relu')(x_intro)</span></span></code></pre>
<p>Now let’s take a closer look at that <strong>Flatten</strong>
layer:</p>
<div id="challenge4" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge4"></a>
</h3>
<div class="callout-content">
<p>Flatten</p>
<p>Inspect the network above:</p>
<ul>
<li>What do you think is the function of the Flatten layer?</li>
<li>Which layer has the most parameters? Do you find this
intuitive?</li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4">
  Show me the solution
  </h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" data-bs-parent="#accordionSolution4" aria-labelledby="headingSolution4">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>model_intro.summary()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Model: "cifar_model_intro"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 32, 32, 3)]       0         
                                                                 
 conv2d (Conv2D)             (None, 30, 30, 50)        1400      
                                                                 
 conv2d_1 (Conv2D)           (None, 28, 28, 50)        22550     
                                                                 
 flatten (Flatten)           (None, 39200)             0         
                                                                 
 dense_1 (Dense)             (None, 10)                392010    
                                                                 
=================================================================
Total params: 415,960
Trainable params: 415,960
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
<ul>
<li><p>The Flatten layer converts the 28x28x50 output of the
convolutional layer into a single one-dimensional vector, that can be
used as input for a dense layer.</p></li>
<li><p>The last dense layer has the most parameters. This layer connects
every single output ‘pixel’ from the convolutional layer to the 10
output classes. That results in a large number of connections, so a
large number of parameters. This undermines a bit the expressiveness of
the convolutional layers that have much fewer parameters.</p></li>
</ul>
</div>
</div>
</div>
</div>
<ul>
<li>
<strong>Flatten</strong>: Flattens the input. Does not affect the
batch size</li>
<li>defined by the keras.layers.Flatten class</li>
</ul></section><section id="we-have-a-model-now-what"><h2 class="section-heading">We have a model now what?<a class="anchor" aria-label="anchor" href="#we-have-a-model-now-what"></a>
</h2>
<hr class="half-width">
<p>This minimal CNN should be able to run with the CIFAR-10 dataset and
provide reasonable results for basic classification tasks. However, do
keep in mind that this model is relatively simple, and its performance
may not be as high as more complex architectures. The reason it’s called
deep learning is because in most cases, the more layers we have, ie, the
deeper and more sophisticated CNN architecture we use, the better the
performance.</p>
<p>How can we tell? We can look at a couple metrics during the training
process to detect whether our model is underfitting or overfitting. To
do that, we first need to continue with the next steps in our Deep
Learning workflow, <strong>Step 5. Choose a loss function and
optimizer</strong> and <strong>Step 6. Train model</strong>. We will go
into more details of these steps in the next episode, but for now we
just need to run this code to create and access the training
history:</p>
<pre><code>#model_intro.compile(optimizer = 'adam', loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),
     metrics = ['accuracy'])
#history_intro = model_intro.fit(train_images, train_labels, epochs=10, 
     validation_data=(val_images, val_labels))</code></pre>
<div class="section level4">
<h4 id="monitor-training-progress-aka-model-evaluation-during-training">Monitor Training Progress (aka Model Evaluation during
Training)<a class="anchor" aria-label="anchor" href="#monitor-training-progress-aka-model-evaluation-during-training"></a>
</h4>
<p>It can be very insightful to plot the training loss to see how the
training progresses.</p>
<p>Using seaborn we can plot the training process using the history:</p>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co"># convert the history to a dataframe for plotting </span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>history_intro_df <span class="op">=</span> pd.DataFrame.from_dict(history_intro.history)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the loss and accuracy from the training process</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">'cifar_model_intro'</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>sns.lineplot(ax<span class="op">=</span>axes[<span class="dv">0</span>], data<span class="op">=</span>history_intro_df[[<span class="st">'loss'</span>, <span class="st">'val_loss'</span>]])</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>sns.lineplot(ax<span class="op">=</span>axes[<span class="dv">1</span>], data<span class="op">=</span>history_intro_df[[<span class="st">'accuracy'</span>, <span class="st">'val_accuracy'</span>]])</span></code></pre>
</div>
<figure><img src="../fig/03_model_intro_accuracy_loss.png" alt="" class="figure mx-auto d-block"></figure><p>This plot can be used to identify whether the training is well
configured or whether there are problems that need to be addressed. When
your validation loss is decreasing, the model is underfit. Underfitting
occurs when the model is too simple or lacks the capacity to capture the
underlying patterns and relationships present in the data. As a result,
the model’s predictions are not accurate, and it fails to generalize
well to unseen data.</p>
<p>Key characteristics of an underfit model include:</p>
<ul>
<li><p>Low Validation Accuracy: This indicates that the model is not
learning from the data effectively.</p></li>
<li><p>Large Training Loss: The training loss (error) is high,
indicating that the model’s predictions are far from the true labels in
the training set.</p></li>
</ul>
<p>How to Address Underfitting:</p>
<ul>
<li>Increase the model’s complexity by adding more layers or units to
the existing layers.</li>
<li>Train the model for more epochs to give it more time to learn from
the data.</li>
<li>Perform data augmentation or feature engineering to provide the
model with more informative input features.</li>
</ul>
<p>Given we intentionally started with a shallow model, let’s try adding
more layers! Now is a good time to discuss the pooling layer.</p>
</div>
<div class="section level4">
<h4 id="pooling-layers">
<strong>Pooling Layers</strong><a class="anchor" aria-label="anchor" href="#pooling-layers"></a>
</h4>
<p>Often in convolutional neural networks, the convolutional layers are
intertwined with <strong>Pooling</strong> layers. As opposed to the
convolutional layer, the pooling layer actually alters the dimensions of
the image and reduces it by a scaling factor. It is basically decreasing
the resolution of your picture. The rationale behind this is that higher
layers of the network should focus on higher-level features of the
image. By introducing a pooling layer, the subsequent convolutional
layer has a broader ‘view’ on the original image.</p>
<ul>
<li>
<strong>MaxPooling2D</strong>: Max pooling operation for 2D spatial
data</li>
<li>defined by the keras.layers.MaxPooling2D class</li>
</ul>
<p>Let us create a new model that includes a pooling layer after each
Conv2D layer.</p>
<pre><code><span><span class="co"># pooling layer</span></span>
<span><span class="co">#x_pool = keras.layers.MaxPooling2D((2, 2))(x_pool)</span></span></code></pre>
<p>The instantiation here has a single parameter, pool_size.</p>
<ul>
<li><p>The function downsamples the input along its spatial dimensions
(height and width) by taking the maximum value over an input window (of
size defined by pool_size) for each channel of the input.</p></li>
<li><p>The resulting output, when using the default “valid” padding
option, has a spatial shape (number of rows or columns) of: <img src="../fig/03_shape_equation.png" alt="" class="figure"></p></li>
<li><p>And again we store a reference so we can pass it to the next
layer.</p></li>
</ul>
<p>Let us also add a second set of convolutional and pooling layers
before flattening the result and passing it to an additional dense
layer.</p>
<pre><code><span><span class="co"># dense layer</span></span>
<span><span class="co">#x_pool = keras.layers.Dense(50, activation='relu')(x_pool)</span></span></code></pre>
<p>Notice the instantiation of this Dense layer is slightly different
from the one used as our output layer. This one has 2 parameters, the
number of neurons and what is called the activation function. We will
look at these activation functions in a later episode.</p>
<p>We then add our final output layer and reassemble, compile, and train
the deeper model with pooling.</p>
<p>Putting it all together:</p>
<div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define the inputs, layers, and outputs of a cnn model with pooling</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="co"># CNN Part 1</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Input layer of 32x32 images with three channels (RGB)</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>inputs_pool <span class="op">=</span> keras.Input(shape<span class="op">=</span>train_images.shape[<span class="dv">1</span>:])</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="co"># CNN Part 2</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Convolutional layer with 50 filters, 3x3 kernel size, and ReLU activation</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>x_pool <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(inputs_pool)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Pooling layer with input window sized 2,2</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>x_pool <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x_pool)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Second Convolutional layer with 50 filters, 3x3 kernel size, and ReLU activation</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>x_pool <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x_pool)</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Second Pooling layer with input window sized 2,2</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>x_pool <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x_pool)</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Flatten layer to convert 2D feature maps into a 1D vector</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>x_pool <span class="op">=</span> keras.layers.Flatten()(x_pool)</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Dense layer</span></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>x_pool <span class="op">=</span> keras.layers.Dense(<span class="dv">50</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x_pool)</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a><span class="co"># CNN Part 3</span></span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Output layer with 10 units (one for each class)</span></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>outputs_pool <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>)(x_pool)</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a><span class="co"># create the pooling model</span></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>model_pool <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs_pool, outputs<span class="op">=</span>outputs_pool, name<span class="op">=</span><span class="st">"cifar_model_pool"</span>)</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>model_pool.<span class="bu">compile</span>(optimizer <span class="op">=</span> <span class="st">'adam'</span>, loss <span class="op">=</span> keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>), </span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>    metrics <span class="op">=</span> [<span class="st">'accuracy'</span>])</span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>history_pool <span class="op">=</span> model_pool.fit(train_images, train_labels, epochs<span class="op">=</span><span class="dv">10</span>, validation_data<span class="op">=</span>(val_images, val_labels))</span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>model_pool.summary()</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a><span class="co"># save pool model</span></span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a>model_pool.save(<span class="st">'fit_outputs/model_pool.h5'</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Model: "cifar_model_pool"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 32, 32, 3)]       0         
                                                                 
 conv2d (Conv2D)             (None, 30, 30, 50)        1400      
                                                                 
 max_pooling2d (MaxPooling2D  (None, 15, 15, 50)       0         
 )                                                               
                                                                 
 conv2d_1 (Conv2D)           (None, 13, 13, 50)        22550     
                                                                 
 max_pooling2d_1 (MaxPooling  (None, 6, 6, 50)         0         
 2D)                                                             
                                                                 
 flatten (Flatten)           (None, 1800)              0         
                                                                 
 dense (Dense)               (None, 50)                90050     
                                                                 
 dense_1 (Dense)             (None, 10)                510       
                                                                 
=================================================================
Total params: 114,510
Trainable params: 114,510
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
<div id="how-to-choose-an-architecture" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="how-to-choose-an-architecture" class="callout-inner">
<h3 class="callout-title">How to choose an architecture?<a class="anchor" aria-label="anchor" href="#how-to-choose-an-architecture"></a>
</h3>
<div class="callout-content">
<p>Even for this neural network, we had to make a choice on the number
of hidden neurons. Other choices to be made are the number of layers and
type of layers (as we will see later). You might wonder how you should
make these architectural choices. Unfortunately, there are no clear
rules to follow here, and it often boils down to a lot of trial and
error. However, it is recommended to look what others have done with
similar datasets and problems. Another best practice is to start with a
relatively simple architecture. Once running start to add layers and
tweak the network to see if performance increases.</p>
</div>
</div>
</div>
<div id="challenge5" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge5"></a>
</h3>
<div class="callout-content">
<p>Network depth</p>
<p>What, do you think, will be the effect of adding a convolutional
layer to your model? Will this model have more or fewer parameters? Try
it out. Create a model that has an additional Conv2d layer with 50
filters after the last MaxPooling2D layer. Train it for 20 epochs and
plot the results.</p>
<p><strong>HINT</strong>: The model definition that we used previously
needs to be adjusted as follows:</p>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>inputs_cnd <span class="op">=</span> keras.Input(shape<span class="op">=</span>train_images.shape[<span class="dv">1</span>:])</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>x_cnd <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(inputs_cnd)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>x_cnd <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x_cnd)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>x_cnd <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x_cnd)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>x_cnd <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x_cnd)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Add your extra layer here</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>x_cnd <span class="op">=</span> keras.layers.Flatten()(x_cnd)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>x_cnd <span class="op">=</span> keras.layers.Dense(<span class="dv">50</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x_cnd)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>outputs_cnd <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>)(x_cnd)</span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution5" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution5" aria-expanded="false" aria-controls="collapseSolution5">
  <h4 class="accordion-header" id="headingSolution5">
  Show me the solution
  </h4>
</button>
<div id="collapseSolution5" class="accordion-collapse collapse" data-bs-parent="#accordionSolution5" aria-labelledby="headingSolution5">
<div class="accordion-body">
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>inputs_cnd = keras.Input(shape=train_images.shape[1:])
x_cnd = keras.layers.Conv2D(50, (3, 3), activation='relu')(inputs_cnd)
x_cnd = keras.layers.MaxPooling2D((2, 2))(x_cnd)
x_cnd = keras.layers.Conv2D(50, (3, 3), activation='relu')(x_cnd)
x_cnd = keras.layers.MaxPooling2D((2, 2))(x_cnd)
x_cnd = keras.layers.Conv2D(50, (3, 3), activation='relu')(x_cnd)
x_cnd = keras.layers.Flatten()(x_cnd)
x_cnd = keras.layers.Dense(50, activation='relu')(x_cnd)
outputs_cnd = keras.layers.Dense(10)(x_cnd)

model_cnd = keras.Model(inputs=inputs_cnd, outputs=outputs_cnd, name="cifar_model_Challenge_network_depth")</code></pre>
</div>
</div>
</div>
</div>
</div>
<p>With the model defined above, we can inspect the number of
parameters:</p>
<div class="codewrapper sourceCode" id="cb26">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>model_cnd.summary()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Model: "cifar_model_Challenge_network_depth"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 input_7 (InputLayer)        [(None, 32, 32, 3)]       0

 conv2d_16 (Conv2D)          (None, 30, 30, 50)        1400

 max_pooling2d_10 (MaxPoolin  (None, 15, 15, 50)       0
 g2D)

 conv2d_17 (Conv2D)          (None, 13, 13, 50)        22550

 max_pooling2d_11 (MaxPoolin  (None, 6, 6, 50)         0
 g2D)

 conv2d_18 (Conv2D)          (None, 4, 4, 50)          22550

 flatten_6 (Flatten)         (None, 800)               0

 dense_11 (Dense)            (None, 50)                40050

 dense_12 (Dense)            (None, 10)                510

=================================================================
Total params: 87,060
Trainable params: 87,060
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
<p>The number of parameters has decreased by adding this layer. We can
see that the conv layer decreases the resolution from 6x6 to 4x4, as a
result, the input of the Dense layer is smaller than in the previous
network.</p>
<div id="other-types-of-data" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="other-types-of-data" class="callout-inner">
<h3 class="callout-title">Other types of data<a class="anchor" aria-label="anchor" href="#other-types-of-data"></a>
</h3>
<div class="callout-content">
<p>Convolutional and Pooling layers are also applicable to different
types of data than image data. Whenever the data is ordered in a
(spatial) dimension, and translation invariant features are expected to
be useful, convolutions can be used. Think for example of time series
data from an accelerometer, audio data for speech recognition, or 3d
structures of chemical compounds.</p>
</div>
</div>
</div>
<p>Adding more layers to our model should increase the accuracy of its
predictions. But before we look at the training metrics for our pooling
model, let’s take a step back and discuss in more detail <strong>Step 5.
Choose a loss function and optimizer</strong> and <strong>Step 6. Train
model</strong>.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Keypoints<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Artificial neural networks (ANN) are a machine learning technique
based on a model inspired by groups of neurons in the brain.</li>
<li>Convolution neural networks (CNN) are a type of ANN designed for
image classification and object detection</li>
<li>The filter size determines the size of the receptive field where
information is extracted and the kernel size changes the mathematical
structure</li>
<li>A CNN can consist of many types of layers including dense (fully
connected), convolutional, pooling, and flatten layers</li>
<li>Convolutional layers make efficient reuse of model parameters.</li>
<li>Pooling layers decrease the resolution of your input</li>
</ul>
</div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</div>
</section></section><section id="aio-04-fit-cnn"><p>Content from <a href="04-fit-cnn.html">Compile and Train a Convolutional Neural Network</a></p>
<hr>
<p> Last updated on 2023-10-12 | 
        
        <a href="https://https://github.com/erinmgraham/icwithcnn/edit/main/episodes/04-fit-cnn.html" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time <i aria-hidden="true" data-feather="clock"></i> 12 minutes </p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right"> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How do you compile a convolutional neural network (CNN)?</li>
<li>What is a loss function?</li>
<li>What is an optimizer?</li>
<li>How do you train (fit) a CNN?</li>
<li>What are hyperparameters?</li>
<li>How do you detect overfitting?</li>
<li>How do you avoid overfitting?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Explain the difference between compiling and training a CNN</li>
<li>Know how to select a loss function for your model</li>
<li>Understand what an optimizer is</li>
<li>Define the terms: learning rate, batch size, epoch</li>
<li>Explain overfitting</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="step-5--choose-a-loss-function-and-optimizer">Step 5. Choose a loss function and optimizer<a class="anchor" aria-label="anchor" href="#step-5--choose-a-loss-function-and-optimizer"></a>
</h3>
<p>We have designed a convolutional neural network (CNN) that in theory
we should be able to train to classify images.</p>
<p>We now need to select an appropriate optimizer and loss function that
we will use during training.</p>
<p>Recall how we compiled our model in the introduction:</p>
<pre><code><span><span class="co"># compile the pooling model</span></span>
<span><span class="co">#model_pool.compile(optimizer = 'adam', </span></span>
<span><span class="co">#              loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),               </span></span>
<span><span class="co">#              metrics = ['accuracy'])</span></span></code></pre>
<div class="section level4">
<h4 id="loss-function">Loss function<a class="anchor" aria-label="anchor" href="#loss-function"></a>
</h4>
<p>The <strong>loss function</strong> tells the training algorithm how
wrong, or how ‘far away’ from the true value the predicted value is. The
purpose of loss functions is to compute the quantity that a model should
seek to minimize during training. Which class of loss functions you
choose depends on your task.</p>
<p><strong>Loss for classification</strong></p>
<p>For classification purposes, there are a number of probabilistic
losses to choose from. We chose
<code>SparseCategoricalCrossentropy</code> because we want to compute
the crossentropy loss between our class labels represented by integers
(i.e., not one-hot encoded) and the model predictions.</p>
<ul>
<li>This loss function is appropriate to use when the data has two or
more label classes.</li>
<li>defined by the keras.losses.SparseCategoricalCrossentropy class</li>
</ul>
<div id="callout1" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout1"></a>
</h3>
<div class="callout-content">
<p>ChatGPT</p>
<p>Logits</p>
<p><strong>Logits</strong> are the raw, unnormalized predictions that a
model generates before they are transformed into probabilities. In many
cases, the final layer of a neural network produces logits, and these
logits are then passed through a function (usually the softmax function)
to obtain probabilities for each class. The softmax function converts
the logits into a probability distribution, where the values for each
class range between 0 and 1 and sum up to 1.</p>
<p>Here’s an example to illustrate this:</p>
<p>Suppose you have a neural network that performs image classification
with three classes: “Cat,” “Dog,” and “Bird.” After feeding an image
through the model, it generates logits as follows:</p>
<p>Logits: [2.5, 1.2, -0.8]</p>
<p>Now, to turn these logits into probabilities, we apply the softmax
function:</p>
<p>Probabilities: [0.755, 0.223, 0.022]</p>
<p>The highest probability (0.755) corresponds to the “Cat” class,
suggesting that the model predicts the image contains a cat with a
probability of approximately 75.5%.</p>
<p>Now, let’s bring in the ‘from_logits’ argument in the context of
defining the loss function. When training a neural network for
multi-class classification tasks, the most common loss function is the
categorical cross-entropy. This loss function takes in probabilities and
the true labels of the data to compute the difference between the
predicted probabilities and the ground truth.</p>
<p>However, sometimes, the model may not produce probabilities directly
but instead produces logits. In such cases, you have two options:</p>
<p>Option 1. Use the ‘from_logits=False’ (default): In this case, Keras
will apply the softmax function to the logits internally before
computing the loss function. It means that you provide the logits, but
Keras handles the transformation to probabilities before calculating the
loss.</p>
<p>Option 2. Use ‘from_logits=True’: If you set this argument to True,
you are telling Keras that the provided values are already logits and no
further transformation is needed. The loss function will directly use
the provided logits to compute the loss, avoiding the internal
application of the softmax function.</p>
<p>In summary, ‘from_logits=True’ is useful when your model outputs
logits, and you want to manually handle the conversion to probabilities
(e.g., when using the logits for other purposes), or if you want to
avoid the additional computational overhead of applying the softmax
function twice (once by you and once by the loss function). On the other
hand, if you provide probabilities or set ‘from_logits=False’, Keras
will handle the conversion to probabilities internally before computing
the loss.</p>
</div>
</div>
</div>
<p><strong>Loss for Regression</strong></p>
<p>For regression tasks, we might want to stipulate that the predicted
numerical values are as close as possible to the true values. This is
commonly done by using the <strong>mean squared error</strong> (mse) or
the <strong>mean absolute error</strong> (mae) loss funtions, both of
which should work. Often, mse is preferred over mae because it
“punishes” large prediction errors more severely.</p>
<ul>
<li>defined by the keras.losses.MeanSquaredError class</li>
</ul>
<p>To compile a model with mse, change the loss argument of the
<code>compile</code> method:</p>
<pre><code><span><span class="co">#model_ex.compile(loss = 'mse')</span></span></code></pre>
<p>For more information on these and other available loss functions in
Keras you can check the <a href="https://keras.io/api/losses/" class="external-link">loss
documentation</a>.</p>
</div>
<div class="section level4">
<h4 id="optimizer">Optimizer<a class="anchor" aria-label="anchor" href="#optimizer"></a>
</h4>
<p>Somewhat coupled to the loss function is the optimizer. The optimizer
here refers to the algorithm with which the model learns to optimize on
the provided loss function.</p>
<p>We need to choose which optimizer to use and, if this optimizer has
parameters, what values to use for those. Furthermore, we need to
specify how many times to show the training samples to the optimizer. In
other words, the optimizer is responsible for taking the output of the
loss function and then applying some changes to the weights within the
network. It is through this process that the “learning” (adjustment of
the weights) is achieved.</p>
<pre><code><span><span class="co"># compile the pooling model</span></span>
<span><span class="co">#model_pool.compile(optimizer = 'adam', </span></span>
<span><span class="co">#              loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),               </span></span>
<span><span class="co">#              metrics = ['accuracy'])</span></span></code></pre>
<p><strong>Adam</strong></p>
<p>Here we picked one of the most common optimizers that works well for
most tasks, the <strong>Adam</strong> optimizer. Similar to activation
functions, the choice of optimizer depends on the problem you are trying
to solve, your model architecture and your data. Adam is a good starting
point though, which is why we chose it. Adam has a number of parameters,
but the default values work well for most problems so we will use it
with its default parameters.</p>
<ul>
<li>defined by the keras.optimizers.Adam class</li>
<li>takes a single parameter <code>learning_rate=0.01</code>
</li>
</ul>
<p>There are many optimizers to choose from so check the <a href="https://keras.io/api/optimizers/" class="external-link">optimizer documentation</a>. A
couple more popular or famous ones include:</p>
<ul>
<li><p><strong>Stochastic Gradient Descent (sgd)</strong>: Stochastic
Gradient Descent (SGD) is one of the fundamental optimization algorithms
used to train machine learning models, especially neural networks. It is
a variant of the gradient descent algorithm, designed to handle large
datasets efficiently.</p></li>
<li>
<p><strong>Root Mean Square (rms)prop</strong>: RMSprop is widely
used in various deep learning frameworks and is one of the predecessors
of more advanced optimizers like Adam, which further refines the concept
of adaptive learning rates. It is an extension of the basic Stochastic
Gradient Descent (SGD) algorithm and addresses some of the challenges of
SGD.</p>
<ul>
<li>For example, one of the main issues with the basic SGD is that it
uses a fixed learning rate for all model parameters throughout the
training process. This fixed learning rate can lead to slow convergence
or divergence (over-shooting) in some cases. RMSprop introduces an
adaptive learning rate mechanism to address this problem.</li>
</ul>
</li>
</ul>
<div id="callout2" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout2"></a>
</h3>
<div class="callout-content">
<p>ChatGPT</p>
<p><strong>Learning Rate</strong></p>
<p>Learning rate is a hyperparameter that determines the step size at
which the model’s parameters are updated during training. A higher
learning rate allows for more substantial parameter updates, which can
lead to faster convergence, but it may risk overshooting the optimal
solution. On the other hand, a lower learning rate leads to smaller
updates, providing more cautious convergence, but it may take longer to
reach the optimal solution. Finding an appropriate learning rate is
crucial for effectively training machine learning models.</p>
<p>In the figure below, we can see that a small learning rate will not
traverse toward the minima of the gradient descent algorithm in a timely
manner i.e. number of epochs.</p>
<figure><img src="https://developers.google.com/static/machine-learning/crash-course/images/LearningRateTooSmall.svg" title="Small learning rate leads to inefficient approach to loss minima" alt="" class="figure mx-auto d-block"><figcaption>Small learning rate leads to inefficient approach to
loss minima</figcaption></figure><p>(This image was obtained from <a href="https://developers.google.com/machine-learning/crash-course/reducing-loss/learning-rate" class="external-link">Google
Developers Machine Learning Crash Course</a> and is licenced under the
<a href="https://creativecommons.org/licenses/by/4.0/" class="external-link">Creative Commons
4.0 Attribution Licence</a>.)</p>
<p>On the other hand, specifying a learning rate that is <em>too
high</em> will result in a loss value that never approaches the minima.
That is, ‘bouncing between the sides’, thus never reaching a minima to
cease learning.</p>
<figure><img src="https://developers.google.com/static/machine-learning/crash-course/images/LearningRateTooLarge.svg" alt="" class="figure mx-auto d-block"><figcaption>A large learning rate results in overshooting the
gradient descent minima</figcaption></figure><p>(This image was obtained from <a href="https://developers.google.com/machine-learning/crash-course/reducing-loss/learning-rate" class="external-link">Google
Developers Machine Learning Crash Course</a> and is licenced under the
<a href="https://creativecommons.org/licenses/by/4.0/" class="external-link">Creative Commons
4.0 Attribution Licence</a>.)</p>
<p>Lastly, we can observe below that a modest learning rate will ensure
that the product of multiplying the scalar gradient value, and the
learning rate does not result in too small steps, nor a chaotic bounce
between sides of the gradient where steepness is greatest.</p>
<figure><img src="https://developers.google.com/static/machine-learning/crash-course/images/LearningRateJustRight.svg" alt="" class="figure mx-auto d-block"><figcaption>An optimal learning rate supports a gradual approach
to the minima</figcaption></figure><p>(This image was obtained from <a href="https://developers.google.com/machine-learning/crash-course/reducing-loss/learning-rate" class="external-link">Google
Developers Machine Learning Crash Course</a> and is licenced under the
<a href="https://creativecommons.org/licenses/by/4.0/" class="external-link">Creative Commons
4.0 Attribution Licence</a>.)</p>
</div>
</div>
</div>
</div>
<div class="section level4">
<h4 id="metrics">Metrics<a class="anchor" aria-label="anchor" href="#metrics"></a>
</h4>
<p>After we select the desired optimizer and loss function we want to
specify the metric(s) to be evaluated by the model during training and
testing. A <strong>metric</strong> is a function that is used to judge
the performance of your model.</p>
<pre><code><span><span class="co"># compile the pooling model</span></span>
<span><span class="co">#model_pool.compile(optimizer = adam', </span></span>
<span><span class="co">#              loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),               </span></span>
<span><span class="co">#              metrics = ['accuracy'])</span></span></code></pre>
<p>Metric functions are similar to loss functions, except that the
results from evaluating a metric are not used when training the model.
Note that you may use any loss function as a metric.</p>
<p>Typically you will use <code>accuracy</code> which calculates how
often predictions matches labels.</p>
<p>The accuracy function creates two local variables, total and count
that are used to compute the frequency with which predictions matches
labels. This frequency is ultimately returned as accuracy: an operation
that simply divides total by count.</p>
<p>For a list of metrics in Keras see <a href="https://keras.io/api/metrics/" class="external-link">metrics</a>.</p>
<p>Now that we have decided on which loss function, optimizer, and
metric to use we can compile the model using <code>model.compile</code>.
Compiling the model prepares it for training.</p>
</div>
</div>
<div class="section level3">
<h3 id="step-6--train-model">Step 6. Train model<a class="anchor" aria-label="anchor" href="#step-6--train-model"></a>
</h3>
<p>We are now ready to train the model.</p>
<p>Training the model is done using the <code>fit</code> method. It
takes the image data and target (label) data as inputs and has several
other parameters for certain options of the training. Here we only set a
different number of epochs.</p>
<p>A training <strong>epoch</strong> means that every sample in the
training data has been shown to the neural network and used to update
its parameters. In general, CNN models improve with more epochs of
training, but only to a point.</p>
<p>We want to train our model for 10 epochs:</p>
<pre><code><span><span class="co"># fit the pooling model</span></span>
<span><span class="co">#history_pool = model_pool.fit(train_images, train_labels, epochs=10, validation_data=(val_images, val_labels))</span></span></code></pre>
<p>As we saw in the previous episode, the fit method returns a history
object that has a history attribute with the training loss and
potentially other metrics per training epoch.</p>
<p>Note there are other arguments we could use to fit our model, see the
documentation for <a href="https://keras.io/api/models/model_training_apis/" class="external-link">fit
method</a>.</p>
<div id="callout3" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout3"></a>
</h3>
<div class="callout-content">
<p>ChatGPT</p>
<p>Batch size</p>
<p>The batch size is an important hyperparameter that determines the
number of training samples processed together before updating the
model’s parameters during each iteration (or mini-batch) of training.
The choice of batch size can have various implications, and there are
situations where using different batch sizes can be beneficial.</p>
<p><strong>Large Datasets and Memory Constraints</strong>: If you have a
large dataset and limited memory, using a smaller batch size can help
fit the data into memory during training. This allows you to train
larger models or use more complex architectures that might not fit with
larger batch sizes.</p>
<p><strong>Training on GPUs</strong>: Modern deep learning frameworks
and libraries are optimized for parallel processing on GPUs. Using
larger batch sizes can fully leverage the parallelism of GPUs and lead
to faster training times. However, the choice of batch size should
consider the available GPU memory.</p>
<p><strong>Noise in Parameter Updates</strong>: Smaller batch sizes
introduce more noise in the gradients, which can help models escape
sharp minima and potentially find better solutions. This regularization
effect is similar to the impact of stochasticity in Stochastic Gradient
Descent (SGD).</p>
<p><strong>Generalization</strong>: Using smaller batch sizes may
improve the generalization of the model. It prevents the model from
overfitting to the training data, as it gets updated more frequently and
experiences more diverse samples during training.</p>
<p>However, it’s essential to consider the trade-offs of using different
batch sizes. Smaller batch sizes may require more iterations to cover
the entire dataset, which can lead to longer training times. Larger
batch sizes can provide more stable gradients but might suffer from
generalization issues. There is no one-size-fits-all answer, and you may
need to experiment with different batch sizes to find the one that works
best for your specific model, architecture, and dataset.</p>
</div>
</div>
</div>
<div class="section level4">
<h4 id="monitor-training-progress-aka-model-evaluation-during-training">Monitor Training Progress (aka Model Evaluation during
Training)<a class="anchor" aria-label="anchor" href="#monitor-training-progress-aka-model-evaluation-during-training"></a>
</h4>
<p>Now that we know more about the compilation and fitting of CNN’s let
us take a look at the training metrics for our pooling model.</p>
<p>Using seaborn again we can plot the training process using the
history:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># convert the history to a dataframe for plotting </span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>history_pool_df <span class="op">=</span> pd.DataFrame.from_dict(history_pool.history)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the loss and accuracy from the training process</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">'cifar_model_pool'</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>sns.lineplot(ax<span class="op">=</span>axes[<span class="dv">0</span>], data<span class="op">=</span>history_pool_df[[<span class="st">'loss'</span>, <span class="st">'val_loss'</span>]])</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>sns.lineplot(ax<span class="op">=</span>axes[<span class="dv">1</span>], data<span class="op">=</span>history_pool_df[[<span class="st">'accuracy'</span>, <span class="st">'val_accuracy'</span>]])</span></code></pre>
</div>
<figure><img src="../fig/04_model_pool_accuracy_loss.png" alt="" class="figure mx-auto d-block"></figure><div id="challenge-the-training-curve" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-the-training-curve" class="callout-inner">
<h3 class="callout-title">Challenge The Training Curve<a class="anchor" aria-label="anchor" href="#challenge-the-training-curve"></a>
</h3>
<div class="callout-content">
<p>Looking at the training curve we have just made.</p>
<ol style="list-style-type: decimal">
<li>How does the training progress?</li>
</ol>
<ul>
<li>Does the training loss increase or decrease?</li>
<li>Does it change fast or slowly?</li>
<li>Is the graph look very jittery?</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Do you think the resulting trained network will work well on the
test set?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">
  Show me the solution
  </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<ol style="list-style-type: decimal">
<li><p>The loss curve should drop quite quickly in a smooth line with
little jitter.</p></li>
<li><p>The results of the training give very little information on its
performance on a test set. You should be careful not to use it as an
indication of a well trained network.</p></li>
</ol>
</div>
</div>
</div>
</div>
<p>If we look at these plots we can see signs of overfitting. If a model
is overfitting, it means that the model performs exceptionally well on
the training data but poorly on the validation or test data. Overfitting
occurs when the model has learned to memorize the noise and specific
patterns in the training data instead of generalizing the underlying
relationships. As a result, the model fails to perform well on new,
unseen data because it has become too specialized to the training
set.</p>
<p>Key characteristics of an overfit model include:</p>
<ul>
<li><p>High Training Accuracy, Low Validation Accuracy: The model
achieves high accuracy on the training data but significantly lower
accuracy on the validation (or test) data.</p></li>
<li><p>Small Training Loss, Large Validation Loss: The training loss is
low, indicating that the model’s predictions closely match the true
labels in the training set. However, the validation loss is high,
indicating that the model’s predictions are far from the true labels in
the validation set.</p></li>
</ul>
<p>How to Address Overfitting:</p>
<ul>
<li>Reduce the model’s complexity by using fewer layers or units to make
it less prone to overfitting.</li>
<li>Collect more training data if possible to provide the model with a
diverse and representative dataset.</li>
<li>Perform data augmentation to artificially increase the size of the
training data and introduce variability.</li>
</ul>
</div>
<div class="section level4">
<h4 id="improve-model-generalization-avoid-overfitting">Improve Model Generalization (avoid Overfitting)<a class="anchor" aria-label="anchor" href="#improve-model-generalization-avoid-overfitting"></a>
</h4>
</div>
<div class="section level4">
<h4 id="dropout">Dropout<a class="anchor" aria-label="anchor" href="#dropout"></a>
</h4>
<p>Note that the training loss continues to decrease, while the
validation loss stagnates, and even starts to increase over the course
of the epochs. Similarly, the accuracy for the validation set does not
improve anymore after some epochs. This means we are overfitting on our
training data set.</p>
<p>Techniques to avoid overfitting, or to improve model generalization,
are termed <strong>regularization techniques</strong>. One of the most
versatile regularization technique is <strong>dropout</strong>
(Srivastava et al., 2014). Dropout essentially means that during each
training cycle a random fraction of the dense layer nodes are turned
off. This is described with the dropout rate between 0 and 1 which
determines the fraction of nodes to silence at a time.</p>
<figure><img src="../fig/04-neural_network_sketch_dropout.png" alt="" class="figure mx-auto d-block"></figure><p>The intuition behind dropout is that it enforces redundancies in the
network by constantly removing different elements of a network. The
model can no longer rely on individual nodes and instead must create
multiple “paths”. In addition, the model has to make predictions with
much fewer nodes and weights (connections between the nodes). As a
result, it becomes much harder for a network to memorize particular
features. At first this might appear a quite drastic approach which
affects the network architecture strongly. In practice, however, dropout
is computationally a very elegant solution which does not affect
training speed. And it frequently works very well.</p>
<p><strong>Important to note</strong>: Dropout layers will only randomly
silence nodes during training! During a predictions step, all nodes
remain active (dropout is off). During training, the sample of nodes
that are silenced are different for each training instance, to give all
nodes a chance to observe enough training data to learn its weights.</p>
<p>Let us add one dropout layer towards the end of the network, that
randomly drops 20% of the input units.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define the inputs, layers, and outputs of a CNN model with dropout</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># CNN Part 1</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Input layer of 32x32 images with three channels (RGB)</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>inputs_dropout <span class="op">=</span> keras.Input(shape<span class="op">=</span>train_images.shape[<span class="dv">1</span>:])</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co"># CNN Part 2</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Convolutional layer with 50 filters, 3x3 kernel size, and ReLU activation</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>x_dropout <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(inputs_dropout)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Pooling layer with input window sized 2,2</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>x_dropout <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x_dropout)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Second Convolutional layer with 50 filters, 3x3 kernel size, and ReLU activation</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>x_dropout <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x_dropout)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Second Pooling layer with input window sized 2,2</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>x_dropout <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x_dropout)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Third Convolutional layer with 50 filters, 3x3 kernel size, and ReLU activation</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>x_dropout <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x_dropout)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Dropout layer andomly drops 20% of the input units</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>x_dropout <span class="op">=</span> keras.layers.Dropout(<span class="fl">0.8</span>)(x_dropout) <span class="co"># This is new!</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Flatten layer to convert 2D feature maps into a 1D vector</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>x_dropout <span class="op">=</span> keras.layers.Flatten()(x_dropout)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Dense layer with 50 neurons and ReLU activation</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>x_dropout <span class="op">=</span> keras.layers.Dense(<span class="dv">50</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x_dropout)</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a><span class="co"># CNN Part 3</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Output layer with 10 units (one for each class)</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>outputs_dropout <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>)(x_dropout)</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="co"># create the dropout model</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>model_dropout <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs_dropout, outputs<span class="op">=</span>outputs_dropout, name<span class="op">=</span><span class="st">"cifar_model_dropout"</span>)</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>model_dropout.summary()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Model: "cifar_model_dropout"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 input_8 (InputLayer)        [(None, 32, 32, 3)]       0

 conv2d_19 (Conv2D)          (None, 30, 30, 50)        1400

 max_pooling2d_12 (MaxPoolin  (None, 15, 15, 50)       0
 g2D)

 conv2d_20 (Conv2D)          (None, 13, 13, 50)        22550

 max_pooling2d_13 (MaxPoolin  (None, 6, 6, 50)         0
 g2D)

 conv2d_21 (Conv2D)          (None, 4, 4, 50)          22550

 dropout_2 (Dropout)         (None, 4, 4, 50)          0

 flatten_7 (Flatten)         (None, 800)               0

 dense_13 (Dense)            (None, 50)                40050

 dense_14 (Dense)            (None, 10)                510

=================================================================
Total params: 87,060
Trainable params: 87,060
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
<p>We can see that the dropout does not alter the dimensions of the
image, and has zero parameters.</p>
<p>We again compile and train the model.</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># compile the dropout model</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>model_dropout.<span class="bu">compile</span>(optimizer <span class="op">=</span> <span class="st">'adam'</span>,</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>              loss <span class="op">=</span> keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>              metrics <span class="op">=</span> [<span class="st">'accuracy'</span>])</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the dropout model</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>history_dropout <span class="op">=</span> model_dropout.fit(train_images, train_labels, epochs<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>(val_images, val_labels))</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co"># save dropout model</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>model_dropout.save(<span class="st">'fit_outputs/model_dropout.h5'</span>)</span></code></pre>
</div>
<p>And inspect the training results:</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># convert the history to a dataframe for plotting </span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>history_dropout_df <span class="op">=</span> pd.DataFrame.from_dict(history_dropout.history)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the loss and accuracy from the training process</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">'cifar_model_dropout'</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>sns.lineplot(ax<span class="op">=</span>axes[<span class="dv">0</span>], data<span class="op">=</span>history_dropout_df[[<span class="st">'loss'</span>, <span class="st">'val_loss'</span>]])</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>sns.lineplot(ax<span class="op">=</span>axes[<span class="dv">1</span>], data<span class="op">=</span>history_dropout_df[[<span class="st">'accuracy'</span>, <span class="st">'val_accuracy'</span>]])</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>val_loss_dropout, val_acc_dropout <span class="op">=</span> model_dropout.evaluate(val_images,  val_labels, verbose<span class="op">=</span><span class="dv">2</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>313/313 - 2s - loss: 1.4683 - accuracy: 0.5307</code></pre>
</div>
<figure><img src="../fig/04_model_dropout_accuracy_loss.png" alt="" class="figure mx-auto d-block"></figure><p>TODO CHECK OH NO Dropout model is terrible now! This is a canned
picture.</p>
<p>Now we see that the gap between the training accuracy and validation
accuracy is much smaller, and that the final accuracy on the validation
set is higher than without dropout. Nevertheless, there is still some
difference between the training loss and validation loss, so we could
experiment with regularization even more.</p>
<div id="callout4" class="callout callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout4"></a>
</h3>
<div class="callout-content">
<p>ChatGPT</p>
<p>Regularization methods for Convolutional Neural Networks (CNNs)</p>
<p>Regularization methods introduce constraints or penalties to the
training process, encouraging the model to be simpler and less prone to
overfitting. Here are some common regularization methods for CNNs:</p>
<p><strong>L1 and L2 Regularization</strong>: L1 and L2 regularization
are the two most common regularization techniques used in deep learning.
They add a penalty term to the loss function during training to restrict
the model’s weights.</p>
<ul>
<li><p>L1 regularization adds the absolute value of the weights to the
loss function. It tends to produce sparse weight vectors, forcing some
of the less important features to have exactly zero weights.</p></li>
<li><p>L2 regularization adds the square of the weights to the loss
function. It encourages the model to have smaller weights overall,
preventing extreme values and reducing the impact of individual
features.</p></li>
</ul>
<p>The regularization strength is controlled by a hyperparameter, often
denoted as lambda (λ), that determines how much weight should be given
to the regularization term. A larger λ value increases the impact of
regularization, making the model simpler and more regularized.</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>randomly “dropping out” a fraction of neurons during training. This
means that during each training iteration, some neurons are temporarily
removed from the network. Dropout effectively reduces the
interdependence between neurons, preventing the network from relying too
heavily on specific neurons and making it more robust.</li>
</ol>
<p><strong>Batch Normalization</strong>: While not explicitly a
regularization technique, Batch Normalization has a regularizing effect
on the model. It normalizes the activations of each layer in the
network, reducing internal covariate shift. This can improve training
stability and reduce the need for aggressive dropout or weight
decay.</p>
<p><strong>Data Augmentation</strong>: Data augmentation is a technique
where the training data is artificially augmented by applying various
transformations like rotation, scaling, flipping, and cropping to create
new examples. This increases the diversity of the training data and
helps the model generalize better to unseen data.</p>
<p><strong>Early Stopping</strong>: Early stopping is a form of
regularization that stops the training process when the model’s
performance on a validation set starts to degrade. This prevents the
model from overfitting by avoiding further training after the point of
best validation performance.</p>
<p>By using regularization techniques, you can improve the
generalization performance of CNNs and reduce the risk of overfitting.
It’s essential to experiment with different regularization methods and
hyperparameters to find the optimal combination for your specific CNN
architecture and dataset.</p>
</div>
</div>
</div>
<div id="challenge2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge2"></a>
</h3>
<div class="callout-content">
<p>Vary dropout rate</p>
<p>Q1. What do you think would happen if you lower the dropout rate? Try
it out, and see how it affects the model training.</p>
<p>Q2. You are varying the dropout rate and checking its effect on the
model performance, what is the term associated to this procedure?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">
  Show me the solution
  </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>Q1. Varying the dropout rate</p>
<p>The code below instantiates and trains a model with varying dropout
rates. You can see from the resulting plot that the ideal dropout rate
in this case is around 0.45. This is where the validation loss is
lowest.</p>
<ul>
<li>NB1: It takes a while to train these 5 networks.</li>
<li>NB2: In the real world you should do this with a test set and not
with the validation set!</li>
</ul>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>dropout_rates <span class="op">=</span> [<span class="fl">0.15</span>, <span class="fl">0.3</span>, <span class="fl">0.45</span>, <span class="fl">0.6</span>, <span class="fl">0.75</span>]</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>val_losses_vary <span class="op">=</span> []</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> dropout_rate <span class="kw">in</span> dropout_rates:</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    inputs_vary <span class="op">=</span> keras.Input(shape<span class="op">=</span>train_images.shape[<span class="dv">1</span>:])</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    x_vary <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(inputs_vary)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    x_vary <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x_vary)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    x_vary <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x_vary)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    x_vary <span class="op">=</span> keras.layers.MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>))(x_vary)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    x_vary <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x_vary)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    x_vary <span class="op">=</span> keras.layers.Dropout(dropout_rate)(x_vary)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    x_vary <span class="op">=</span> keras.layers.Flatten()(x_vary)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    x_vary <span class="op">=</span> keras.layers.Dense(<span class="dv">50</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x_vary)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    outputs_vary <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>)(x_vary)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    model_vary <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs_vary, outputs<span class="op">=</span>outputs_vary, name<span class="op">=</span><span class="st">"cifar_model_vary"</span>)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    model_vary.<span class="bu">compile</span>(optimizer <span class="op">=</span> <span class="st">'adam'</span>,</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>              loss <span class="op">=</span> keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>              metrics <span class="op">=</span> [<span class="st">'accuracy'</span>])</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>    model_vary.fit(train_images, train_labels, epochs<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>(val_images, val_labels))</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>    val_loss_vary, val_acc_vary <span class="op">=</span> model_vary.evaluate(val_images,  val_labels)</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>    val_losses_vary.append(val_loss_vary)</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>loss_df <span class="op">=</span> pd.DataFrame({<span class="st">'dropout_rate'</span>: dropout_rates, <span class="st">'val_loss_vary'</span>: val_losses_vary})</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>sns.lineplot(data<span class="op">=</span>loss_df, x<span class="op">=</span><span class="st">'dropout_rate'</span>, y<span class="op">=</span><span class="st">'val_loss_vary'</span>)</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>model_vary.save(<span class="st">'fit_outputs/model_vary.h5'</span>)</span></code></pre>
</div>
<figure><img src="../fig/04_vary_dropout_rate.png" alt="" class="figure mx-auto d-block"></figure><p>Q2. Term associated to this procedure This is called hyperparameter
tuning.</p>
</div>
</div>
</div>
</div>
</div>
</div>
<section id="choose-the-best-model-and-use-it-to-predict"><h2 class="section-heading">Choose the best model and use it to predict<a class="anchor" aria-label="anchor" href="#choose-the-best-model-and-use-it-to-predict"></a>
</h2>
<hr class="half-width">
<p>Based on our evaluation of the loss and accuracy metrics, the
<code>model_dropout</code> appears to have the best performance
<strong>of the models we have examined thus far</strong>. The next step
is to use these models to predict object classes on our test
dataset.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Keypoints<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>The choice of loss function will depend on your dataset and aim</li>
<li>The choice of optimizer often depends on experimentation and
empirical evaluation</li>
<li>Fitting separate models with different hyperparameters and comparing
their performance is a common and good practice in deep learning</li>
<li>Dropout is one way to prevent overfitting</li>
</ul>
</div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-05-evaluate-predict-cnn"><p>Content from <a href="05-evaluate-predict-cnn.html">Evaluate a Convolutional Neural Network and Make Predictions (Classifications)</a></p>
<hr>
<p> Last updated on 2023-10-12 | 
        
        <a href="https://https://github.com/erinmgraham/icwithcnn/edit/main/episodes/05-evaluate-predict-cnn.html" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time <i aria-hidden="true" data-feather="clock"></i> 12 minutes </p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right"> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How do you use a model to make a prediction?</li>
<li>How do you measure model prediction accuracy?</li>
<li>What is a hyperparameter?</li>
<li>What can you do to improve model performance?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Use a convolutional neural network (CNN) to make a prediction (ie
classify an image)</li>
<li>Explain how to measure the performance of a CNN</li>
<li>Explain hyperparameter tuning</li>
<li>Be familiar with advantages and disadvantages of different
optimizers</li>
<li>Understand what steps to take to improve model accuracy</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="step-7--perform-a-predictionclassification">Step 7. Perform a Prediction/Classification<a class="anchor" aria-label="anchor" href="#step-7--perform-a-predictionclassification"></a>
</h3>
<p>After you fully train the network to a satisfactory performance on
the training and validation sets, we use it to perform predictions on a
special hold-out set, the <strong>test</strong> set. The prediction
accuracy of the model on new images will be used in <strong>Step 8.
Measuring performance</strong> to measure the performance of the
network.</p>
<div class="section level4">
<h4 id="prepare-test-dataset">Prepare test dataset<a class="anchor" aria-label="anchor" href="#prepare-test-dataset"></a>
</h4>
<p>Recall in Episode 2 Introduction to Image Data we discussed how to
split your data into training, validation and test datasets and why. In
most cases, that means you already have a test set on hand. For example,
in that episode we create a variable <code>test_images</code> and
assigned a sample of the CINIC-10 dataset.</p>
<p>When creating a test set there are a few things to check:</p>
<ul>
<li>only contains images that the model has never seen before</li>
<li>sufficiently large to provide a meaningful evaluation of model
performance
<ul>
<li>images from every target label</li>
<li>images of classes not in your target set</li>
</ul>
</li>
<li>processed in the same way as your training set</li>
</ul>
<p>Check to make sure you have a model in memory and a test dataset:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># check correct model is loaded</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>model_best <span class="op">=</span> keras.models.load_model(<span class="st">'fit_outputs/model_dropout.h5'</span>) <span class="co"># pick your best model</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'We are using'</span>, model_best.name)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># check test image dataset is loaded</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The number and shape of images in our test dataset is:'</span>, test_images.shape)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The number of labels in our test dataset is:'</span>, <span class="bu">len</span>(test_labels))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>We are using  cifar_model_dropout
The number and shape of images in our test dataset is:  (10000, 32, 32, 3)
The number of labels in our test dataset is:  10000</code></pre>
</div>
<div id="challenge1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge1"></a>
</h3>
<div class="callout-content">
<p>Is the CINIC-10 model a good test data set? Hint: Read the ‘Details’
and ‘Construction’ sections of the <a href="https://github.com/BayesWatch/cinic-10/" class="external-link">CINIC-10</a>.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">
  Show me the solution
  </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>No! “The entirety of the original CIFAR-10 test set is within the
above mentioned new test set.”</p>
<p>Make sure the images you use for test have not been used to
train!</p>
</div>
</div>
</div>
</div>
<div id="challenge2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge2"></a>
</h3>
<div class="callout-content">
<p>How big should our test data set be?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">
  Show me the solution
  </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>Depends! Recall in an Episode 02 Introduction to Image Data Callout
we talked about the different ways to partition the data into training,
validation and test data sets. For example, using the <strong>Stratified
Sampling</strong> technique, we might split the data using these
rations: 80-10-10 or 70-15-15.</p>
</div>
</div>
</div>
</div>
</div>
<div class="section level4">
<h4 id="predict">Predict<a class="anchor" aria-label="anchor" href="#predict"></a>
</h4>
<p>Armed with a test dataset, we will use our CNN to predict their class
labels using the <code>predict</code> function and then use these
predictions in Step 8 to measure the performance of our trained
network.</p>
<p>Recall our model will return a vector of probabilities, one for each
class. By finding the class with the highest probability, we can select
the most likely class name of the object.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># use our current best model to predict probability of each class on new test set</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>predicted_prob <span class="op">=</span> model_best.predict(test_images)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># create a list of class names </span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>class_names <span class="op">=</span> [<span class="st">'airplane'</span>, <span class="st">'automobile'</span>, <span class="st">'bird'</span>, <span class="st">'cat'</span>, <span class="st">'deer'</span>, <span class="st">'dog'</span>, <span class="st">'frog'</span>, <span class="st">'horse'</span>, <span class="st">'ship'</span>, <span class="st">'truck'</span>]</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># convert probability predictions to table using class names for column names</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>prediction_df <span class="op">=</span> pd.DataFrame(predicted_prob, columns<span class="op">=</span>class_names)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># inspect </span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prediction_df.head())</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>airplane  automobile      bird  ...     horse      ship     truck
0  0.165748   -0.118394  0.062156  ...  0.215477  0.013811 -0.047446
1  0.213530   -0.126139  0.052813  ...  0.264517  0.009097 -0.091710
2  0.211900   -0.099055  0.047890  ...  0.242345 -0.014492 -0.073153
3  0.187883   -0.085144  0.044609  ...  0.217864  0.007502 -0.055209
4  0.190110   -0.118892  0.054869  ...  0.252434 -0.030064 -0.061485

[5 rows x 10 columns]</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># now find the maximum probability for each image</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>predicted_labels <span class="op">=</span> predicted_prob.argmax(axis<span class="op">=</span><span class="dv">1</span>)</span></code></pre>
</div>
</div>
</div>
<div class="section level3">
<h3 id="step-8--measuring-performance">Step 8. Measuring performance<a class="anchor" aria-label="anchor" href="#step-8--measuring-performance"></a>
</h3>
<p>Once we trained the network we want to measure its performance. There
are many different methods available for measuring performance and which
one is best depends on the type of task we are attempting. These metrics
are often published as an indication of how well our network
performs.</p>
<p>An easy way to visually check the observed versus predicted classes
is to plot the index of each:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the predicted versus the true class</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># training labels are numeric; we want test labels to be the same for plotting</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># use the list of class names to convert test_labels to test_values</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># recall train_values were numeric, not strings</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># use element position in class_names to generate label values</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>test_labels_values <span class="op">=</span> [] </span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(test_labels)):</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    test_labels_values.append(class_names.index(test_labels[i]))</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co"># make the plot</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>plt.scatter(test_labels_values, predicted_labels)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Test Class'</span>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Predicted Class'</span>)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="dv">0</span>, <span class="dv">9</span>)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="dv">9</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="co">#plt.axline(xy1=(0,0), xy2=(9,9), linestyle='--') # expected</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="../fig/05_pred_v_true_plot_scatter.png" alt="" class="figure mx-auto d-block"></figure><p>To obtain a more quantitative measure of model performance, we can
create a confusion matrix.</p>
<div class="section level4">
<h4 id="confusion-matrix">Confusion matrix<a class="anchor" aria-label="anchor" href="#confusion-matrix"></a>
</h4>
<p>In the case of multiclass classifications (c.f. binary
classifications), each cell value (C<sub>i,j</sub>) is equal to the
number of observations known to be in group <em>i</em> and predicted to
be in group <em>j</em>.</p>
<figure><img src="../fig/05_confusion_matrix_explained.png" alt="" class="figure mx-auto d-block"></figure><p>To create a confusion matrix we will use another convenient function
from sklearn called <code>confusion_matrix</code>. This function takes
as a first parameter the true labels of the test set. The second
parameter is the predicted labels which we did above.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> confusion_matrix(test_labels_values, predicted_labels)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(conf_matrix)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[[ 24   0   0   0   0   0   0 975   1   0]
 [ 38   0   0   0   0   0   0 962   0   0]
 [ 25   0   0   0   0   0   0 975   0   0]
 [ 35   0   0   0   0   0   0 965   0   0]
 [ 23   0   0   0   0   0   0 977   0   0]
 [ 43   0   0   0   0   0   0 957   0   0]
 [ 26   0   0   0   0   0   0 974   0   0]
 [ 39   0   0   0   0   0   0 961   0   0]
 [ 22   0   0   0   0   0   0 978   0   0]
 [ 36   0   0   0   0   0   0 964   0   0]]</code></pre>
</div>
<p>Unfortunately, this matrix is kinda hard to read. It’s not clear
which column and which row corresponds to which class. So let’s convert
it to a pandas dataframe with its index and columns set to the class
labels as follows:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to a pandas dataframe</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>confusion_df <span class="op">=</span> pd.DataFrame(conf_matrix, index<span class="op">=</span>class_names, columns<span class="op">=</span>class_names)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the names of the x and y axis, this helps with the readability of the heatmap.</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>confusion_df.index.name <span class="op">=</span> <span class="st">'True Label'</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>confusion_df.columns.name <span class="op">=</span> <span class="st">'Predicted Label'</span></span></code></pre>
</div>
<p>We can then use the <code>heatmap</code> function from seaborn to
create a nice visualization of the confusion matrix.</p>
<ul>
<li>the <code>annot=True</code> parameter here will put the numbers from
the confusion matrix in the heatmap</li>
<li>the <code>fmt=3g</code> will display the values with 3 significant
digits</li>
</ul>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>sns.heatmap(confusion_df, annot<span class="op">=</span><span class="va">True</span>)</span></code></pre>
</div>
<figure><img src="../fig/05_pred_v_true_confusion_matrix.png" alt="" class="figure mx-auto d-block"></figure><div id="challenge3" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge3"></a>
</h3>
<div class="callout-content">
<p>Confusion Matrix</p>
<p>Measure the performance of the neural network you trained and
visualized as a confusion matrix.</p>
<p>Q1. Did the neural network perform well on the test set?</p>
<p>Q2. Did you expect this from the training loss you saw?</p>
<p>Q3. What could we do to improve the performance?</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">
  Show me the solution
  </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<p>Q1. The confusion matrix shows that the predictions for terrible and
can improved.</p>
<p>Q2. I expected the performance to be poor because the accuracy of the
model I chose was only 10% on the validation set.</p>
<p>Q3. We can try many things to improve the performance from here. One
of the first things we can try is to change the network architecture.
However, in the interest of time and given we already saw how to build a
CNN we will try to change the training parameters.</p>
</div>
</div>
</div>
</div>
<div id="challenge4" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge4"></a>
</h3>
<div class="callout-content">
<p>Try your own image!</p>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4">
  Show me the solution
  </h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" data-bs-parent="#accordionSolution4" aria-labelledby="headingSolution4">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># specify a new image and prepare it to match CIFAR-10 dataset</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> icwithcnn_functions <span class="im">import</span> prepare_image_icwithcnn</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>new_img_path <span class="op">=</span> <span class="st">"../data/Jabiru_TGS.JPG"</span> <span class="co"># path to YOUR image</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>new_img_prepped <span class="op">=</span> prepare_image_icwithcnn(new_img_path)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co"># predict the classname</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>result_intro <span class="op">=</span> model_intro.predict(new_img_prepped) <span class="co"># make prediction</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result_intro) <span class="co"># probability for each class</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(class_names[result_intro.argmax()]) <span class="co"># class with highest probability</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Result: [[-2.0185328   9.337507   -2.4551604  -0.4688053  -4.599108   -3.5822825
   6.427376   -0.09437321  0.82065487  1.2978227 ]]
Class name: automobile

NOTE your output will vary!
</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="step-9--tune-hyperparameters">Step 9. Tune hyperparameters<a class="anchor" aria-label="anchor" href="#step-9--tune-hyperparameters"></a>
</h3>
<p>Recall the following from Episode 1:</p>
<div class="section level4">
<h4 id="what-are-hyperparameters">What are hyperparameters?<a class="anchor" aria-label="anchor" href="#what-are-hyperparameters"></a>
</h4>
<p>Hyperparameters are the parameters set by the person configuring the
model instead of those learned by the algorithm itself. Like the dials
on a radio which are <em>tuned</em> to the best frequency,
hyperparameters can be <em>tuned</em> to the best combination for a
given model and context.</p>
<p>These hyperparameters can include the learning rate, the number of
layers in the network, the number of neurons per layer, and many more.
The tuning process is systematic searching for the best combination of
hyperparameters that will optimize the model’s performance.</p>
<p>In some cases, it might be necessary to adjust these and re-run the
training many times before we are happy with the result.</p>
<p>Table 1. List of some of the hyperparameters to tune and when.</p>
<table class="table">
<thead><tr class="header">
<th>During Build</th>
<th>When Compiling</th>
<th>During Training</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>number of neurons</td>
<td>loss function</td>
<td>epoch</td>
</tr>
<tr class="even">
<td>activation function</td>
<td>optimizer</td>
<td>batch size</td>
</tr>
<tr class="odd">
<td></td>
<td>learning rate</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>batch size</td>
<td></td>
</tr>
</tbody>
</table>
<p>One common method for hyperparameter tuning is <strong>grid
search</strong>.</p>
</div>
<div class="section level4">
<h4 id="what-is-grid-search">What is Grid Search?<a class="anchor" aria-label="anchor" href="#what-is-grid-search"></a>
</h4>
<p>Grid Search or <code>GridSearchCV</code> (as per the library function
call) is foundation method for hyperparameter tuning. The aim of
hyperparameter tuning is to define a grid of possible values for each
hyperparameter you want to tune. GridSearch will then evaluate the model
performance for each combination of hyperparameters in a brute-force
manner, iterating through every possible combination in the grid.</p>
<p>For instance, suppose you’re tuning two hyperparameters:</p>
<ul>
<li><p>Learning rate: with possible values [0.01, 0.1, 1]</p></li>
<li><p>Batch size: with possible values [10, 50, 100]</p></li>
<li><p>GridSearch will evaluate the model for all 3x3 = 9 combinations
(e.g., {0.01, 10}, {0.01, 50}, {0.1, 10}, and so on)</p></li>
</ul>
</div>
</div>
<div class="section level3">
<h3 id="tune-hyperparameters-example-use-gridsearch-to-tune-optimizer">Tune Hyperparameters Example: use GridSearch to tune
<strong>Optimizer</strong>
<a class="anchor" aria-label="anchor" href="#tune-hyperparameters-example-use-gridsearch-to-tune-optimizer"></a>
</h3>
<p>In episode 04 we talked briefly about the <code>Adam</code> optimizer
used in our <code>model.compile</code> discussion. Recall the optimizer
refers to the algorithm with which the model learns to optimize on the
provided loss function.</p>
<p>Here we will use our introductory model to demonstrate how GridSearch
is expressed in code to search for an optimizer.</p>
<p>First, we will define a <strong>build function</strong> to use during
GridSearch. This function will compile the model for each combination of
parameters prior to evaluation.</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_model():</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Input layer of 32x32 images with three channels (RGB)</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>train_images.shape[<span class="dv">1</span>:])</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convolutional layer with 50 filters, 3x3 kernel size, and ReLU activation</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>     <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(inputs)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Second Convolutional layer</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Flatten layer to convert 2D feature maps into a 1D vector</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Flatten()(x)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Output layer with 10 units (one for each class)</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>)(x)</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># create the model</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    mode <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>outputs)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># compile the pooling model</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(optimizer <span class="op">=</span> <span class="st">'adam'</span>, loss <span class="op">=</span> keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>), metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span></code></pre>
</div>
<p>Secondly, we can define our GridSearch parameters and assign fit
results to a variable for output.</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scikeras.wrappers <span class="im">import</span> KerasClassifier</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Wrap the model</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> KerasClassifier(build_fn<span class="op">=</span>create_model, epochs<span class="op">=</span><span class="dv">2</span>, batch_size<span class="op">=</span><span class="dv">32</span>, verbose<span class="op">=</span><span class="dv">0</span>)  <span class="co"># epochs, batch_size, verbose can be adjusted as required. Using low epochs to save computation time and demonstration purposes only</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the grid search parameters</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> [<span class="st">'SGD'</span>, <span class="st">'RMSprop'</span>, <span class="st">'Adam'</span>]</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> <span class="bu">dict</span>(optimizer<span class="op">=</span>optimizer)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> GridSearchCV(estimator<span class="op">=</span>model, param_grid<span class="op">=</span>param_grid, n_jobs<span class="op">=</span><span class="dv">1</span>, cv<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>grid_result <span class="op">=</span> grid.fit(train_images, train_labels)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Summarize results</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best: </span><span class="sc">%f</span><span class="st"> using </span><span class="sc">%s</span><span class="st">"</span> <span class="op">%</span> (grid_result.best_score_, grid_result.best_params_))</span></code></pre>
</div>
<p>Output from the GridSearch process should look similar to:</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Best: 0.586660 using {'optimizer': 'RMSprop'}</code></pre>
</div>
<p>Thus, we can interpret from this output that our best tested
optimiser is the <strong>root mean square propagation</strong>
optimiser, or RMSprop.</p>
<p>Curious about RMSprop? Read more here: <a href="https://keras.io/api/optimizers/rmsprop/" class="external-link">RMSprop in Keras</a> and
<a href="https://optimization.cbe.cornell.edu/index.php?title=RMSProp" class="external-link">RMSProp,
Cornell University</a>.</p>
</div>
<div class="section level3">
<h3 id="tune-hyperparameters-example-use-brute-force-to-tune-activation-function">Tune Hyperparameters Example: use brute force to tune
<strong>Activation Function</strong>
<a class="anchor" aria-label="anchor" href="#tune-hyperparameters-example-use-brute-force-to-tune-activation-function"></a>
</h3>
<p>In episode 03 we talked briefly about the <code>relu</code>
activation function passed as an argument to our <code>Conv2D</code>
hidden layers.</p>
<p>An activation function is like a switch or a filter that we use in
artificial neural networks, inspired by how our brains work. These
functions play a crucial role in determining whether a neuron (a small
unit in the neural network) should “fire” or become active.</p>
<p>Think of an activation function as a tiny decision-maker for each
neuron in a neural network. It helps determine whether the neuron should
‘fire’, or pass on information, or stay ‘off’ and remain silent, much
like a light switch that decides whether the light should be ON or OFF.
Activation functions are crucial because they add non-linearity to the
neural network. Without them, the network would be like a simple linear
model, unable to learn complex patterns in data.</p>
<div class="section level4">
<h4 id="how-do-you-know-what-activation-function-to-choose">How do you know what activation function to choose?<a class="anchor" aria-label="anchor" href="#how-do-you-know-what-activation-function-to-choose"></a>
</h4>
<p>Neural networks can be tuned to leverage many different types of
activation functions. In fact, it is a crucial decision as the choice of
activation function will have a direct impact on the performance of the
model.</p>
<p>Table 2. Descrition of each activation function, its benefits, and
drawbacks.</p>
<table style="width:100%;" class="table">
<colgroup>
<col width="16%">
<col width="50%">
<col width="33%">
</colgroup>
<thead><tr class="header">
<th>Activation Function</th>
<th>Positives</th>
<th>Negatives</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>ReLU</td>
<td>- Addresses vanishing gradient problem <br> - Computationally
efficient</td>
<td>- Can cause “dying neurons” <br> - Not zero-centered</td>
</tr>
<tr class="even">
<td>Leaky ReLU</td>
<td>- Addresses the “dying ReLU” problem <br> - Computationally
efficient</td>
<td>- Empirical results can be inconsistent <br> - Not
zero-centered</td>
</tr>
<tr class="odd">
<td>Sigmoid</td>
<td>- Outputs between 0 and 1 <br> - Smooth gradient</td>
<td>- Can cause vanishing gradient problem <br> - Computationally more
expensive</td>
</tr>
<tr class="even">
<td>Tanh</td>
<td>- Outputs between -1 and 1 <br> - Zero-centered</td>
<td>- Can still suffer from vanishing gradients to some extent</td>
</tr>
<tr class="odd">
<td>Softmax</td>
<td>- Used for multi-class classification <br> - Outputs a probability
distribution</td>
<td>- Used only in the output layer for classification tasks</td>
</tr>
<tr class="even">
<td>SELU</td>
<td>- Self-normalizing properties <br> - Can outperform ReLU in deeper
networks</td>
<td>- Requires specific weight initialization <br> - May not perform
well outside of deep architectures</td>
</tr>
</tbody>
</table>
</div>
<div class="section level4">
<h4 id="assessing-activiation-function-performance">Assessing activiation function performance<a class="anchor" aria-label="anchor" href="#assessing-activiation-function-performance"></a>
</h4>
<p>The code below serves as a practical means for exploring activation
performance on an image dataset.</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function to create a model with a given activation function</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_model(activation_function):</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Input layer of 32x32 images with three channels (RGB)</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>train_images.shape[<span class="dv">1</span>:])</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convolutional layer with 50 filters, 3x3 kernel size, and ReLU activation</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span>activation_function)(inputs)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Second Convolutional layer</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Conv2D(<span class="dv">50</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span>activation_function)(x)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Flatten layer to convert 2D feature maps into a 1D vector</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Flatten()(x)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Output layer with 10 units (one for each class)</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>)(x)</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># create the model</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>outputs)</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># create the model</span></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(optimizer <span class="op">=</span> <span class="st">'adam'</span>, loss <span class="op">=</span> keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>), metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a><span class="co"># List of activation functions to try</span></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>activations <span class="op">=</span> [<span class="st">'relu'</span>, <span class="st">'sigmoid'</span>, <span class="st">'tanh'</span>, <span class="st">'selu'</span>, keras.layers.LeakyReLU()]</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>history_data <span class="op">=</span> {}</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Train a model with each activation function and store the history</span></span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> activation <span class="kw">in</span> activations:</span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> create_model(activation)</span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>    history <span class="op">=</span> model.fit(train_images, train_labels, epochs<span class="op">=</span><span class="dv">10</span>, validation_data<span class="op">=</span>(val_images, val_labels))</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>    history_data[<span class="bu">str</span>(activation)] <span class="op">=</span> history</span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the validation accuracy for each activation function</span></span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> activation, history <span class="kw">in</span> history_data.items():</span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>    plt.plot(history.history[<span class="st">'val_accuracy'</span>], label<span class="op">=</span>activation)</span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Validation accuracy for different activation functions'</span>)</span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epochs'</span>)</span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Validation Accuracy'</span>)</span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="../fig/05_tune_activation_results.png" alt="" class="figure mx-auto d-block"></figure><p>You can see in this figure that after 10 epochs the <code>relu</code>
and <code>sigmoid</code> activation functions appear to converge around
0.60% validation accuracy. We recommend when tuning your model to ensure
you use enough epochs to be confident in your results.</p>
<div id="challenge5" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge5"></a>
</h3>
<div class="callout-content">
<p>Open question: What could be next steps to further improve the
model?</p>
<p>With unlimited options to modify the model architecture or to play
with the training parameters, deep learning can trigger very extensive
hunting for better and better results. Usually models are “well
behaving” in the sense that small chances to the architectures also only
result in small changes of the performance (if any). It is often
tempting to hunt for some magical settings that will lead to much better
results. But do those settings exist? Applying common sense is often a
good first step to make a guess of how much better could results be. In
the present case we might certainly not expect to be able to reliably
predict sunshine hours for the next day with 5-10 minute precision. But
how much better our model could be exactly, often remains difficult to
answer.</p>
<ul>
<li>What changes to the model architecture might make sense to
explore?</li>
<li>Ignoring changes to the model architecture, what might notably
improve the prediction quality?</li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution5" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution5" aria-expanded="false" aria-controls="collapseSolution5">
  <h4 class="accordion-header" id="headingSolution5">
  Show me the solution
  </h4>
</button>
<div id="collapseSolution5" class="accordion-collapse collapse" data-bs-parent="#accordionSolution5" aria-labelledby="headingSolution5">
<div class="accordion-body">
<p>This is an open question. And we don’t actually know how far one
could push this sunshine hour prediction (try it out yourself if you
like! We’re curious!). But there is a few things that might be worth
exploring.</p>
<p>Regarding the model architecture:</p>
<ul>
<li>In the present case we do not see a magical silver bullet to
suddenly boost the performance. But it might be worth testing if deeper
networks do better (more layers).</li>
</ul>
<p>Other changes that might impact the quality notably:</p>
<ul>
<li>The most obvious answer here would be: more data! Even this will not
always work (e.g. if data is very noisy and uncorrelated, more data
might not add much).</li>
<li>Related to more data: use data augmentation. By creating realistic
variations of the available data, the model might improve as well.</li>
<li>More data can mean more data points (you can test it yourself by
taking more than the 3 years we used here!)</li>
<li>More data can also mean more features! What about adding the
month?</li>
<li>The labels we used here (sunshine hours) are highly biased, many
days with no or nearly no sunshine but few with &gt;10 hours. Techniques
such as oversampling or undersampling might handle such biased labels
better. Another alternative would be to not only look at data from one
day, but use the data of a longer period such as a full week. This will
turn the data into time series data which in turn might also make it
worth to apply different model architectures….</li>
</ul>
</div>
</div>
</div>
</div>
<p>By now you should have a well-trained, finely-tuned model that makes
accurate predictions and are ready to share the model with others.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Keypoints<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Use model.predict to make a prediction with your model</li>
<li>Model accuracy must be measured on a test dataset with images your
model has not seen before</li>
<li>There are many hyperparameters to choose from to improve model
performance</li>
</ul>
</div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</div>
</div></section><section id="aio-06-conclusion"><p>Content from <a href="06-conclusion.html">Conclusion</a></p>
<hr>
<p> Last updated on 2023-10-11 | 
        
        <a href="https://https://github.com/erinmgraham/icwithcnn/edit/main/episodes/06-conclusion.html" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time <i aria-hidden="true" data-feather="clock"></i> 12 minutes </p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right"> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How do I share my convolutional neural network (CNN)?</li>
<li>Where can I find pre-trained models?</li>
<li>What is a GPU?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Learn how to save and load models</li>
<li>Know where to look for pretrained models</li>
<li>Understand what a GPU is and what it can do for you</li>
<li>Explain when to use a CNN and when not to</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="step-10--share-model">Step 10. Share model<a class="anchor" aria-label="anchor" href="#step-10--share-model"></a>
</h3>
<p>Now that we have a trained network that performs at a level we are
happy with and can maintain high prediction accuracy on a test dataset
we might want to consider publishing a file with both the architecture
of our network and the weights which it has learned (assuming we did not
use a pre-trained network). This will allow others to use it as as
pre-trained network for their own purposes and for them to (mostly)
reproduce our result.</p>
<p>We have already seen how to save a model with
<code>model.save</code>:</p>
<pre><code><span><span class="co">#model.save('model_final.h5')</span></span></code></pre>
<p>The <code>save</code> method is actually an alias for
<code>tf.keras.saving.save_model()</code> where the default
<code>save_format=NONE</code>. By adding the extension
<strong>.h5</strong> to our filename, keras will save the model in the
legacy HDF5 format.</p>
<p>This saved model can be loaded again by using the
<code>load_model</code> method as follows:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load a saved model</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>pretrained_model <span class="op">=</span> keras.models.load_model(<span class="st">'model_final.h5'</span>)</span></code></pre>
</div>
<p>This loaded model can be used as before to predict.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># use the pretrained model here</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> icwithcnn_functions <span class="im">import</span> prepare_image_icwithcnn</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>new_img_path <span class="op">=</span> <span class="st">"../data/Jabiru_TGS.JPG"</span> <span class="co"># path to image</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>new_img_prepped <span class="op">=</span> prepare_image_icwithcnn(new_img_path)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># predict the class name</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>y_pretrained_pred <span class="op">=</span> pretrained_model.predict(new_img_prepped)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>pretrained_predicted_class <span class="op">=</span> class_names[y_pretrained_pred.argmax()]</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pretrained_predicted_class)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="va">frog</span></span></code></pre>
</div>
<p>The HDF5 file format contains:</p>
<ul>
<li>configuration (architecture)</li>
<li>weights</li>
<li>optimizer’s state (if any)
<ul>
<li>allows you to continue training; useful for checkpointing</li>
</ul>
</li>
</ul>
<p>Note that saving the model does not save the training history (ie
training and validation loss and accuracy). For that you will need to
save the model history dataframe we created for plotting.</p>
<p>To find out more about other file formats you can use to save your
model see the Keras documentation for <a href="https://keras.io/api/saving/" class="external-link">Saving and Serialization</a>.</p>
<p>To share your model with a wider audience it is recommended you
create git repository, such as <a href="https://github.com/" class="external-link">GitHub</a>,
and upload your code, images, and model outputs to the cloud. In some
cases, you may be able to offer up your model to an online repository of
pretrained models.</p>
<div class="section level4">
<h4 id="choosing-a-pretrained-model">Choosing a pretrained model<a class="anchor" aria-label="anchor" href="#choosing-a-pretrained-model"></a>
</h4>
<p>If your data and problem is very similar to what others have done,
you can often use a pretrained network. Even if your problem is
different, but the data type is common (for example images), you can use
a pretrained network and finetune it for your problem. A large number of
openly available pretrained networks can be found in the <a href="https://modelzoo.co/" class="external-link">Model Zoo</a>, <a href="https://pytorch.org/hub/" class="external-link">pytorch hub</a> or <a href="https://pytorch.org/hub/" class="external-link">tensorflow hub</a>.</p>
</div>
</div>
<div class="section level3">
<h3 id="what-else-do-i-need-to-know">What else do I need to know?<a class="anchor" aria-label="anchor" href="#what-else-do-i-need-to-know"></a>
</h3>
<div class="section level4">
<h4 id="how-to-choose-a-deep-learning-library">How to choose a Deep Learning Library<a class="anchor" aria-label="anchor" href="#how-to-choose-a-deep-learning-library"></a>
</h4>
<p>In this lesson we chose to use <a href="https://keras.io/" class="external-link">Keras</a>
because it was designed to be easy to use and usually requires fewer
lines of code than other libraries. Keras can actually work on top of
TensorFlow (and several other libraries), hiding away the complexities
of TensorFlow while still allowing you to make use of their
features.</p>
<p>The performance of Keras is sometimes not as good as other libraries
and if you are going to move on to create very large networks using very
large datasets then you might want to consider one of the other
libraries. But for many applications the performance difference will not
be enough to worry about and the time you will save with simpler code
will exceed what you will save by having the code run a little
faster.</p>
<p>Keras also benefits from a very good set of <a href="https://keras.io/guides/" class="external-link">online documentation</a> and a large
user community. You will find that most of the concepts from Keras
translate very well across to the other libraries if you wish to learn
them at a later date.</p>
<p>A couple of those libraries include:</p>
<ul>
<li><p><a href="https://www.tensorflow.org/" class="external-link">TensorFlow</a> was
developed by Google and is one of the older Deep Learning libraries,
ported across many languages since it was first released to the public
in 2015. It is very versatile and capable of much more than Deep
Learning but as a result it often takes a lot more lines of code to
write Deep Learning operations in TensorFlow than in other libraries. It
offers (almost) seamless integration with GPU accelerators and Google’s
own TPU (Tensor Processing Unit) chips that are built specially for
machine learning.</p></li>
<li><p><a href="https://pytorch.org/" class="external-link">PyTorch</a> was developed by
Facebook in 2016 and is a popular choice for Deep Learning applications.
It was developed for Python from the start and feels a lot more
“pythonic” than TensorFlow. Like TensorFlow it was designed to do more
than just Deep Learning and offers some very low level interfaces. <a href="https://www.pytorchlightning.ai/" class="external-link">PyTorch Lightning</a> offers a
higher level interface to PyTorch to set up experiments. Like TensorFlow
it is also very easy to integrate PyTorch with a GPU. In many benchmarks
it outperforms the other libraries.</p></li>
<li><p>NEW <a href="https://keras.io/keras_core/announcement/?utm_source=ADSA&amp;utm_campaign=60c8d8b6cb-EMAIL_CAMPAIGN_2022_10_04_06_04_COPY_01&amp;utm_medium=email&amp;utm_term=0_5401c7226a-60c8d8b6cb-461545621" class="external-link">Keras
Core</a> In Fall 2023, this library will become Keras 3.0. Keras Core is
a full rewrite of the Keras codebase that rebases it on top of a modular
backend architecture. It makes it possible to run Keras workflows on top
of arbitrary frameworks — starting with TensorFlow, JAX, and
PyTorch.</p></li>
</ul>
</div>
<div class="section level4">
<h4 id="what-is-a-gpu-and-do-i-need-one">What is a GPU and do I need one?<a class="anchor" aria-label="anchor" href="#what-is-a-gpu-and-do-i-need-one"></a>
</h4>
<p>A <strong>GPU</strong>, or <strong>Graphics Processing Unit</strong>,
is a specialized electronic circuit designed to accelerate graphics
rendering and image processing in a computer. In the context of deep
learning and machine learning, GPUs have become essential due to their
ability to perform parallel computations at a much faster rate compared
to traditional central processing units (CPUs). This makes them
well-suited for the intensive matrix and vector operations that are
common in deep learning algorithms.</p>
<p>As you have seen in this lesson, training CNN models can take a long
time. If you follow the steps presented here you will find you are
training multiple models to find the one best suited to your needs,
particularly when fine tuning hyperparameters. However you have also
seen that running on CPU only machines can be done! So while a GPU is
not an absolute requirement for deep learning, it can significantly
accelerate your deep learning work and make it more efficient,
especially for larger and more complex tasks.</p>
<p>If you don’t have access to a powerful GPU locally, you can use cloud
services that provide GPU instances for deep learning. This can be a
cost-effective option for many users.</p>
</div>
<div class="section level4">
<h4 id="it-this-the-bestonly-way-to-code-up-cnns-for-image-classification">It this the best/only way to code up CNN’s for image
classification?<a class="anchor" aria-label="anchor" href="#it-this-the-bestonly-way-to-code-up-cnns-for-image-classification"></a>
</h4>
<p>Absolutely not! The code we used in today’s workshop might today be
considered old fashioned. A lot of the data preprocessing we did by hand
can now be done by simply adding different layer types to your model.
See, for example, the <a href="https://keras.io/guides/preprocessing_layers/" class="external-link">preprocessing
layers</a> available with keras.</p>
<p>The point is that this technology, both hardware and software, is
dynamic and changing at exponentially increasing rates. It is essential
to stay curious and open to learning and follow up with continuous
education and practice. Other strategies to stay informed include:</p>
<ul>
<li>Online communications and forums, such as the Reddit’s <a href="https://www.reddit.com/r/MachineLearning/?rdt=58875" class="external-link">r/MachineLearning</a>
and <a href="https://datascience.stackexchange.com/" class="external-link">Data Science Stack
Exchange</a>
<ul>
<li>watch out for outdated threads!</li>
</ul>
</li>
<li>Academic journals and conferences
<ul>
<li>Unlike other sciences, computer science digital libraries like <a href="https://arxiv.org/" class="external-link">arXiv</a> enable researchers to publish their
preprints in advance and disseminates recent advances more quickly than
traditional methods of publishing</li>
</ul>
</li>
<li>
<a href="https://github.com/" class="external-link">GitHub</a> repositories</li>
<li>Practice
<ul>
<li>like any other language, you must use it or lose it!</li>
</ul>
</li>
</ul>
</div>
<div class="section level4">
<h4 id="what-other-uses-are-there-for-neural-networks">What other uses are there for neural networks?<a class="anchor" aria-label="anchor" href="#what-other-uses-are-there-for-neural-networks"></a>
</h4>
<p>In addition to image classification, we saw in the introduction other
computer vision tasks including object detection and instance and
semantic segmentation. These can all be done with CNN’s and are readily
transferable to videos. Also included in these tasks is medical imaging
for diagnoses of disease and, of course, facial recognition.</p>
<p>However, there are many other tasks which CNNs are well suited
for:</p>
<ul>
<li>Language tasks
<ul>
<li>Natural Language Processing (NLP) for text classification (sentiment
analysis, spam detection, topic classification)</li>
<li>Speech Recognition for speech to text conversion</li>
</ul>
</li>
<li>Drug Discovery</li>
<li>Time-series analysis (sensor readings, financial data, health
monitoring)</li>
<li>Robotics</li>
<li>Self-driving cars</li>
</ul>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Keypoints<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Deep Learning is well suited to classification and prediction
problems such as image recognition.</li>
<li>To use Deep Learning effectively we need to go through a workflow
of: defining the problem, identifying inputs and outputs, preparing
data, choosing the type of network, choosing a loss function, training
the model, tuning Hyperparameters, measuring performance before we can
classify data.</li>
<li>Keras is a Deep Learning library that is easier to use than many of
the alternatives such as TensorFlow and PyTorch.</li>
<li>Graphical Processing Units are useful, though not essential, for
deep learning tasks</li>
</ul>
</div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</div>
</div></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
				<p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>
        
        <a href="https://https://github.com/erinmgraham/icwithcnn/edit/main/README.html" class="external-link">Edit on GitHub</a>
        
	
        | <a href="https://https://github.com/erinmgraham/icwithcnn/blob/main/CONTRIBUTING.html" class="external-link">Contributing</a> 
        | <a href="https://https://github.com/erinmgraham/icwithcnn/" class="external-link">Source</a></p>
				<p><a href="https://https://github.com/erinmgraham/icwithcnn/blob/main/CITATION" class="external-link">Cite</a> | <a href="mailto:erin.graham@jcu.edu.au">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">
        
        <p>Materials licensed under <a href="../LICENSE.html">CC-BY 4.0</a> by the authors</p>
        
        <p><a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">Template licensed under CC-BY 4.0</a> by <a href="https://carpentries.org" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.14.0" class="external-link">sandpaper (0.14.0)</a>,
        <a href="https://github.com/carpentries/pegboard/tree/0.7.1" class="external-link">pegboard (0.7.1)</a>,
      and <a href="https://github.com/carpentries/varnish/tree/0.3.1" class="external-link">varnish (0.3.1)</a>.</p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
			<i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back to top"></i><br><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://.github.io/github.com/instructor/aio.html",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://.github.io/github.com/instructor/aio.html",
  "identifier": "https://.github.io/github.com/instructor/aio.html",
  "dateCreated": "2023-05-03",
  "dateModified": "2023-10-24",
  "datePublished": "2023-10-24"
}

  </script><script>
		feather.replace();
	</script><!-- Matomo
    2022-11-07: we have gotten a notification that we have an overage for our
    tracking and I'm pretty sure this has to do with Workbench usage.
    Considering that I am not _currently_ using this tracking because I do not
    yet know how to access the data, I am turning this off for now.
  <script>
    var _paq = window._paq = window._paq || [];
    /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
    _paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
    _paq.push(["setDomains", ["*.preview.carpentries.org","*.datacarpentry.github.io","*.datacarpentry.org","*.librarycarpentry.github.io","*.librarycarpentry.org","*.swcarpentry.github.io", "*.carpentries.github.io"]]);
    _paq.push(["setDoNotTrack", true]);
    _paq.push(["disableCookies"]);
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
          var u="https://carpentries.matomo.cloud/";
          _paq.push(['setTrackerUrl', u+'matomo.php']);
          _paq.push(['setSiteId', '1']);
          var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
          g.async=true; g.src='https://cdn.matomo.cloud/carpentries.matomo.cloud/matomo.js'; s.parentNode.insertBefore(g,s);
        })();
  </script>
  End Matomo Code -->
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

